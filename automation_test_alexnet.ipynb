{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:49:21.374350Z",
     "start_time": "2024-11-21T03:49:17.563007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.models import list_models\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "def log_message(message, level=\"info\"):\n",
    "    \"\"\"\n",
    "    Log a message, with support for info, warning, and error levels.\n",
    "    If the level is \"error\", this function raises a ValueError with the message.\n",
    "\n",
    "    Parameters:\n",
    "        message (str): The message to log.\n",
    "        level (str): The level of the message (\"info\", \"warning\", or \"error\").\n",
    "    \"\"\"\n",
    "    if level == \"info\":\n",
    "        print(f\"[INFO] {message}\")\n",
    "    elif level == \"warning\":\n",
    "        print(f\"[WARNING] {message}\")\n",
    "    elif level == \"error\":\n",
    "        print(f\"[ERROR] {message}\")\n",
    "        raise ValueError(message)  # Raise an error if level is \"error\"\n",
    "\n",
    "def sanitize_string(s):\n",
    "    \"\"\"\n",
    "    Convert a string to lowercase, strip special characters, and replace spaces with underscores.\n",
    "\n",
    "    Parameters:\n",
    "        s (str): The input string to sanitize.\n",
    "\n",
    "    Returns:\n",
    "        str: The sanitized string.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    s = s.replace(' ', '_')\n",
    "    s = re.sub(r'[^a-z0-9_]', '', s)\n",
    "    return s\n",
    "\n",
    "def ensure_directory_exists(dir_path):\n",
    "    \"\"\"\n",
    "    Ensure the specified directory exists; create it if it does not, including required subdirectories.\n",
    "    Set permissions to 777 for the main directory and its contents recursively.\n",
    "\n",
    "    Parameters:\n",
    "        dir_path (str): The path of the directory to ensure exists.\n",
    "\n",
    "    Returns:\n",
    "        str: The absolute path of the directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    xczu5egsfvc784\n",
    "    # List of required subdirectories\n",
    "    subdirs = ['dataset', 'src', 'checkpoints', 'output']\n",
    "    dirs_to_clear = ['checkpoints', 'output']    \n",
    "    \n",
    "    # Create each subdirectory if it doesnâ€™t exist\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(dir_path, subdir)\n",
    "        os.makedirs(subdir_path, exist_ok=True)\n",
    "    \n",
    "    # Set permissions recursively for main directory and all subdirectories/files\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        os.chmod(root, 0o777)\n",
    "        for d in dirs:\n",
    "            os.chmod(os.path.join(root, d), 0o777)\n",
    "        for f in files:\n",
    "            os.chmod(os.path.join(root, f), 0o777)\n",
    "    \n",
    "    # Clear all contents in the specified directories\n",
    "    for clear_dir in dirs_to_clear:\n",
    "        clear_path = os.path.join(dir_path, clear_dir)\n",
    "        for filename in os.listdir(clear_path):\n",
    "            file_path = os.path.join(clear_path, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)  # Remove file or link\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove directory and all contents\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "    return os.path.abspath(dir_path)\n",
    "\n",
    "def check_file_exists(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file exists at the specified path.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path of the file to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return os.path.isfile(file_path)\n",
    "\n",
    "def is_archive_file(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file is a valid archive (zip or tar).\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path of the file to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file is a valid archive, False otherwise.\n",
    "    \"\"\"\n",
    "    return zipfile.is_zipfile(file_path) or tarfile.is_tarfile(file_path)"
   ],
   "id": "d749f7e3abf77d99",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:19.003984Z",
     "start_time": "2024-11-19T18:57:18.983737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "\n",
    "def setup_project(prj_name, brd_name, model_type, project_folder=None, model_py_file=None, model_pth_file=None, torch_vision_model=None, finn_pretrained_model=None, dataset_type=None, custom_dataset=None, torch_vision_dataset=None):\n",
    "    \"\"\"\n",
    "    Set up a project with a specified structure, including creating necessary directories, \n",
    "    validating model type, and checking for necessary files.\n",
    "\n",
    "    Parameters:\n",
    "        finn_pretrained_model: \n",
    "        prj_name (str): The name of the project.\n",
    "        model_type (str): The type of model ('untrained', 'custom_pretrained', or 'torch_vision_pretrained').\n",
    "        project_folder (str, optional): The main folder for the project. A new folder is generated if not provided.\n",
    "        model_py_file (str, optional): The filename of the Python script defining the model architecture, required for 'untrained' and 'custom_pretrained' models.\n",
    "        model_pth_file (str, optional): The filename of the .pth file with pre-trained weights, required for 'custom_pretrained' models.\n",
    "        torch_vision_model (str, optional): The name of the TorchVision model to load if 'torch_vision_pretrained' is selected.\n",
    "        dataset_type (str, optional): Type of dataset for 'untrained' models ('torch_vision_dataset' or 'custom_dataset').\n",
    "        custom_dataset (str, optional): Path to the custom dataset file for 'untrained' models with 'custom_dataset' dataset type.\n",
    "        torch_vision_dataset (str, optional): Name of the TorchVision dataset class for 'untrained' models with 'torch_vision_dataset' dataset type.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with project setup information.\n",
    "    \"\"\"\n",
    "    log_message(\"Setting up project\")\n",
    "    working_folder = \"/home/fastqnn/finn/notebooks/Fast-QNN/outputs/txaviour/\"\n",
    "    prj_info = {}\n",
    "    \n",
    "    # Ensure Project Name is provided\n",
    "    if not prj_name:\n",
    "        log_message(\"Project Name is required\", level=\"error\")\n",
    "\n",
    "    # Sanitize project name and set project info\n",
    "    prj_name_stripped = sanitize_string(prj_name)\n",
    "    display_name = prj_name\n",
    "    prj_info[\"Display_Name\"] = display_name\n",
    "    prj_info[\"Stripped_Name\"] = prj_name_stripped\n",
    "\n",
    "    if not project_folder:\n",
    "        # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") # for production mode\n",
    "        timestamp = \"0\"\n",
    "        project_folder = f\"{working_folder}{prj_name_stripped}_{timestamp}\"\n",
    "    \n",
    "    folder_path = ensure_directory_exists(project_folder)\n",
    "    prj_info[\"Folder\"] = folder_path\n",
    "    \n",
    "    available_boards = pynq_part_map.keys()\n",
    "    if brd_name in available_boards:\n",
    "        prj_info[\"Board_name\"] = brd_name\n",
    "    else:\n",
    "        log_message(f\"'{brd_name}' is not a valid board name. Available board names are: {available_boards}\", level=\"error\")\n",
    "\n",
    "    # Validate Model Type\n",
    "    valid_model_types = [\"untrained\", \"custom_pretrained\", \"torch_vision_pretrained\", \"finn_pretrained\"]\n",
    "    if model_type not in valid_model_types:\n",
    "        log_message(f\"Model Type must be one of {valid_model_types}\", level=\"error\")\n",
    "    prj_info['Model_Type'] = model_type\n",
    "    \n",
    "    # Handle Model Type specific requirements\n",
    "    if model_type in [\"untrained\", \"custom_pretrained\"]:\n",
    "        if not model_py_file or not check_file_exists(os.path.join(project_folder, 'src', model_py_file)):\n",
    "            log_message(f\"Model Py File '{model_py_file}' does not exist in '{project_folder}'\", level=\"error\")\n",
    "        prj_info[\"Model_Py_File\"] = model_py_file\n",
    "\n",
    "    if model_type == \"custom_pretrained\":\n",
    "        if not model_pth_file or not check_file_exists(os.path.join(project_folder, 'src', model_pth_file)):\n",
    "            log_message(f\"Model Pth File '{model_pth_file}' does not exist in '{project_folder}'\", level=\"error\")\n",
    "        prj_info[\"Model_Pth_File\"] = model_pth_file\n",
    "\n",
    "    if model_type == \"torch_vision_pretrained\":\n",
    "        available_models = list_models(module=models)\n",
    "        if not torch_vision_model or torch_vision_model not in available_models:\n",
    "            log_message(f\"Torch Vision Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Pretrained_Model\"] = torch_vision_model\n",
    "\n",
    "    if model_type == \"finn_pretrained\":\n",
    "        available_models = [\"cnv_1w1a\", \"cnv_1w2a\", \"cnv_2w2a\", \"lfc_1w1a\", \"lfc_1w2a\", \"sfc_1w1a\", \"sfc_1w2a\", \"sfc_2w2a\", \"tfc_1w1a\", \"tfc_1w2a\", \"tfc_2w2a\", \"quant_mobilenet_v1_4b\"]\n",
    "        if not finn_pretrained_model or finn_pretrained_model not in available_models:\n",
    "            log_message(f\"Finn Pretrained Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Pretrained_Model\"] = finn_pretrained_model\n",
    "\n",
    "    # Handle Dataset requirements for Untrained models\n",
    "    if model_type == \"untrained\":\n",
    "        if dataset_type not in [\"torch_vision_dataset\", \"custom_dataset\"]:\n",
    "            log_message(\"Dataset Type must be either 'Torch Vision' or 'Custom Dataset' for Untrained models\", level=\"error\")\n",
    "        prj_info[\"Dataset_Type\"] = dataset_type\n",
    "\n",
    "        if dataset_type == \"custom_dataset\":\n",
    "            custom_dataset_path = os.path.join(project_folder, 'dataset', custom_dataset)\n",
    "            if not custom_dataset or not check_file_exists(custom_dataset_path) or not is_archive_file(custom_dataset_path):\n",
    "                log_message(f\"Custom Dataset '{custom_dataset}' must exist in '{project_folder}' and be an archive file (zip or tar)\", level=\"error\")\n",
    "            prj_info[\"Custom_Dataset\"] = custom_dataset\n",
    "\n",
    "        elif dataset_type == \"torch_vision_dataset\":\n",
    "            available_datasets = [cls_name.lower() for cls_name in dir(datasets) if not cls_name.startswith('_')]\n",
    "            if not torch_vision_dataset or torch_vision_dataset.lower() not in available_datasets:\n",
    "                log_message(f\"Torch Vision Dataset must be one of: {available_datasets}\", level=\"error\")\n",
    "            prj_info[\"Torch_Vision_Dataset\"] = torch_vision_dataset\n",
    "            \n",
    "    log_message(f\"Project setup complete. {prj_name} has been initialized.\")\n",
    "    return prj_info"
   ],
   "id": "4df70eb120061ecb",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:19.055457Z",
     "start_time": "2024-11-19T18:57:19.040744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.util.test import get_test_model_trained\n",
    "\n",
    "\n",
    "def load_pretrained_model(model_name, model_type, src_folder, initial_channels = 3, max_size = 4096):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model from TorchVision and ensures all downloads are in the src folder of the Project.\n",
    "\n",
    "    Parameters:\n",
    "        model_type: \n",
    "        initial_channels: \n",
    "        max_size: \n",
    "        model_name (str): The name of the pre-trained model to load (e.g., 'alexnet', 'resnet50').\n",
    "        src_folder (str): The folder where model downloads will be stored. Default is 'src'.\n",
    "                \n",
    "    Returns:\n",
    "        torch.nn.Module: The loaded pre-trained model.\n",
    "    \"\"\"\n",
    "    log_message(f\"Loading {model_type} Model: {model_name}\")\n",
    "    # Ensure the src folder exists\n",
    "    os.makedirs(src_folder, exist_ok=True)\n",
    "    # Set TORCH_HOME to the src folder to store the model downloads there\n",
    "    os.environ['TORCH_HOME'] = src_folder\n",
    "    pretrained_model = None\n",
    "    if model_type == \"torch_vision_pretrained\":\n",
    "        # Load the model from torch.hub with the specified model name\n",
    "        pretrained_model = torch.hub.load('pytorch/vision:v0.10.0', model_name, pretrained=True)\n",
    "    elif model_type == \"finn_pretrained\":\n",
    "        if model_name == \"cnv_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 1, 1)\n",
    "        elif model_name == \"cnv_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 1, 2)\n",
    "        elif model_name == \"cnv_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 2, 2)\n",
    "        elif model_name == \"lfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"LFC\", 1, 1)\n",
    "        elif model_name == \"lfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"LFC\", 1, 2)\n",
    "        elif model_name == \"sfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 1, 1)\n",
    "        elif model_name == \"sfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 1, 2)\n",
    "        elif model_name == \"sfc_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 2, 2)\n",
    "        elif model_name == \"tfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 1, 1)\n",
    "        elif model_name == \"tfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 1, 2)\n",
    "        elif model_name == \"tfc_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 2, 2)\n",
    "        elif model_name == \"quant_mobilenet_v1_4b\":\n",
    "            pretrained_model = get_test_model_trained(\"mobilenet\", 4, 4)\n",
    "    # List of common input shapes to test first (both square and non-square)\n",
    "    common_shapes = [\n",
    "        (1, initial_channels, 32, 32),  # Typical for many models\n",
    "        (1, initial_channels, 28, 28),  # Typical for many models\n",
    "        (1, initial_channels, 224, 224),  # Typical for many models\n",
    "        (1, initial_channels, 299, 299),  # For models like Inception\n",
    "        (1, initial_channels, 128, 128),  # Smaller size\n",
    "        (1, initial_channels, 256, 256),  # Larger square size\n",
    "        (1, initial_channels, 320, 240),  # Common non-square size\n",
    "        (1, initial_channels, 240, 320),  # Non-square (swapped dimensions)\n",
    "        (1, initial_channels, 256, 128),  # Non-square\n",
    "        (1, initial_channels, 128, 256),  # Non-square (swapped)\n",
    "    ]\n",
    "\n",
    "    # Try common shapes first\n",
    "    for shape in common_shapes:\n",
    "        try:\n",
    "            dummy_input = torch.rand(*shape)\n",
    "            pretrained_model(dummy_input)\n",
    "            log_message(f\"Compatible common input shape found: {shape}\")\n",
    "            return pretrained_model, shape\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "\n",
    "    # If no common shape worked, test all possible square and non-square shapes up to max_size\n",
    "    for width in range(1, max_size + 1, 1):  # Step by 16 for efficiency\n",
    "        for height in range(1, max_size + 1, 1):\n",
    "            try:\n",
    "                dummy_input = torch.rand(1, initial_channels, width, height)\n",
    "                pretrained_model(dummy_input)\n",
    "                pretrained_model_input_shape = (1, initial_channels, width, height)\n",
    "                log_message(f\"Compatible input shape found: {pretrained_model_input_shape}\")\n",
    "                return pretrained_model, pretrained_model_input_shape\n",
    "            except RuntimeError:\n",
    "                continue\n",
    "\n",
    "    log_message(\"Could not determine a compatible input shape within the specified range.\", level=\"error\")"
   ],
   "id": "e912d407f448ecb4",
   "outputs": [],
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "id": "3b4a8b34b65b86a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:20.476151Z",
     "start_time": "2024-11-19T18:57:19.095083Z"
    }
   },
   "source": [
    "prj_name_input = \"AlexNet 1W1A Test\"\n",
    "board_name_input = \"Pynq-Z2\"\n",
    "prj_folder_input = sanitize_string(prj_name_input)\n",
    "model_type_input = \"torch_vision_pretrained\"\n",
    "torch_vision_model_input = \"alexnet\"\n",
    "Project_Info = setup_project(prj_name=prj_name_input, brd_name=board_name_input, model_type=model_type_input, torch_vision_model=torch_vision_model_input)\n",
    "\n",
    "input_model= None\n",
    "input_model_shape = None\n",
    "\n",
    "if Project_Info['Model_Type'] == \"untrained\":\n",
    "    log_message(\"Training for untrained models are not supported at the moment!\", level=\"error\")\n",
    "elif Project_Info['Model_Type'] == \"custom_pretrained\":\n",
    "    log_message(\"Custom Pretrained models are not supported at the moment!\", level=\"error\")\n",
    "elif Project_Info['Model_Type'] == \"torch_vision_pretrained\" or Project_Info['Model_Type'] == \"finn_pretrained\":\n",
    "    pretrained_folder = os.path.join(Project_Info['Folder'],\"src\")\n",
    "    input_model, input_model_shape = load_pretrained_model(Project_Info['Pretrained_Model'], Project_Info['Model_Type'], pretrained_folder, initial_channels=3)\n",
    "else:\n",
    "    log_message(\"Unsupported Model Type\", level=\"error\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting up project\n",
      "[INFO] Project setup complete. AlexNet 1W1A Test has been initialized.\n",
      "[INFO] Loading torch_vision_pretrained Model: alexnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fastqnn/finn/notebooks/Fast-QNN/outputs/txaviour/alexnet_1w1a_test_0/src/hub/pytorch_vision_v0.10.0\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Compatible common input shape found: (1, 3, 224, 224)\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:20.528824Z",
     "start_time": "2024-11-19T18:57:20.522624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_onnx_checkpoint(project_info, suffix):\n",
    "    \"\"\"\n",
    "    Generates the export path for ONNX files based on a specified suffix.\n",
    "\n",
    "    Parameters:\n",
    "        project_info (dict): Dictionary containing project information (e.g., 'Folder' and 'Stripped_Name').\n",
    "        suffix (str): The suffix to append to the exported ONNX filename (e.g., \"model1\" for \"model1_export.onnx\").\n",
    "\n",
    "    Returns:\n",
    "        str: The full path to the export file.\n",
    "    \"\"\"\n",
    "    log_message(f\"Saving Checkpoint: {suffix}\")\n",
    "    suffix = sanitize_string(suffix)\n",
    "    filename = f\"{project_info['Stripped_Name']}_{suffix}.onnx\"\n",
    "    return os.path.join(project_info['Folder'], \"checkpoints\", filename)\n"
   ],
   "id": "1f4ae7b982c2a64c",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:26.685830Z",
     "start_time": "2024-11-19T18:57:20.568267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "export_onnx_path = set_onnx_checkpoint(Project_Info,\"Brevitas Export\")\n",
    "export_qonnx(input_model, torch.randn(input_model_shape), export_onnx_path, opset_version=9)\n",
    "qonnx_cleanup(export_onnx_path, out_file=export_onnx_path)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Brevitas Export\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:33.393529Z",
     "start_time": "2024-11-19T18:57:26.712708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "model = ModelWrapper(export_onnx_path)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"QONNX to FINN\"))"
   ],
   "id": "de436732503bc809",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: QONNX to FINN\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:33.429888Z",
     "start_time": "2024-11-19T18:57:33.421902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from qonnx.transformation.remove import RemoveIdentityOps\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import (GiveReadableTensorNames,\n",
    "                                          GiveUniqueNodeNames,\n",
    "                                          RemoveStaticGraphInputs,\n",
    "                                          RemoveUnusedTensors, GiveUniqueParameterTensors, SortGraph)\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "def tidy_up_transforms(input_tidy_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a series of transformations to a model and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_tidy_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply transformations\n",
    "    input_tidy_model = input_tidy_model.transform(GiveUniqueParameterTensors())\n",
    "    input_tidy_model = input_tidy_model.transform(InferShapes())\n",
    "    input_tidy_model = input_tidy_model.transform(FoldConstants())\n",
    "    input_tidy_model = input_tidy_model.transform(GiveUniqueNodeNames())\n",
    "    input_tidy_model = input_tidy_model.transform(GiveReadableTensorNames())\n",
    "    input_tidy_model = input_tidy_model.transform(InferDataTypes())\n",
    "    input_tidy_model = input_tidy_model.transform(RemoveStaticGraphInputs())\n",
    "    input_tidy_model = input_tidy_model.transform(InferDataLayouts())\n",
    "    input_tidy_model = input_tidy_model.transform(RemoveUnusedTensors())\n",
    "    input_tidy_model = input_tidy_model.transform(DoubleToSingleFloat())\n",
    "    input_tidy_model = input_tidy_model.transform(SortGraph())\n",
    "    input_tidy_model = input_tidy_model.transform(RemoveIdentityOps())\n",
    "\n",
    "    # Save the transformed model\n",
    "    input_tidy_model.save(save_name)\n",
    "\n",
    "    return input_tidy_model"
   ],
   "id": "a5d959d6407a1ec2",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:37.471605Z",
     "start_time": "2024-11-19T18:57:33.453799Z"
    }
   },
   "cell_type": "code",
   "source": "model = tidy_up_transforms(model, set_onnx_checkpoint(Project_Info,\"Tidy ONNX Post Finn\"))",
   "id": "21fb27ba948a8cbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Tidy ONNX Post Finn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/home_dir/.local/lib/python3.10/site-packages/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:37.739911Z",
     "start_time": "2024-11-19T18:57:37.735622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.util.pytorch import ToTensor\n",
    "\n",
    "log_message(\"Skipping Pre-Processing. Will be expecting the user to handle it in application!\", level=\"warning\")"
   ],
   "id": "511d18741790a4a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Skipping Pre-Processing. Will be expecting the user to handle it in application!\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:37.771412Z",
     "start_time": "2024-11-19T18:57:37.766147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = set_onnx_checkpoint(Project_Info,\"Pre Proc ONNX Finn\")\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name, opset_version=9)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\"\"\""
   ],
   "id": "57af62ce8ef3c010",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nglobal_inp_name = model.graph.input[0].name\\nishape = model.get_tensor_shape(global_inp_name)\\n# preprocessing: torchvision\\'s ToTensor divides uint8 inputs by 255\\ntotensor_pyt = ToTensor()\\nchkpt_preproc_name = set_onnx_checkpoint(Project_Info,\"Pre Proc ONNX Finn\")\\nexport_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name, opset_version=9)\\nqonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\\npre_model = ModelWrapper(chkpt_preproc_name)\\npre_model = pre_model.transform(ConvertQONNXtoFINN())\\nmodel = model.transform(MergeONNXModels(pre_model))\\n# add input quantization annotation: UINT8 for all BNN-PYNQ models\\nglobal_inp_name = model.graph.input[0].name\\nmodel.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:38.718985Z",
     "start_time": "2024-11-19T18:57:37.800658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiplyByOne(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = x * 2\n",
    "        out = out + 0  # Adding a no-op to prevent graph simplification\n",
    "        out = out / 2  # Adding a no-op to prevent graph simplification\n",
    "        return out\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "mul_model = MultiplyByOne()\n",
    "\n",
    "chkpt_mul_name = set_onnx_checkpoint(Project_Info, \"Mul_One_ONNX\")\n",
    "export_qonnx(mul_model, torch.randn(ishape), chkpt_mul_name, opset_version=9)\n",
    "qonnx_cleanup(chkpt_mul_name, out_file=chkpt_mul_name)\n",
    "\n",
    "mul_model_wrapper = ModelWrapper(chkpt_mul_name)\n",
    "mul_model_wrapper = mul_model_wrapper.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(MergeONNXModels(mul_model_wrapper))\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])"
   ],
   "id": "13c356373544a8a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Mul_One_ONNX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/home_dir/.local/lib/python3.10/site-packages/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:41.211007Z",
     "start_time": "2024-11-19T18:57:38.743271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"Post Processing\"))\n",
    "model = tidy_up_transforms(model, set_onnx_checkpoint(Project_Info,\"Tidy Post PrePost Proc\"))"
   ],
   "id": "49bbd723443c4e65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Post Processing\n",
      "[INFO] Saving Checkpoint: Tidy Post PrePost Proc\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:41.248617Z",
     "start_time": "2024-11-19T18:57:41.238024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.qonnx.qonnx_activation_handlers import QuantReluHandler\n",
    "from onnx import TensorProto\n",
    "from qonnx.transformation.base import Transformation\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from onnx import helper as oh\n",
    "\n",
    "class CustomReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Implementing an approximation of abs(x) using only arithmetic operations\n",
    "        epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "        abs_x = (x * x + epsilon) / (x + epsilon)\n",
    "        return (x + abs_x) / 2\n",
    "\n",
    "class CustomReluBreakdown(Transformation):\n",
    "    \"\"\"\n",
    "    Custom transformation to iterate through the nodes, identify ReLU nodes,\n",
    "    and convert the next node to a quant node while applying the QuantReluHandler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bit_width=1, quant_type=DataType[\"UINT8\"], narrow=0, signed=0):\n",
    "        super().__init__()\n",
    "        self.bit_width = bit_width\n",
    "        self.quant_type = quant_type\n",
    "        self.narrow = narrow\n",
    "        self.signed = signed\n",
    "\n",
    "    def apply(self, transform_model):\n",
    "        graph = transform_model.graph\n",
    "        graph_modified = False\n",
    "        convert_next_node = False\n",
    "\n",
    "        for idx, node in enumerate(graph.node):\n",
    "            if convert_next_node:\n",
    "                # Prepare names for additional input tensors\n",
    "                scale_name = f\"{node.name}_scale\"\n",
    "                zero_point_name = f\"{node.name}_zero_point\"\n",
    "                bit_width_name = f\"{node.name}_bit_width\"\n",
    "\n",
    "                # Create initializers for the scale, zero-point, and bit-width tensors\n",
    "                scale_initializer = oh.make_tensor(\n",
    "                    scale_name, TensorProto.FLOAT, dims=[], vals=[1.0]\n",
    "                )\n",
    "                zero_point_initializer = oh.make_tensor(\n",
    "                    zero_point_name, TensorProto.INT8, dims=[], vals=[0]\n",
    "                )\n",
    "                bit_width_initializer = oh.make_tensor(\n",
    "                    bit_width_name, TensorProto.INT32, dims=[], vals=[self.bit_width]\n",
    "                )\n",
    "\n",
    "                # Add initializers to the model\n",
    "                transform_model.graph.initializer.extend([\n",
    "                    scale_initializer, zero_point_initializer, bit_width_initializer\n",
    "                ])\n",
    "\n",
    "                # Convert the current node to a quant node with additional inputs\n",
    "                quant_node_name = f\"{node.name}_quant\"\n",
    "                quant_node = oh.make_node(\n",
    "                    \"Quant\",\n",
    "                    inputs=[node.input[0], scale_name, zero_point_name, bit_width_name],\n",
    "                    outputs=node.output,\n",
    "                    name=quant_node_name,\n",
    "                    domain=\"qonnx.custom_op.general\"\n",
    "                )\n",
    "                # Add the attributes to the Quant node\n",
    "                quant_node.attribute.extend([\n",
    "                    oh.make_attribute(\"bit_width\", int(self.bit_width)),\n",
    "                    oh.make_attribute(\"quant_type\", str(self.quant_type)),\n",
    "                    oh.make_attribute(\"narrow\", int(self.narrow)),\n",
    "                    oh.make_attribute(\"signed\", int(self.signed))\n",
    "                ])\n",
    "\n",
    "                # Insert the Quant node into the graph and remove the original node\n",
    "                graph.node.insert(idx, quant_node)\n",
    "                graph.node.remove(node)\n",
    "\n",
    "                # Call QuantReluHandler on the current Quant node\n",
    "                handler = QuantReluHandler(transform_model, quant_node, idx)\n",
    "                handler.replace_quant_node()\n",
    "\n",
    "                convert_next_node = False\n",
    "                graph_modified = True\n",
    "\n",
    "            if node.op_type == \"Relu\":\n",
    "                # Raise a flag to convert the next node to a quant node\n",
    "                convert_next_node = True\n",
    "\n",
    "       # transform_model = transform_model.transform(InferShapes())\n",
    "        return (transform_model, graph_modified)\n"
   ],
   "id": "a45466c16fde453c",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:57:41.347056Z",
     "start_time": "2024-11-19T18:57:41.270602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import traceback\n",
    "from qonnx.transformation.batchnorm_to_affine import BatchNormToAffine\n",
    "from qonnx.transformation.general import ConvertSubToAdd, ConvertDivToMul\n",
    "from finn.transformation.streamline import Streamline, RoundAndClipThresholds, CollapseRepeatedMul, ConvertSignToThres, \\\n",
    "    CollapseRepeatedAdd\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from finn.transformation.streamline.absorb import (AbsorbTransposeIntoMultiThreshold,\n",
    "                                                   AbsorbScalarMulAddIntoTopK,\n",
    "                                                   AbsorbSignBiasIntoMultiThreshold,\n",
    "                                                   AbsorbAddIntoMultiThreshold,\n",
    "                                                   AbsorbMulIntoMultiThreshold, FactorOutMulSignMagnitude,\n",
    "                                                   Absorb1BitMulIntoMatMul, Absorb1BitMulIntoConv)\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants, MoveAddPastMul, \\\n",
    "    MoveScalarAddPastMatMul, MoveAddPastConv, MoveScalarMulPastMatMul, MoveScalarMulPastConv, \\\n",
    "    MoveMaxPoolPastMultiThreshold, MoveLinearPastEltwiseAdd, MoveLinearPastFork\n",
    "from finn.transformation.streamline.absorb import AbsorbConsecutiveTransposes\n",
    "\n",
    "def streamline_transforms(input_streamline_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a series of streamlining transformations to a model and saves the resulting model.\n",
    "    If a transformation fails, it logs the error and moves on to the next transformation.\n",
    "\n",
    "    Parameters:\n",
    "        input_streamline_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        AbsorbSignBiasIntoMultiThreshold(),\n",
    "        ConvertSubToAdd(),\n",
    "        ConvertDivToMul(),\n",
    "        CollapseRepeatedMul(),\n",
    "        BatchNormToAffine(),\n",
    "        ConvertSignToThres(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarAddPastMatMul(),\n",
    "        MoveAddPastConv(),\n",
    "        MoveScalarMulPastMatMul(),\n",
    "        MoveScalarMulPastConv(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarLinearPastInvariants(),\n",
    "        CollapseRepeatedAdd(),\n",
    "        AbsorbAddIntoMultiThreshold(),\n",
    "        FactorOutMulSignMagnitude(),\n",
    "        MoveMaxPoolPastMultiThreshold(),\n",
    "        AbsorbMulIntoMultiThreshold(),\n",
    "        Absorb1BitMulIntoMatMul(),\n",
    "        Absorb1BitMulIntoConv(),\n",
    "        AbsorbMulIntoMultiThreshold(),\n",
    "        Streamline(),\n",
    "        LowerConvsToMatMul(),\n",
    "        MakeMaxPoolNHWC(),\n",
    "        AbsorbTransposeIntoMultiThreshold(),\n",
    "        ConvertBipolarMatMulToXnorPopcount(),\n",
    "        Streamline(),\n",
    "        AbsorbScalarMulAddIntoTopK(),\n",
    "        RoundAndClipThresholds(),\n",
    "        MoveLinearPastEltwiseAdd(),\n",
    "        MoveLinearPastFork()\n",
    "    ]\n",
    "    for iter_id in range(3):\n",
    "        for transform in transformations:\n",
    "            try:\n",
    "                input_streamline_model = input_streamline_model.transform(transform)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "        input_streamline_model = input_streamline_model.transform(CustomReluBreakdown())\n",
    "        # Save the transformed model after tidying up\n",
    "        input_streamline_model = tidy_up_transforms(input_streamline_model, save_name)\n",
    "    \n",
    "    return input_streamline_model"
   ],
   "id": "6bdb92934093ceb8",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:59:52.941511Z",
     "start_time": "2024-11-19T18:57:41.370162Z"
    }
   },
   "cell_type": "code",
   "source": "model = streamline_transforms(model, set_onnx_checkpoint(Project_Info,\"Streamlined ONNX\"))",
   "id": "d969950c0d96fc53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Streamlined ONNX\n",
      "operands could not be broadcast together with shapes (55,1) (64,1) \n",
      "operands could not be broadcast together with shapes (55,1) (64,1) \n",
      "operands could not be broadcast together with shapes (55,1) (64,1) \n",
      "operands could not be broadcast together with shapes (55,1) (64,1) \n",
      "operands could not be broadcast together with shapes (55,1) (64,1) \n",
      "operands could not be broadcast together with shapes (55,1) (64,1) \n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:59:53.015708Z",
     "start_time": "2024-11-19T18:59:53.009573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.fpgadataflow.convert_to_hw_layers import (\n",
    "    InferBinaryMatrixVectorActivation,\n",
    "    InferQuantizedMatrixVectorActivation,\n",
    "    InferLabelSelectLayer,\n",
    "    InferThresholdingLayer,\n",
    "    InferStreamingMaxPool,\n",
    "    InferConvInpGen,\n",
    "    InferAddStreamsLayer,\n",
    "    InferChannelwiseLinearLayer,\n",
    "    InferConcatLayer,\n",
    "    InferDuplicateStreamsLayer,\n",
    "    InferGlobalAccPoolLayer,\n",
    "    InferLookupLayer,\n",
    "    InferPool,\n",
    "    InferStreamingEltwise,\n",
    "    InferUpsample,\n",
    "    InferVectorVectorActivation\n",
    ")\n",
    "\n",
    "def to_hw_transforms(input_hw_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a comprehensive series of hardware-oriented transformations to a model and saves the resulting model.\n",
    "    If a transformation fails, it logs the error and moves on to the next transformation.\n",
    "\n",
    "    Parameters:\n",
    "        input_hw_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        InferBinaryMatrixVectorActivation(),\n",
    "        InferQuantizedMatrixVectorActivation(),\n",
    "        InferLabelSelectLayer(),\n",
    "        InferThresholdingLayer(),\n",
    "        InferConvInpGen(),\n",
    "        InferStreamingMaxPool(),\n",
    "        InferAddStreamsLayer(),\n",
    "        InferChannelwiseLinearLayer(),\n",
    "        InferConcatLayer(),\n",
    "        InferDuplicateStreamsLayer(),\n",
    "        InferGlobalAccPoolLayer(),\n",
    "        InferLookupLayer(),\n",
    "        InferPool(),\n",
    "        InferStreamingEltwise(),\n",
    "        InferUpsample(),\n",
    "        InferVectorVectorActivation(),\n",
    "        RemoveCNVtoFCFlatten(),\n",
    "        AbsorbConsecutiveTransposes()\n",
    "    ]\n",
    "\n",
    "    for iter_id in range(3):\n",
    "        for transform in transformations:\n",
    "            try:\n",
    "                input_hw_model = input_hw_model.transform(transform)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "        # Apply final tidy-up transformations\n",
    "        input_hw_model = tidy_up_transforms(input_hw_model, save_name)\n",
    "\n",
    "    # Save the final transformed model\n",
    "    input_hw_model.save(save_name)\n",
    "\n",
    "    return input_hw_model"
   ],
   "id": "e34005da6e651b95",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T19:00:01.465833Z",
     "start_time": "2024-11-19T18:59:53.044775Z"
    }
   },
   "cell_type": "code",
   "source": "model = to_hw_transforms(model, set_onnx_checkpoint(Project_Info,\"To HW Layers\"))",
   "id": "3231949dbb03adb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: To HW Layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:394: UserWarning: Stride is not equal to kernel. Node cannot be converted to\n",
      "                        StreamingMaxPool layer.\n",
      "  warnings.warn(warn_str)\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T19:00:01.519957Z",
     "start_time": "2024-11-19T19:00:01.513949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "\n",
    "def dataflow_partitioning(input_data_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies dataflow partitioning transformation to the model and saves the resulting parent and dataflow models.\n",
    "\n",
    "    Parameters:\n",
    "        input_data_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The directory to save the transformed models.\n",
    "\n",
    "    Returns:\n",
    "        ModelWrapper: The transformed parent model.\n",
    "    \"\"\"\n",
    "    # Apply dataflow partitioning\n",
    "    parent_model = input_data_model.transform(CreateDataflowPartition())\n",
    "    parent_model.save(save_name)\n",
    "    \n",
    "    # Retrieve the dataflow partition model filename\n",
    "    sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "    sdp_node = getCustomOp(sdp_node)\n",
    "    dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "    # Load and return the dataflow model\n",
    "    dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "    \n",
    "    return dataflow_model"
   ],
   "id": "7b896786bf7eccae",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T19:00:01.652664Z",
     "start_time": "2024-11-19T19:00:01.565044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = dataflow_partitioning(model, set_onnx_checkpoint(Project_Info,\"Dataflow Partition Parent Model\"))\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"Dataflow Partition Streaming Model\"))"
   ],
   "id": "a18f4bf215064d2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Dataflow Partition Parent Model\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "cycle-free graph violated: partition depends on itself",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[109], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mdataflow_partitioning\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_onnx_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mProject_Info\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDataflow Partition Parent Model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39msave(set_onnx_checkpoint(Project_Info,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataflow Partition Streaming Model\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "Cell \u001B[0;32mIn[108], line 18\u001B[0m, in \u001B[0;36mdataflow_partitioning\u001B[0;34m(input_data_model, save_name)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03mApplies dataflow partitioning transformation to the model and saves the resulting parent and dataflow models.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m    ModelWrapper: The transformed parent model.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Apply dataflow partitioning\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m parent_model \u001B[38;5;241m=\u001B[39m \u001B[43minput_data_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCreateDataflowPartition\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m parent_model\u001B[38;5;241m.\u001B[39msave(save_name)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Retrieve the dataflow partition model filename\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/qonnx/core/modelwrapper.py:140\u001B[0m, in \u001B[0;36mModelWrapper.transform\u001B[0;34m(self, transformation, make_deepcopy, cleanup)\u001B[0m\n\u001B[1;32m    138\u001B[0m model_was_changed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m model_was_changed:\n\u001B[0;32m--> 140\u001B[0m     (transformed_model, model_was_changed) \u001B[38;5;241m=\u001B[39m \u001B[43mtransformation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformed_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cleanup:\n\u001B[1;32m    142\u001B[0m     transformed_model\u001B[38;5;241m.\u001B[39mcleanup()\n",
      "File \u001B[0;32m/home/fastqnn/finn/src/finn/transformation/fpgadataflow/create_dataflow_partition.py:80\u001B[0m, in \u001B[0;36mCreateDataflowPartition.apply\u001B[0;34m(self, model)\u001B[0m\n\u001B[1;32m     77\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;66;03m# first, use the generic partitioning functionality to split up the graph\u001B[39;00m\n\u001B[0;32m---> 80\u001B[0m parent_model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[43m    \u001B[49m\u001B[43mPartitionFromLambda\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartitioning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43massign_partition_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartition_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartition_model_dir\u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;66;03m# change node types to StreamingDataflowPartition\u001B[39;00m\n\u001B[1;32m     86\u001B[0m p_nodes \u001B[38;5;241m=\u001B[39m parent_model\u001B[38;5;241m.\u001B[39mget_nodes_by_op_type(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenericPartition\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/qonnx/core/modelwrapper.py:140\u001B[0m, in \u001B[0;36mModelWrapper.transform\u001B[0;34m(self, transformation, make_deepcopy, cleanup)\u001B[0m\n\u001B[1;32m    138\u001B[0m model_was_changed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m model_was_changed:\n\u001B[0;32m--> 140\u001B[0m     (transformed_model, model_was_changed) \u001B[38;5;241m=\u001B[39m \u001B[43mtransformation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformed_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cleanup:\n\u001B[1;32m    142\u001B[0m     transformed_model\u001B[38;5;241m.\u001B[39mcleanup()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/qonnx/transformation/create_generic_partitions.py:119\u001B[0m, in \u001B[0;36mPartitionFromLambda.apply\u001B[0;34m(self, model)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m to_check:\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m node \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    118\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[0;32m--> 119\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitioning(node) \u001B[38;5;241m!=\u001B[39m partition_id\n\u001B[1;32m    120\u001B[0m         ), \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mcycle-free graph violated: partition depends on itself\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m    121\u001B[0m         \u001B[38;5;66;03m# print(node)\u001B[39;00m\n\u001B[1;32m    122\u001B[0m         predecessors \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfind_direct_predecessors(node)\n",
      "\u001B[0;31mAssertionError\u001B[0m: cycle-free graph violated: partition depends on itself"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "\n",
    "def specialize_layers_transform(input_specialize_model, board_name, save_name):\n",
    "    \"\"\"\n",
    "    Applies layer specialization transformation to a dataflow model for the specified FPGA part and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_specialize_model (ModelWrapper): The dataflow model to transform.\n",
    "        board_name (str): The FPGA board for which to specialize the layers.\n",
    "        save_name (str): The path to save the specialized model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed and specialized dataflow model.\n",
    "    \"\"\"\n",
    "    fpga_part = pynq_part_map[board_name]\n",
    "    # Apply specialization for FPGA layers\n",
    "    input_specialize_model = input_specialize_model.transform(SpecializeLayers(fpga_part))\n",
    "\n",
    "    # Save the specialized model\n",
    "    input_specialize_model.save(save_name)\n",
    "\n",
    "    return input_specialize_model"
   ],
   "id": "29068574207cfe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = specialize_layers_transform(model, Project_Info['Board_name'], set_onnx_checkpoint(Project_Info,f\"Specialize Model Layers to {Project_Info['Board_name']}\"))",
   "id": "66573cba9e8b29aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def folding_transform(input_folding_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies folding configuration to fully connected (MVAU_hls) and sliding window (ConvolutionInputGenerator_rtl)\n",
    "    layers in the model and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_folding_model (ModelWrapper): The specialized model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed and folded dataflow model.\n",
    "    \"\"\"\n",
    "    folding_config = [\n",
    "    (16, 3, [128]),\n",
    "    (32, 32, [128]),\n",
    "    (16, 32, [128]),\n",
    "    (16, 32, [128]),\n",
    "    (4, 32, [81]),\n",
    "    (1, 32, [2]),\n",
    "    (1, 4, [2]),\n",
    "    (1, 8, [128]),\n",
    "    (5, 1, [3]),\n",
    "    ]\n",
    "    \n",
    "    # Apply folding configuration to fully connected layers\n",
    "    fc_layers = input_folding_model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "    for fcl, (pe, simd, ififodepth) in zip(fc_layers, folding_config):\n",
    "        fcl_inst = getCustomOp(fcl)\n",
    "        fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "        fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "        fcl_inst.set_nodeattr(\"inFIFODepths\", ififodepth)\n",
    "    \n",
    "    # Apply SIMD values from the folding configuration to sliding window layers\n",
    "    swg_layers = input_folding_model.get_nodes_by_op_type(\"ConvolutionInputGenerator_rtl\")\n",
    "    for i in range(len(swg_layers)):\n",
    "        swg_inst = getCustomOp(swg_layers[i])\n",
    "        simd = folding_config[i][1]\n",
    "        swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    \n",
    "    # Apply unique node names to all nodes and save the transformed model\n",
    "    input_folding_model = input_folding_model.transform(GiveUniqueNodeNames())\n",
    "    input_folding_model.save(save_name)\n",
    "\n",
    "    return input_folding_model\n"
   ],
   "id": "cd5527c3889d3ed2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = folding_transform(model, set_onnx_checkpoint(Project_Info, \"Folded Model\"))",
   "id": "1269bd96fdeb9db3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "\n",
    "def zynq_build_transform(input_zynq_model, save_name, brd_name):\n",
    "    \"\"\"\n",
    "    Applies the ZynqBuild transformation to a model for the specified PYNQ board and clock period.\n",
    "\n",
    "    Parameters:\n",
    "        input_zynq_model (ModelWrapper): Folded model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "        brd_name (str): Name of the PYNQ board to target.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model after applying ZynqBuild.\n",
    "    \"\"\"\n",
    "    target_clk_ns = 10\n",
    "    # Apply ZynqBuild transformation\n",
    "    input_zynq_model = input_zynq_model.transform(ZynqBuild(platform=brd_name, period_ns=target_clk_ns))\n",
    "    \n",
    "    # Save the transformed model\n",
    "    input_zynq_model.save(save_name)\n",
    "\n",
    "    return input_zynq_model\n"
   ],
   "id": "394bed78132471e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "\n",
    "def pynq_driver_transform(input_driver_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies the MakePYNQDriver transformation to the model to generate a PYNQ-compatible driver.\n",
    "\n",
    "    Parameters:\n",
    "        input_driver_model (ModelWrapper): ZynqBuild model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model with PYNQ driver compatibility.\n",
    "    \"\"\"\n",
    "    # Apply MakePYNQDriver transformation\n",
    "    input_driver_model = input_driver_model.transform(MakePYNQDriver(\"zynq-iodma\"))\n",
    "    \n",
    "    # Save the transformed model\n",
    "    input_driver_model.save(save_name)\n",
    "\n",
    "    return input_driver_model\n"
   ],
   "id": "d1f6d7caa00a8452",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = zynq_build_transform(model, set_onnx_checkpoint(Project_Info, \"Zynq Build\"), Project_Info['Board_name'])\n",
    "model = pynq_driver_transform(model, set_onnx_checkpoint(Project_Info, \"Pynq Driver\"))"
   ],
   "id": "669b7375d6509996",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
