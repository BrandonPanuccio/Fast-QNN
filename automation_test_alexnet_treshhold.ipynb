{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "\n",
    "def log_message(message, level=\"info\"):\n",
    "    \"\"\"\n",
    "    Log a message, with support for info, warning, and error levels.\n",
    "    If the level is \"error\", this function raises a ValueError with the message.\n",
    "\n",
    "    Parameters:\n",
    "        message (str): The message to log.\n",
    "        level (str): The level of the message (\"info\", \"warning\", or \"error\").\n",
    "    \"\"\"\n",
    "    if level == \"info\":\n",
    "        print(f\"[INFO] {message}\")\n",
    "    elif level == \"warning\":\n",
    "        print(f\"[WARNING] {message}\")\n",
    "    elif level == \"error\":\n",
    "        print(f\"[ERROR] {message}\")\n",
    "        raise ValueError(message)  # Raise an error if level is \"error\"\n",
    "\n",
    "def sanitize_string(s):\n",
    "    \"\"\"\n",
    "    Convert a string to lowercase, strip special characters, and replace spaces with underscores.\n",
    "\n",
    "    Parameters:\n",
    "        s (str): The input string to sanitize.\n",
    "\n",
    "    Returns:\n",
    "        str: The sanitized string.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    s = s.replace(' ', '_')\n",
    "    s = re.sub(r'[^a-z0-9_]', '', s)\n",
    "    return s\n",
    "\n",
    "def ensure_directory_exists(dir_path):\n",
    "    \"\"\"\n",
    "    Ensure the specified directory exists; create it if it does not, including required subdirectories.\n",
    "    Set permissions to 777 for the main directory and its contents recursively.\n",
    "\n",
    "    Parameters:\n",
    "        dir_path (str): The path of the directory to ensure exists.\n",
    "\n",
    "    Returns:\n",
    "        str: The absolute path of the directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # List of required subdirectories\n",
    "    subdirs = ['dataset', 'src', 'checkpoints', 'output']\n",
    "    dirs_to_clear = ['checkpoints', 'output']    \n",
    "    \n",
    "    # Create each subdirectory if it doesnâ€™t exist\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(dir_path, subdir)\n",
    "        os.makedirs(subdir_path, exist_ok=True)\n",
    "    \n",
    "    # Set permissions recursively for main directory and all subdirectories/files\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        os.chmod(root, 0o777)\n",
    "        for d in dirs:\n",
    "            os.chmod(os.path.join(root, d), 0o777)\n",
    "        for f in files:\n",
    "            os.chmod(os.path.join(root, f), 0o777)\n",
    "    \n",
    "    # Clear all contents in the specified directories\n",
    "    for clear_dir in dirs_to_clear:\n",
    "        clear_path = os.path.join(dir_path, clear_dir)\n",
    "        for filename in os.listdir(clear_path):\n",
    "            file_path = os.path.join(clear_path, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)  # Remove file or link\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove directory and all contents\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "    return os.path.abspath(dir_path)\n",
    "\n",
    "def check_file_exists(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file exists at the specified path.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path of the file to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return os.path.isfile(file_path)\n",
    "\n",
    "def is_archive_file(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file is a valid archive (zip or tar).\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path of the file to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file is a valid archive, False otherwise.\n",
    "    \"\"\"\n",
    "    return zipfile.is_zipfile(file_path) or tarfile.is_tarfile(file_path)"
   ],
   "id": "d749f7e3abf77d99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "\n",
    "def setup_project(prj_name, brd_name, model_type, project_folder=None, model_py_file=None, model_pth_file=None, sample_untrained_model=None, sample_pretrained_model=None, finn_pretrained_model=None, dataset_type=None, custom_dataset=None, torch_vision_dataset=None):\n",
    "    \"\"\"\n",
    "    Set up a project with a specified structure, including creating necessary directories, \n",
    "    validating model type, and checking for necessary files.\n",
    "\n",
    "    Parameters:\n",
    "        prj_name (str): The name of the project.\n",
    "        brd_name (str): The name of the target board for the project (e.g., PYNQ board).\n",
    "        model_type (str): The type of model ('untrained', 'sample_untrained', 'custom_pretrained', \n",
    "                          'sample_pretrained', or 'finn_pretrained').\n",
    "        project_folder (str, optional): The main folder for the project. A new folder is generated if not provided.\n",
    "        model_py_file (str, optional): The filename of the Python script defining the model architecture, required \n",
    "                                       for 'untrained' and 'custom_pretrained' models.\n",
    "        model_pth_file (str, optional): The filename of the .pth file with pre-trained weights, required for \n",
    "                                        'custom_pretrained' models.\n",
    "        sample_untrained_model (str, optional): The name of the sample untrained model to load, required for \n",
    "                                                'sample_untrained' models.\n",
    "        sample_pretrained_model (str, optional): The name of the sample pretrained model to load, required for \n",
    "                                                 'sample_pretrained' models.\n",
    "        finn_pretrained_model (str, optional): The name of the FINN pretrained model to load, required for \n",
    "                                               'finn_pretrained' models.\n",
    "        dataset_type (str, optional): Type of dataset for 'untrained' models ('torch_vision_dataset' or \n",
    "                                      'custom_dataset').\n",
    "        custom_dataset (str, optional): Path to the custom dataset file for 'untrained' models with 'custom_dataset' \n",
    "                                        dataset type.\n",
    "        torch_vision_dataset (str, optional): Name of the TorchVision dataset class for 'untrained' models with \n",
    "                                              'torch_vision_dataset' dataset type.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with project setup information including project name, folder path, model details, \n",
    "              and dataset information.\n",
    "    \"\"\"\n",
    "    log_message(\"Setting up project\")\n",
    "    working_folder = \"/home/fastqnn/finn/notebooks/Fast-QNN/outputs/txaviour/\"\n",
    "    prj_info = {}\n",
    "    \n",
    "    # Ensure Project Name is provided\n",
    "    if not prj_name:\n",
    "        log_message(\"Project Name is required\", level=\"error\")\n",
    "\n",
    "    # Sanitize project name and set project info\n",
    "    prj_name_stripped = sanitize_string(prj_name)\n",
    "    display_name = prj_name\n",
    "    prj_info[\"Display_Name\"] = display_name\n",
    "    prj_info[\"Stripped_Name\"] = prj_name_stripped\n",
    "\n",
    "    if not project_folder:\n",
    "        # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") # for production mode\n",
    "        timestamp = \"0\"\n",
    "        project_folder = f\"{working_folder}{prj_name_stripped}_{timestamp}\"\n",
    "    \n",
    "    folder_path = ensure_directory_exists(project_folder)\n",
    "    prj_info[\"Folder\"] = folder_path\n",
    "    \n",
    "    available_boards = pynq_part_map.keys()\n",
    "    if brd_name in available_boards:\n",
    "        prj_info[\"Board_name\"] = brd_name\n",
    "    else:\n",
    "        log_message(f\"'{brd_name}' is not a valid board name. Available board names are: {available_boards}\", level=\"error\")\n",
    "\n",
    "    # Validate Model Type\n",
    "    valid_model_types = [\"untrained\", \"sample_untrained\", \"custom_pretrained\", \"sample_pretrained\", \"finn_pretrained\"]\n",
    "    if model_type not in valid_model_types:\n",
    "        log_message(f\"Model Type must be one of {valid_model_types}\", level=\"error\")\n",
    "    prj_info['Model_Type'] = model_type\n",
    "    \n",
    "    # Handle Model Type specific requirements\n",
    "    if model_type in [\"untrained\", \"custom_pretrained\"]:\n",
    "        if not model_py_file or not check_file_exists(os.path.join(project_folder, 'src', model_py_file)):\n",
    "            log_message(f\"Model Py File '{model_py_file}' does not exist in '{project_folder}'\", level=\"error\")\n",
    "        prj_info[\"Model_Py_File\"] = model_py_file\n",
    "\n",
    "    if model_type == \"custom_pretrained\":\n",
    "        if not model_pth_file or not check_file_exists(os.path.join(project_folder, 'src', model_pth_file)):\n",
    "            log_message(f\"Model Pth File '{model_pth_file}' does not exist in '{project_folder}'\", level=\"error\")\n",
    "        prj_info[\"Model_Pth_File\"] = model_pth_file\n",
    "\n",
    "    if model_type == \"sample_pretrained\":\n",
    "        available_models = [\"alexnet_3w3a_cifar10\", \"alexnet_3w3a_mnist\"]\n",
    "        if not sample_pretrained_model or sample_pretrained_model not in available_models:\n",
    "            log_message(f\"Sample Pretrained Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Pretrained_Model\"] = sample_pretrained_model\n",
    "        \n",
    "    if model_type == \"sample_untrained\":\n",
    "        available_models = [\"alexnet\", \"resnet\"]\n",
    "        if not sample_untrained_model or sample_untrained_model not in available_models:\n",
    "            log_message(f\"Sample Untrained Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Sample_Untrained_Model\"] = sample_untrained_model\n",
    "\n",
    "    if model_type == \"finn_pretrained\":\n",
    "        available_models = [\"cnv_1w1a\", \"cnv_1w2a\", \"cnv_2w2a\", \"lfc_1w1a\", \"lfc_1w2a\", \"sfc_1w1a\", \"sfc_1w2a\", \"sfc_2w2a\", \"tfc_1w1a\", \"tfc_1w2a\", \"tfc_2w2a\", \"quant_mobilenet_v1_4b\"]\n",
    "        if not finn_pretrained_model or finn_pretrained_model not in available_models:\n",
    "            log_message(f\"Finn Pretrained Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Pretrained_Model\"] = finn_pretrained_model\n",
    "\n",
    "    # Handle Dataset requirements for Untrained models\n",
    "    if model_type == \"untrained\" or model_type == \"sample_untrained\":\n",
    "        if dataset_type not in [\"torch_vision_dataset\", \"custom_dataset\"]:\n",
    "            log_message(\"Dataset Type must be either 'Torch Vision' or 'Custom Dataset' for Untrained models\", level=\"error\")\n",
    "        prj_info[\"Dataset_Type\"] = dataset_type\n",
    "\n",
    "        if dataset_type == \"custom_dataset\":\n",
    "            custom_dataset_path = os.path.join(project_folder, 'dataset', custom_dataset)\n",
    "            if not custom_dataset or not check_file_exists(custom_dataset_path) or not is_archive_file(custom_dataset_path):\n",
    "                log_message(f\"Custom Dataset '{custom_dataset}' must exist in '{project_folder}' and be an archive file (zip or tar)\", level=\"error\")\n",
    "            prj_info[\"Custom_Dataset\"] = custom_dataset\n",
    "\n",
    "        elif dataset_type == \"torch_vision_dataset\":\n",
    "            available_datasets = [cls_name.lower() for cls_name in dir(datasets) if not cls_name.startswith('_')]\n",
    "            if not torch_vision_dataset or torch_vision_dataset.lower() not in available_datasets:\n",
    "                log_message(f\"Torch Vision Dataset must be one of: {available_datasets}\", level=\"error\")\n",
    "            prj_info[\"Torch_Vision_Dataset\"] = torch_vision_dataset\n",
    "            \n",
    "    log_message(f\"Project setup complete. {prj_name} has been initialized.\")\n",
    "    return prj_info"
   ],
   "id": "4df70eb120061ecb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "\n",
    "class QuantAlexNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A slightly adapted AlexNet-like architecture for e.g. MNIST-like inputs.\n",
    "    Uses Brevitas quantization layers (QuantConv2d, QuantLinear, QuantReLU).\n",
    "    Explicitly sets 'weight_quant_type' and 'quant_type' so that FINN can parse them.\n",
    "    Layer-by-layer structure (no nn.Sequential).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_bits=1, num_classes=10):\n",
    "        super(QuantAlexNet, self).__init__()\n",
    "\n",
    "        # Decide on quantization type based on bit-width\n",
    "        if num_bits == 1:\n",
    "            weight_quant_type = QuantType.BINARY\n",
    "            act_quant_type = QuantType.BINARY\n",
    "        else:\n",
    "            weight_quant_type = QuantType.INT\n",
    "            act_quant_type = QuantType.INT\n",
    "\n",
    "        # ---------------------------\n",
    "        #  CONVOLUTIONAL FEATURES\n",
    "        # ---------------------------\n",
    "\n",
    "        # 1st Conv block\n",
    "        self.conv1 = qnn.QuantConv2d(\n",
    "            in_channels=1, out_channels=64, kernel_size=11, stride=4, padding=2,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act1 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 2nd Conv block\n",
    "        self.conv2 = qnn.QuantConv2d(\n",
    "            in_channels=64, out_channels=192, kernel_size=5, padding=2,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act2 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3rd Conv block\n",
    "        self.conv3 = qnn.QuantConv2d(\n",
    "            in_channels=192, out_channels=384, kernel_size=3, padding=1,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act3 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "\n",
    "        # 4th Conv block\n",
    "        self.conv4 = qnn.QuantConv2d(\n",
    "            in_channels=384, out_channels=256, kernel_size=3, padding=1,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act4 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "\n",
    "        # 5th Conv block\n",
    "        self.conv5 = qnn.QuantConv2d(\n",
    "            in_channels=256, out_channels=256, kernel_size=3, padding=1,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act5 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "        # Skipping final MaxPool because of the small input size (28x28).\n",
    "\n",
    "        # ---------------------------\n",
    "        #    CLASSIFIER LAYERS\n",
    "        # ---------------------------\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc1 = qnn.QuantLinear(\n",
    "            256, 4096,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act_fc1 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = qnn.QuantLinear(\n",
    "            4096, 4096,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act_fc2 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "\n",
    "        self.fc3 = qnn.QuantLinear(\n",
    "            4096, num_classes,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st Conv block\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # 2nd Conv block\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # 3rd Conv block\n",
    "        x = self.conv3(x)\n",
    "        x = self.act3(x)\n",
    "\n",
    "        # 4th Conv block\n",
    "        x = self.conv4(x)\n",
    "        x = self.act4(x)\n",
    "\n",
    "        # 5th Conv block\n",
    "        x = self.conv5(x)\n",
    "        x = self.act5(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Classifier\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fc1(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act_fc2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "8240186d1a3b0798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.util.test import get_test_model_trained\n",
    "\n",
    "\n",
    "def load_pretrained_model(model_name, model_type, src_folder, initial_channels = 3, max_size = 4096):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model from TorchVision and ensures all downloads are in the src folder of the Project.\n",
    "\n",
    "    Parameters:\n",
    "        model_type: \n",
    "        initial_channels: \n",
    "        max_size: \n",
    "        model_name (str): The name of the pre-trained model to load (e.g., 'alexnet', 'resnet50').\n",
    "        src_folder (str): The folder where model downloads will be stored. Default is 'src'.\n",
    "                \n",
    "    Returns:\n",
    "        torch.nn.Module: The loaded pre-trained model.\n",
    "    \"\"\"\n",
    "    log_message(f\"Loading {model_type} Model: {model_name}\")\n",
    "    # Ensure the src folder exists\n",
    "    os.makedirs(src_folder, exist_ok=True)\n",
    "    # Set TORCH_HOME to the src folder to store the model downloads there\n",
    "    os.environ['TORCH_HOME'] = src_folder\n",
    "    pretrained_model = None\n",
    "    if model_type == \"sample_pretrained\":\n",
    "        if model_name == \"alexnet_3w3a_cifar10\":\n",
    "            pretrained_model = QuantAlexNet(num_bits=3, num_classes=10).to('cpu')\n",
    "            pretrained_model.load_state_dict(torch.load(\"/home/fastqnn/finn/notebooks/Fast-QNN/alexnet_3w3a_cifar10.pth\", map_location=torch.device('cpu')))\n",
    "            pretrained_model.eval()\n",
    "        if model_name == \"alexnet_3w3a_mnist\":\n",
    "            pretrained_model = QuantAlexNet(num_bits=3, num_classes=10).to('cpu')\n",
    "            pretrained_model.load_state_dict(torch.load(\"/home/fastqnn/finn/notebooks/Fast-QNN/alexnet_3w3a_mnist.pth\", map_location=torch.device('cpu')))\n",
    "            pretrained_model.eval()\n",
    "    elif model_type == \"finn_pretrained\":\n",
    "        if model_name == \"cnv_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 1, 1)\n",
    "        elif model_name == \"cnv_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 1, 2)\n",
    "        elif model_name == \"cnv_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 2, 2)\n",
    "        elif model_name == \"lfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"LFC\", 1, 1)\n",
    "        elif model_name == \"lfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"LFC\", 1, 2)\n",
    "        elif model_name == \"sfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 1, 1)\n",
    "        elif model_name == \"sfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 1, 2)\n",
    "        elif model_name == \"sfc_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 2, 2)\n",
    "        elif model_name == \"tfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 1, 1)\n",
    "        elif model_name == \"tfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 1, 2)\n",
    "        elif model_name == \"tfc_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 2, 2)\n",
    "        elif model_name == \"quant_mobilenet_v1_4b\":\n",
    "            pretrained_model = get_test_model_trained(\"mobilenet\", 4, 4)\n",
    "    # List of common input shapes to test first (both square and non-square)\n",
    "    common_shapes = [\n",
    "        (1, initial_channels, 32, 32),  # Typical for many models\n",
    "        (1, initial_channels, 28, 28),  # Typical for many models\n",
    "        (1, initial_channels, 224, 224),  # Typical for many models\n",
    "        (1, initial_channels, 299, 299),  # For models like Inception\n",
    "        (1, initial_channels, 128, 128),  # Smaller size\n",
    "        (1, initial_channels, 256, 256),  # Larger square size\n",
    "        (1, initial_channels, 320, 240),  # Common non-square size\n",
    "        (1, initial_channels, 240, 320),  # Non-square (swapped dimensions)\n",
    "        (1, initial_channels, 256, 128),  # Non-square\n",
    "        (1, initial_channels, 128, 256),  # Non-square (swapped)\n",
    "    ]\n",
    "\n",
    "    # Try common shapes first\n",
    "    for shape in common_shapes:\n",
    "        try:\n",
    "            dummy_input = torch.rand(*shape)\n",
    "            pretrained_model(dummy_input)\n",
    "            log_message(f\"Compatible common input shape found: {shape}\")\n",
    "            return pretrained_model, shape\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "\n",
    "    # If no common shape worked, test all possible square and non-square shapes up to max_size\n",
    "    for width in range(1, max_size + 1, 1):  # Step by 16 for efficiency\n",
    "        for height in range(1, max_size + 1, 1):\n",
    "            try:\n",
    "                dummy_input = torch.rand(1, initial_channels, width, height)\n",
    "                pretrained_model(dummy_input)\n",
    "                pretrained_model_input_shape = (1, initial_channels, width, height)\n",
    "                log_message(f\"Compatible input shape found: {pretrained_model_input_shape}\")\n",
    "                return pretrained_model, pretrained_model_input_shape\n",
    "            except RuntimeError:\n",
    "                continue\n",
    "\n",
    "    log_message(\"Could not determine a compatible input shape within the specified range.\", level=\"error\")"
   ],
   "id": "e912d407f448ecb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b4a8b34b65b86a4",
   "metadata": {},
   "source": [
    "prj_name_input = \"Alexnet 3w3a MNIST\"\n",
    "board_name_input = \"Pynq-Z2\"\n",
    "prj_folder_input = sanitize_string(prj_name_input)\n",
    "model_type_input = \"sample_pretrained\"\n",
    "pretrained_model_name_input = \"alexnet_3w3a_mnist\"\n",
    "Project_Info = setup_project(prj_name=prj_name_input, brd_name=board_name_input, model_type=model_type_input, sample_pretrained_model=pretrained_model_name_input)\n",
    "\n",
    "input_model= None\n",
    "input_model_shape = None\n",
    "\n",
    "if Project_Info['Model_Type'] == \"untrained\":\n",
    "    log_message(\"Training for untrained models are not supported at the moment!\", level=\"error\")\n",
    "elif Project_Info['Model_Type'] == \"custom_pretrained\":\n",
    "    log_message(\"Custom Pretrained models are not supported at the moment!\", level=\"error\")\n",
    "elif Project_Info['Model_Type'] == \"sample_pretrained\" or Project_Info['Model_Type'] == \"finn_pretrained\":\n",
    "    pretrained_folder = os.path.join(Project_Info['Folder'],\"src\")\n",
    "    input_model, input_model_shape = load_pretrained_model(Project_Info['Pretrained_Model'], Project_Info['Model_Type'], pretrained_folder, initial_channels=1)\n",
    "else:\n",
    "    log_message(\"Unsupported Model Type\", level=\"error\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def set_onnx_checkpoint(project_info, suffix):\n",
    "    \"\"\"\n",
    "    Generates the export path for ONNX files based on a specified suffix.\n",
    "\n",
    "    Parameters:\n",
    "        project_info (dict): Dictionary containing project information (e.g., 'Folder' and 'Stripped_Name').\n",
    "        suffix (str): The suffix to append to the exported ONNX filename (e.g., \"model1\" for \"model1_export.onnx\").\n",
    "\n",
    "    Returns:\n",
    "        str: The full path to the export file.\n",
    "    \"\"\"\n",
    "    log_message(f\"Saving Checkpoint: {suffix}\")\n",
    "    suffix = sanitize_string(suffix)\n",
    "    filename = f\"{project_info['Stripped_Name']}_{suffix}.onnx\"\n",
    "    return os.path.join(project_info['Folder'], \"checkpoints\", filename)\n"
   ],
   "id": "1f4ae7b982c2a64c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "export_onnx_path = set_onnx_checkpoint(Project_Info,\"Brevitas Export\")\n",
    "export_qonnx(input_model, torch.randn(input_model_shape), export_onnx_path, opset_version=9)\n",
    "qonnx_cleanup(export_onnx_path, out_file=export_onnx_path)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "model = ModelWrapper(export_onnx_path)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"QONNX to FINN\"))"
   ],
   "id": "de436732503bc809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from qonnx.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from qonnx.transformation.remove import RemoveIdentityOps\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import (GiveReadableTensorNames,\n",
    "                                          GiveUniqueNodeNames,\n",
    "                                          RemoveStaticGraphInputs,\n",
    "                                          RemoveUnusedTensors, GiveUniqueParameterTensors, SortGraph)\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "def tidy_up_transforms(input_tidy_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a series of transformations to a model and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_tidy_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply transformations\n",
    "    input_tidy_model = input_tidy_model.transform(GiveUniqueParameterTensors())\n",
    "    input_tidy_model = input_tidy_model.transform(InferShapes())\n",
    "    input_tidy_model = input_tidy_model.transform(FoldConstants())\n",
    "    input_tidy_model = input_tidy_model.transform(GiveUniqueNodeNames())\n",
    "    input_tidy_model = input_tidy_model.transform(GiveReadableTensorNames())\n",
    "    input_tidy_model = input_tidy_model.transform(InferDataTypes())\n",
    "    input_tidy_model = input_tidy_model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "    # Save the transformed model\n",
    "    input_tidy_model.save(save_name)\n",
    "\n",
    "    return input_tidy_model"
   ],
   "id": "a5d959d6407a1ec2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = tidy_up_transforms(model, set_onnx_checkpoint(Project_Info,\"Tidy ONNX Post Finn\"))",
   "id": "21fb27ba948a8cbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.util.pytorch import ToTensor\n",
    "\n",
    "log_message(\"Skipping Pre-Processing. Will be expecting the user to handle it in application!\", level=\"warning\")"
   ],
   "id": "511d18741790a4a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = set_onnx_checkpoint(Project_Info,\"Pre Proc ONNX Finn\")\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name, opset_version=9)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\"\"\""
   ],
   "id": "57af62ce8ef3c010",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiplyByOne(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = x * 2\n",
    "        out = out + 0  # Adding a no-op to prevent graph simplification\n",
    "        out = out / 2  # Adding a no-op to prevent graph simplification\n",
    "        return out\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "mul_model = MultiplyByOne()\n",
    "\n",
    "chkpt_mul_name = set_onnx_checkpoint(Project_Info, \"Mul_One_ONNX\")\n",
    "export_qonnx(mul_model, torch.randn(ishape), chkpt_mul_name, opset_version=9)\n",
    "qonnx_cleanup(chkpt_mul_name, out_file=chkpt_mul_name)\n",
    "\n",
    "mul_model_wrapper = ModelWrapper(chkpt_mul_name)\n",
    "mul_model_wrapper = mul_model_wrapper.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(MergeONNXModels(mul_model_wrapper))\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])"
   ],
   "id": "13c356373544a8a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"Post Processing\"))\n",
    "model = tidy_up_transforms(model, set_onnx_checkpoint(Project_Info,\"Tidy Post PrePost Proc\"))"
   ],
   "id": "49bbd723443c4e65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from finn.transformation.qonnx.qonnx_activation_handlers import QuantActBaseHandler, np_default_dtype\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "\n",
    "class ReluToMultiThresholdHandler(QuantActBaseHandler):\n",
    "    \"\"\"Class for converting a regular non-quantized ReLU operation to a MultiThreshold node in Xilinx FINN.\"\"\"\n",
    "    def _check_compatibility(self):\n",
    "        # Check if the ReLU operation is supported for transformation\n",
    "        pass\n",
    "\n",
    "    def _calculate_act_bias(self):\n",
    "        # No bias is applied for standard ReLU\n",
    "        return np.array([0.0], dtype=np_default_dtype)\n",
    "\n",
    "    def _calculate_thresholds(self):\n",
    "        # Thresholds for ReLU are typically defined as a single zero threshold\n",
    "        output_shape = self._model.get_tensor_shape(self._q_node.output[0])\n",
    "        num_output_channels = output_shape[1]  # Assuming NCHW or NHWC format\n",
    "        thresholds = np.zeros((num_output_channels, 1), dtype=np_default_dtype)\n",
    "        return thresholds\n",
    "\n",
    "    def _calculate_act_scale(self):\n",
    "        # Standard ReLU does not apply scaling\n",
    "        return np.array([1.0], dtype=np_default_dtype)\n",
    "\n",
    "    def _remove_activation_node(self, multi_threshold_node):\n",
    "        # Find the activation node\n",
    "        act_node = self._model.find_direct_predecessors(self._q_node)\n",
    "        if act_node is None:\n",
    "            raise RuntimeError(\n",
    "                \"For handling of Relu activations a predecesor to dgdgdg\"\n",
    "            )\n",
    "        act_node = act_node[0]\n",
    "        if act_node.op_type != \"Relu\":\n",
    "            raise RuntimeError(\n",
    "                \"The predecesor of the Quant node must be Relu for handling \"\n",
    "                \"of activations.\"\n",
    "            )\n",
    "\n",
    "        # Remove the activation node\n",
    "        self._model.graph.node.remove(act_node)\n",
    "        return multi_threshold_node\n",
    "\n",
    "    def calculate_node_parameters(self):\n",
    "        # Calculate all parameters for replacing ReLU with MultiThreshold\n",
    "        return {\n",
    "            \"out_dtype\": \"FLOAT32\",  # Regular ReLU typically works with float32\n",
    "            \"thresholds\": self._calculate_thresholds(),\n",
    "            \"adder_bias\": self._calculate_act_bias(),\n",
    "            \"mul_scale\": self._calculate_act_scale(),\n",
    "        }\n",
    "\n",
    "    def replace_relu_node(self):\n",
    "        \"\"\"Replace a regular ReLU operation with a MultiThreshold node.\"\"\"\n",
    "        # Check compatibility\n",
    "        self._check_compatibility()\n",
    "\n",
    "        # Shorten instance variables\n",
    "        model = self._model\n",
    "        graph = model.graph\n",
    "        n = self._q_node\n",
    "        running_node_index = self._q_index\n",
    "\n",
    "        # Calculate parameters for the MultiThreshold node\n",
    "        parameter_dict = self.calculate_node_parameters()\n",
    "        thresholds = parameter_dict[\"thresholds\"]\n",
    "        adder_bias = parameter_dict[\"adder_bias\"]\n",
    "        mul_scale = parameter_dict[\"mul_scale\"]\n",
    "        out_dtype = parameter_dict[\"out_dtype\"]\n",
    "\n",
    "        # Create threshold tensor\n",
    "        thresh_tensor = oh.make_tensor_value_info(\n",
    "            model.make_new_valueinfo_name(),\n",
    "            TensorProto.FLOAT,\n",
    "            thresholds.shape,\n",
    "        )\n",
    "        graph.value_info.append(thresh_tensor)\n",
    "        model.set_initializer(thresh_tensor.name, thresholds)\n",
    "        act_node = self._model.find_direct_predecessors(self._q_node)\n",
    "        act_node = act_node[0]\n",
    "        # Insert MultiThreshold node\n",
    "        mt_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            [act_node.input[0], thresh_tensor.name],\n",
    "            [n.output[0]],\n",
    "            out_dtype=out_dtype,\n",
    "            domain=\"qonnx.custom_op.general\",\n",
    "        )\n",
    "        graph.node.insert(running_node_index, mt_node)\n",
    "        running_node_index += 1\n",
    "\n",
    "        # Get MultiThreshold instance\n",
    "        mt_inst = getCustomOp(graph.node[running_node_index - 1])\n",
    "\n",
    "        # Set bias and scale attributes\n",
    "        mt_inst.set_nodeattr(\"out_scale\", mul_scale[0].item())\n",
    "        mt_inst.set_nodeattr(\"out_bias\", adder_bias[0].item())\n",
    "        mt_inst.set_nodeattr(\"out_dtype\", out_dtype)\n",
    "\n",
    "        # Remove the original ReLU node\n",
    "        self._remove_activation_node(mt_node)\n",
    "        graph.node.remove(n)\n",
    "\n",
    "        # Return the updated model\n",
    "        return self._model\n"
   ],
   "id": "a7dc93abb28939d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.qonnx.qonnx_activation_handlers import QuantReluHandler\n",
    "from onnx import TensorProto\n",
    "from qonnx.transformation.base import Transformation\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from onnx import helper as oh\n",
    "\n",
    "class CustomReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Implementing an approximation of abs(x) using only arithmetic operations\n",
    "        epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "        abs_x = (x * x + epsilon) / (x + epsilon)\n",
    "        return (x + abs_x) / 2\n",
    "\n",
    "class CustomReluBreakdown(Transformation):\n",
    "    \"\"\"\n",
    "    Custom transformation to iterate through the nodes, identify ReLU nodes,\n",
    "    and convert the next node to a quant node while applying the QuantReluHandler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bit_width=1, quant_type=DataType[\"UINT8\"], narrow=0, signed=0):\n",
    "        super().__init__()\n",
    "        self.bit_width = bit_width\n",
    "        self.quant_type = quant_type\n",
    "        self.narrow = narrow\n",
    "        self.signed = signed\n",
    "\n",
    "    def apply(self, transform_model):\n",
    "        graph = transform_model.graph\n",
    "        graph_modified = False\n",
    "        convert_next_node = False\n",
    "\n",
    "        for idx, node in enumerate(graph.node):\n",
    "            if convert_next_node:\n",
    "                # Call QuantReluHandler on the current Quant node\n",
    "                handler = ReluToMultiThresholdHandler(transform_model, node, idx)\n",
    "                handler.replace_relu_node()\n",
    "\n",
    "                convert_next_node = False\n",
    "                graph_modified = True\n",
    "\n",
    "            if node.op_type == \"Relu\":\n",
    "                # Raise a flag to convert the next node to a quant node\n",
    "                convert_next_node = True\n",
    "\n",
    "       # transform_model = transform_model.transform(InferShapes())\n",
    "        return (transform_model, graph_modified)\n"
   ],
   "id": "a45466c16fde453c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from onnx import helper as oh\n",
    "from onnx import TensorProto\n",
    "from qonnx.transformation.base import Transformation\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReluToMultiThresholdTransform(Transformation):\n",
    "    \"\"\"\n",
    "    Custom transformation to replace ReLU nodes with MultiThreshold nodes\n",
    "    in Xilinx FINN. Works directly on ReLU nodes and reroutes their inputs/outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply(self, transform_model):\n",
    "        graph = transform_model.graph\n",
    "        graph_modified = False\n",
    "\n",
    "        for idx, node in enumerate(graph.node):\n",
    "            if node.op_type == \"Relu\":\n",
    "                # Replace the ReLU node with a MultiThreshold node\n",
    "                self.replace_relu_with_multithreshold(transform_model, node, idx)\n",
    "                graph_modified = True\n",
    "\n",
    "        return (transform_model, graph_modified)\n",
    "\n",
    "    def replace_relu_with_multithreshold(self, transform_model, relu_node, relu_index):\n",
    "        \"\"\"\n",
    "        Replaces a ReLU node with a MultiThreshold node.\n",
    "        \"\"\"\n",
    "        graph = transform_model.graph\n",
    "\n",
    "        # Get the input to the ReLU node (its predecessor)\n",
    "        if len(relu_node.input) != 1:\n",
    "            raise RuntimeError(f\"ReLU node {relu_node.name} has unexpected inputs.\")\n",
    "        relu_input = relu_node.input[0]\n",
    "\n",
    "        # Get the output of the ReLU node (its successor)\n",
    "        if len(relu_node.output) != 1:\n",
    "            raise RuntimeError(f\"ReLU node {relu_node.name} has unexpected outputs.\")\n",
    "        relu_output = relu_node.output[0]\n",
    "\n",
    "        # Calculate parameters for MultiThreshold\n",
    "        thresholds = self._calculate_thresholds(transform_model, relu_node)\n",
    "        adder_bias = self._calculate_act_bias()\n",
    "        mul_scale = self._calculate_act_scale()\n",
    "        out_dtype = \"FLOAT32\"\n",
    "\n",
    "        # Create threshold tensor\n",
    "        thresh_tensor = oh.make_tensor_value_info(\n",
    "            transform_model.make_new_valueinfo_name(),\n",
    "            TensorProto.FLOAT,\n",
    "            thresholds.shape,\n",
    "        )\n",
    "        graph.value_info.append(thresh_tensor)\n",
    "        transform_model.set_initializer(thresh_tensor.name, thresholds)\n",
    "\n",
    "        # Create MultiThreshold node\n",
    "        mt_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs=[relu_input, thresh_tensor.name],\n",
    "            outputs=[relu_output],\n",
    "            domain=\"qonnx.custom_op.general\",\n",
    "        )\n",
    "        mt_node.attribute.extend([\n",
    "            oh.make_attribute(\"out_scale\", float(mul_scale[0])),\n",
    "            oh.make_attribute(\"out_bias\", float(adder_bias[0])),\n",
    "            oh.make_attribute(\"out_dtype\", out_dtype),\n",
    "        ])\n",
    "\n",
    "        # Insert MultiThreshold node in place of ReLU node\n",
    "        graph.node.insert(relu_index, mt_node)\n",
    "\n",
    "        # Remove the original ReLU node\n",
    "        graph.node.remove(relu_node)\n",
    "\n",
    "    def _calculate_thresholds(self, transform_model, relu_node):\n",
    "        \"\"\"\n",
    "        Calculates the thresholds for the MultiThreshold node based on the ReLU operation.\n",
    "        \"\"\"\n",
    "        # ReLU threshold is always zero\n",
    "        output_shape = transform_model.get_tensor_shape(relu_node.output[0])\n",
    "        num_output_channels = output_shape[1]  # Assuming NCHW or NHWC format\n",
    "        thresholds = np.zeros((num_output_channels, 1), dtype=np.float32)\n",
    "        return thresholds\n",
    "\n",
    "    def _calculate_act_bias(self):\n",
    "        \"\"\"\n",
    "        Calculates the bias for the MultiThreshold node.\n",
    "        \"\"\"\n",
    "        # ReLU does not apply a bias\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "\n",
    "    def _calculate_act_scale(self):\n",
    "        \"\"\"\n",
    "        Calculates the scale for the MultiThreshold node.\n",
    "        \"\"\"\n",
    "        # ReLU does not apply scaling\n",
    "        return np.array([1.0], dtype=np.float32)\n"
   ],
   "id": "53fa26aa1d5bd4e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import traceback\n",
    "from qonnx.transformation.batchnorm_to_affine import BatchNormToAffine\n",
    "from qonnx.transformation.general import ConvertSubToAdd, ConvertDivToMul\n",
    "from finn.transformation.streamline import Streamline, RoundAndClipThresholds, CollapseRepeatedMul, ConvertSignToThres, \\\n",
    "    CollapseRepeatedAdd\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from finn.transformation.streamline.absorb import (AbsorbTransposeIntoMultiThreshold,\n",
    "                                                   AbsorbScalarMulAddIntoTopK,\n",
    "                                                   AbsorbSignBiasIntoMultiThreshold,\n",
    "                                                   AbsorbAddIntoMultiThreshold,\n",
    "                                                   AbsorbMulIntoMultiThreshold, FactorOutMulSignMagnitude,\n",
    "                                                   Absorb1BitMulIntoMatMul, Absorb1BitMulIntoConv)\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants, MoveAddPastMul, \\\n",
    "    MoveScalarAddPastMatMul, MoveAddPastConv, MoveScalarMulPastMatMul, MoveScalarMulPastConv, \\\n",
    "    MoveMaxPoolPastMultiThreshold, MoveLinearPastEltwiseAdd, MoveLinearPastFork\n",
    "from finn.transformation.streamline.absorb import AbsorbConsecutiveTransposes\n",
    "\n",
    "def streamline_transforms(input_streamline_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a series of streamlining transformations to a model and saves the resulting model.\n",
    "    If a transformation fails, it logs the error and moves on to the next transformation.\n",
    "\n",
    "    Parameters:\n",
    "        input_streamline_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        AbsorbSignBiasIntoMultiThreshold(),\n",
    "        ConvertSubToAdd(),\n",
    "        ConvertDivToMul(),\n",
    "        CollapseRepeatedMul(),\n",
    "        BatchNormToAffine(),\n",
    "        ConvertSignToThres(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarAddPastMatMul(),\n",
    "        MoveAddPastConv(),\n",
    "        MoveScalarMulPastMatMul(),\n",
    "        MoveScalarMulPastConv(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarLinearPastInvariants(),\n",
    "        CollapseRepeatedAdd(),\n",
    "        AbsorbAddIntoMultiThreshold(),\n",
    "        FactorOutMulSignMagnitude(),\n",
    "        MoveMaxPoolPastMultiThreshold(),\n",
    "        AbsorbMulIntoMultiThreshold(),\n",
    "        Absorb1BitMulIntoMatMul(),\n",
    "        Absorb1BitMulIntoConv(),\n",
    "        AbsorbMulIntoMultiThreshold(),\n",
    "        Streamline(),\n",
    "        LowerConvsToMatMul(),\n",
    "        MakeMaxPoolNHWC(),\n",
    "        AbsorbTransposeIntoMultiThreshold(),\n",
    "        ConvertBipolarMatMulToXnorPopcount(),\n",
    "        Streamline(),\n",
    "        AbsorbScalarMulAddIntoTopK(),\n",
    "        RoundAndClipThresholds(),\n",
    "        MoveLinearPastEltwiseAdd(),\n",
    "        MoveLinearPastFork()\n",
    "    ]\n",
    "    \n",
    "    input_streamline_model = input_streamline_model.transform(ReluToMultiThresholdTransform())\n",
    "    input_streamline_model = tidy_up_transforms(input_streamline_model, save_name)\n",
    "    for iter_id in range(3):\n",
    "        for transform in transformations:\n",
    "            try:\n",
    "                input_streamline_model = input_streamline_model.transform(transform)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "        input_streamline_model = tidy_up_transforms(input_streamline_model, save_name)\n",
    "        \n",
    "        input_streamline_model = input_streamline_model.transform(InferDataLayouts())\n",
    "        input_streamline_model = input_streamline_model.transform(RemoveUnusedTensors())\n",
    "        input_streamline_model = input_streamline_model.transform(DoubleToSingleFloat())\n",
    "        input_streamline_model = input_streamline_model.transform(SortGraph())\n",
    "        input_streamline_model = input_streamline_model.transform(RemoveIdentityOps())\n",
    "    \n",
    "    return input_streamline_model"
   ],
   "id": "6bdb92934093ceb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = streamline_transforms(model, set_onnx_checkpoint(Project_Info,\"Streamlined ONNX\"))",
   "id": "d969950c0d96fc53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.fpgadataflow.convert_to_hw_layers import (\n",
    "    InferBinaryMatrixVectorActivation,\n",
    "    InferQuantizedMatrixVectorActivation,\n",
    "    InferLabelSelectLayer,\n",
    "    InferThresholdingLayer,\n",
    "    InferStreamingMaxPool,\n",
    "    InferConvInpGen,\n",
    "    InferAddStreamsLayer,\n",
    "    InferChannelwiseLinearLayer,\n",
    "    InferConcatLayer,\n",
    "    InferDuplicateStreamsLayer,\n",
    "    InferGlobalAccPoolLayer,\n",
    "    InferLookupLayer,\n",
    "    InferPool,\n",
    "    InferStreamingEltwise,\n",
    "    InferUpsample,\n",
    "    InferVectorVectorActivation\n",
    ")\n",
    "\n",
    "def to_hw_transforms(input_hw_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a comprehensive series of hardware-oriented transformations to a model and saves the resulting model.\n",
    "    If a transformation fails, it logs the error and moves on to the next transformation.\n",
    "\n",
    "    Parameters:\n",
    "        input_hw_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        InferBinaryMatrixVectorActivation(),\n",
    "        InferQuantizedMatrixVectorActivation(),\n",
    "        InferLabelSelectLayer(),\n",
    "        InferThresholdingLayer(),\n",
    "        InferConvInpGen(),\n",
    "        InferStreamingMaxPool(),\n",
    "        InferAddStreamsLayer(),\n",
    "        InferChannelwiseLinearLayer(),\n",
    "        InferConcatLayer(),\n",
    "        InferDuplicateStreamsLayer(),\n",
    "        InferGlobalAccPoolLayer(),\n",
    "        InferLookupLayer(),\n",
    "        InferPool(),\n",
    "        InferStreamingEltwise(),\n",
    "        InferUpsample(),\n",
    "        InferVectorVectorActivation(),\n",
    "        RemoveCNVtoFCFlatten(),\n",
    "        AbsorbConsecutiveTransposes()\n",
    "    ]\n",
    "\n",
    "    for iter_id in range(3):\n",
    "        for transform in transformations:\n",
    "            try:\n",
    "                input_hw_model = input_hw_model.transform(transform)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "        # Apply final tidy-up transformations\n",
    "        input_hw_model = tidy_up_transforms(input_hw_model, save_name)\n",
    "        \n",
    "        input_hw_model = input_hw_model.transform(InferDataLayouts())\n",
    "        input_hw_model = input_hw_model.transform(RemoveUnusedTensors())\n",
    "        input_hw_model = input_hw_model.transform(DoubleToSingleFloat())\n",
    "        input_hw_model = input_hw_model.transform(SortGraph())\n",
    "        input_hw_model = input_hw_model.transform(RemoveIdentityOps())\n",
    "\n",
    "    # Save the final transformed model\n",
    "    input_hw_model.save(save_name)\n",
    "\n",
    "    return input_hw_model"
   ],
   "id": "e34005da6e651b95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = to_hw_transforms(model, set_onnx_checkpoint(Project_Info,\"To HW Layers\"))",
   "id": "3231949dbb03adb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "\n",
    "def dataflow_partitioning(input_data_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies dataflow partitioning transformation to the model and saves the resulting parent and dataflow models.\n",
    "\n",
    "    Parameters:\n",
    "        input_data_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The directory to save the transformed models.\n",
    "\n",
    "    Returns:\n",
    "        ModelWrapper: The transformed parent model.\n",
    "    \"\"\"\n",
    "    # Apply dataflow partitioning\n",
    "    parent_model = input_data_model.transform(CreateDataflowPartition())\n",
    "    parent_model.save(save_name)\n",
    "    \n",
    "    # Retrieve the dataflow partition model filename\n",
    "    sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "    sdp_node = getCustomOp(sdp_node)\n",
    "    dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "    # Load and return the dataflow model\n",
    "    dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "    \n",
    "    return dataflow_model"
   ],
   "id": "7b896786bf7eccae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = dataflow_partitioning(model, set_onnx_checkpoint(Project_Info,\"Dataflow Partition Parent Model\"))\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"Dataflow Partition Streaming Model\"))"
   ],
   "id": "a18f4bf215064d2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "\n",
    "def specialize_layers_transform(input_specialize_model, board_name, save_name):\n",
    "    \"\"\"\n",
    "    Applies layer specialization transformation to a dataflow model for the specified FPGA part and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_specialize_model (ModelWrapper): The dataflow model to transform.\n",
    "        board_name (str): The FPGA board for which to specialize the layers.\n",
    "        save_name (str): The path to save the specialized model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed and specialized dataflow model.\n",
    "    \"\"\"\n",
    "    fpga_part = pynq_part_map[board_name]\n",
    "    # Apply specialization for FPGA layers\n",
    "    input_specialize_model = input_specialize_model.transform(SpecializeLayers(fpga_part))\n",
    "\n",
    "    # Save the specialized model\n",
    "    input_specialize_model.save(save_name)\n",
    "\n",
    "    return input_specialize_model"
   ],
   "id": "29068574207cfe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = specialize_layers_transform(model, Project_Info['Board_name'], set_onnx_checkpoint(Project_Info,f\"Specialize Model Layers to {Project_Info['Board_name']}\"))",
   "id": "66573cba9e8b29aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# model = ModelWrapper(\"/home/fastqnn/finn/notebooks/Fast-QNN/outputs/txaviour/alexnet_3w3a_mnist_0/checkpoints/alexnet_3w3a_mnist_specialize_model_layers_to_pynqzu.onnx\")",
   "id": "eb11b8cb9d541fbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "import os\n",
    "from finn.transformation.fpgadataflow.set_folding import SetFolding\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "from finn.transformation.fpgadataflow.insert_iodma import InsertIODMA\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_int_attr(node_inst, attr_name = None):\n",
    "    \"\"\"\n",
    "    Safely retrieves an integer attribute from a node.\n",
    "\n",
    "    Parameters:\n",
    "        node_inst: The node instance.\n",
    "        attr_name (str): The name of the attribute to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        int: The integer value of the attribute.\n",
    "    \"\"\"\n",
    "    if attr_name is not None:\n",
    "        attr = node_inst.get_nodeattr(attr_name)\n",
    "    else:\n",
    "        attr = node_inst\n",
    "    if isinstance(attr, (tuple, list)):\n",
    "        if len(attr) == 0:\n",
    "            raise ValueError(f\"Attribute '{attr_name}' for node '{node_inst.name}' is an empty tuple/list.\")\n",
    "        return get_int_attr(attr[0])\n",
    "    elif isinstance(attr, int):\n",
    "        return attr\n",
    "    else:\n",
    "        raise TypeError(f\"Attribute '{attr_name}' for node '{node_inst.name}' is of unsupported type {type(attr)}.\")\n",
    "\n",
    "\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "\n",
    "def generate_valid_folding_factors(folding_model, max_pe=32, max_simd=32, max_bitwidth=8191, min_infifodepth=8, max_infifodepth=128):\n",
    "    \"\"\"\n",
    "    Generates a folding configuration dictionary ensuring all constraints are met,\n",
    "    including specific SIMD requirements from HLS synthesis.\n",
    "\n",
    "    Parameters:\n",
    "        max_infifodepth:\n",
    "        min_infifodepth:\n",
    "        folding_model (ModelWrapper): The FINN model.\n",
    "        max_pe (int): Maximum allowed PE value.\n",
    "        max_simd (int): Maximum allowed SIMD value.\n",
    "        max_bitwidth (int): Maximum allowed stream width in bits.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with node names as keys and their folding parameters as values.\n",
    "    \"\"\"\n",
    "    folding_config = {}\n",
    "\n",
    "    # Process MVAU_hls layers\n",
    "    fc_layers = folding_model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "    for node in fc_layers:\n",
    "        node_inst = getCustomOp(node)\n",
    "        node_name = node.name\n",
    "\n",
    "        # Safely retrieve attributes\n",
    "        try:\n",
    "            MW = get_int_attr(node_inst, \"MW\")  # Number of input features\n",
    "            MH = get_int_attr(node_inst, \"MH\")  # Number of output features\n",
    "        except (ValueError, TypeError) as e:\n",
    "            log_message(f\"Error retrieving attributes for node '{node_name}': {e}\", \"error\")\n",
    "            continue  # Skip this node or handle as needed\n",
    "\n",
    "        input_bitwidth = node_inst.get_input_datatype().bitwidth()\n",
    "        output_bitwidth = node_inst.get_output_datatype().bitwidth()\n",
    "\n",
    "        # Calculate minimum required SIMD based on HLS constraint\n",
    "        min_simd = math.ceil(MW / 1024)\n",
    "\n",
    "        # Determine valid SIMD: largest divisor of MW <= max_simd and ensures (SIMD * input_bitwidth) <= max_bitwidth\n",
    "        simd_candidates = [\n",
    "            x for x in range(min_simd, min(max_simd, MW) + 1)\n",
    "            if MW % x == 0 and (x * input_bitwidth) <= max_bitwidth\n",
    "        ]\n",
    "        simd = max(simd_candidates) if simd_candidates else min_simd  # Ensure at least min_simd\n",
    "\n",
    "        # Determine valid PE: largest divisor of MH <= max_pe and ensures (PE * output_bitwidth) <= max_bitwidth\n",
    "        pe_candidates = [\n",
    "            x for x in range(1, min(max_pe, MH) + 1)\n",
    "            if MH % x == 0 and (x * output_bitwidth) <= max_bitwidth\n",
    "        ]\n",
    "        pe = max(pe_candidates) if pe_candidates else 1\n",
    "\n",
    "        if pe * simd > 16:\n",
    "            infifodepth = min(max_infifodepth, 32)  # Example: Set to 32 if high parallelism\n",
    "        else:\n",
    "            infifodepth = max(min_infifodepth, 8)\n",
    "\n",
    "        max_possible_infifodepth = max_bitwidth // (simd * input_bitwidth)\n",
    "        infifodepth = min(infifodepth, max_possible_infifodepth)\n",
    "        infifodepth = max(infifodepth, min_infifodepth)\n",
    "\n",
    "        log_message(f\"Node '{node_name}': Selected infifodepth={infifodepth}\")\n",
    "\n",
    "        folding_config[node_name] = {\n",
    "            \"PE\": pe,\n",
    "            \"SIMD\": simd,\n",
    "            \"infifodepth\": infifodepth\n",
    "        }\n",
    "        log_message(f\"Configured node '{node_name}' with PE={pe}, SIMD={simd}, infifodepth={infifodepth}\")\n",
    "\n",
    "    # Process VVAU layers if any (similar logic can be applied)\n",
    "    vvau_layers = folding_model.get_nodes_by_op_type(\"VVAU\")\n",
    "    for node in vvau_layers:\n",
    "        node_inst = getCustomOp(node)\n",
    "        node_name = node.name\n",
    "\n",
    "        # Safely retrieve attributes\n",
    "        try:\n",
    "            MW = get_int_attr(node_inst, \"MW\")  # Number of input features\n",
    "            MH = get_int_attr(node_inst, \"MH\")  # Number of output features\n",
    "        except (ValueError, TypeError) as e:\n",
    "            log_message(f\"Error retrieving attributes for node '{node_name}': {e}\", \"error\")\n",
    "            continue  # Skip this node or handle as needed\n",
    "\n",
    "        input_bitwidth = node_inst.get_input_datatype().bitwidth()\n",
    "        output_bitwidth = node_inst.get_output_datatype().bitwidth()\n",
    "\n",
    "        # Calculate minimum required SIMD based on HLS constraint\n",
    "        min_simd = math.ceil(MW / 1024)\n",
    "\n",
    "        # Determine valid SIMD and PE, ensuring SWG SIMD == VVAU PE if window-parallelism is enabled\n",
    "        simd_candidates = [\n",
    "            x for x in range(min_simd, min(max_simd, MW) + 1)\n",
    "            if MW % x == 0 and (x * input_bitwidth) <= max_bitwidth\n",
    "        ]\n",
    "        simd = max(simd_candidates) if simd_candidates else min_simd  # Ensure at least min_simd\n",
    "\n",
    "        pe_candidates = [\n",
    "            x for x in range(1, min(max_pe, MH) + 1)\n",
    "            if MH % x == 0 and (x * output_bitwidth) <= max_bitwidth\n",
    "        ]\n",
    "        pe = max(pe_candidates) if pe_candidates else 1\n",
    "\n",
    "        # Ensure SWG SIMD == VVAU PE (as per VVAU constraints in window-parallelism)\n",
    "        pe = simd\n",
    "\n",
    "        if pe * simd > 16:\n",
    "            infifodepth = min(max_infifodepth, 32)  # Example: Set to 32 if high parallelism\n",
    "        else:\n",
    "            infifodepth = max(min_infifodepth, 8)\n",
    "\n",
    "        max_possible_infifodepth = max_bitwidth // (simd * input_bitwidth)\n",
    "        infifodepth = min(infifodepth, max_possible_infifodepth)\n",
    "        infifodepth = max(infifodepth, min_infifodepth)\n",
    "\n",
    "        log_message(f\"Node '{node_name}': Selected infifodepth={infifodepth}\")\n",
    "\n",
    "        folding_config[node_name] = {\n",
    "            \"PE\": pe,\n",
    "            \"SIMD\": simd,\n",
    "            \"infifodepth\": infifodepth\n",
    "        }\n",
    "        log_message(f\"Configured node '{node_name}' with PE={pe}, SIMD={simd}, infifodepth={infifodepth}\")\n",
    "\n",
    "    return folding_config\n",
    "\n",
    "def apply_folding_config(folding_model, folding_config):\n",
    "    \"\"\"\n",
    "    Applies the folding configuration to the model.\n",
    "\n",
    "    Parameters:\n",
    "        folding_model (ModelWrapper): The FINN model.\n",
    "        folding_config (dict): Folding configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        ModelWrapper: The model with updated folding parameters.\n",
    "    \"\"\"\n",
    "    # Apply folding to MVAU_hls layers\n",
    "    fc_layers = folding_model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "    for node in fc_layers:\n",
    "        node_name = node.name\n",
    "        if node_name in folding_config:\n",
    "            config = folding_config[node_name]\n",
    "            node_inst = getCustomOp(node)\n",
    "            node_inst.set_nodeattr(\"PE\", config[\"PE\"])\n",
    "            node_inst.set_nodeattr(\"SIMD\", config[\"SIMD\"])\n",
    "            log_message(f\"Updated node '{node_name}' with PE={config['PE']} and SIMD={config['SIMD']}\")\n",
    "\n",
    "    # Apply folding to VVAU layers\n",
    "    vvau_layers = folding_model.get_nodes_by_op_type(\"VVAU\")\n",
    "    for node in vvau_layers:\n",
    "        node_name = node.name\n",
    "        if node_name in folding_config:\n",
    "            config = folding_config[node_name]\n",
    "            node_inst = getCustomOp(node)\n",
    "            node_inst.set_nodeattr(\"PE\", config[\"PE\"])\n",
    "            node_inst.set_nodeattr(\"SIMD\", config[\"SIMD\"])\n",
    "            log_message(f\"Updated node '{node_name}' with PE={config['PE']} and SIMD={config['SIMD']}\")\n",
    "\n",
    "    return folding_model\n",
    "\n",
    "def validate_stream_widths(folding_model, max_bitwidth=8191):\n",
    "    \"\"\"\n",
    "    Validates that all stream widths are within the allowed maximum.\n",
    "\n",
    "    Parameters:\n",
    "        folding_model (ModelWrapper): The FINN model.\n",
    "        max_bitwidth (int): Maximum allowed stream width in bits.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If any stream width exceeds max_bitwidth.\n",
    "    \"\"\"\n",
    "    # Check MVAU_hls layers\n",
    "    fc_layers = folding_model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "    for node in fc_layers:\n",
    "        node_inst = getCustomOp(node)\n",
    "        simd = node_inst.get_nodeattr(\"SIMD\")\n",
    "        pe = node_inst.get_nodeattr(\"PE\")\n",
    "        input_bitwidth = node_inst.get_input_datatype().bitwidth()\n",
    "        output_bitwidth = node_inst.get_output_datatype().bitwidth()\n",
    "\n",
    "        instream_width = simd * input_bitwidth\n",
    "        outstream_width = pe * output_bitwidth\n",
    "\n",
    "        assert instream_width <= max_bitwidth, f\"{node.name} has input stream width {instream_width} bits > {max_bitwidth}\"\n",
    "        assert outstream_width <= max_bitwidth, f\"{node.name} has output stream width {outstream_width} bits > {max_bitwidth}\"\n",
    "\n",
    "    # Check VVAU layers if any\n",
    "    vvau_layers = folding_model.get_nodes_by_op_type(\"VVAU\")\n",
    "    for node in vvau_layers:\n",
    "        node_inst = getCustomOp(node)\n",
    "        simd = node_inst.get_nodeattr(\"SIMD\")\n",
    "        pe = node_inst.get_nodeattr(\"PE\")\n",
    "        input_bitwidth = node_inst.get_input_datatype().bitwidth()\n",
    "        output_bitwidth = node_inst.get_output_datatype().bitwidth()\n",
    "\n",
    "        instream_width = simd * input_bitwidth\n",
    "        outstream_width = pe * output_bitwidth\n",
    "\n",
    "        assert instream_width <= max_bitwidth, f\"{node.name} has input stream width {instream_width} bits > {max_bitwidth}\"\n",
    "        assert outstream_width <= max_bitwidth, f\"{node.name} has output stream width {outstream_width} bits > {max_bitwidth}\"\n",
    "\n",
    "    print(\"All stream widths are within the allowed maximum.\")\n",
    "\n",
    "def insert_dwcs_if_needed(model_dwc):\n",
    "    \"\"\"\n",
    "    Inserts DataWidthConverters (DWCs) where stream widths between consecutive layers do not match.\n",
    "\n",
    "    Parameters:\n",
    "        model_dwc (ModelWrapper): The FINN model.\n",
    "\n",
    "    Returns:\n",
    "        ModelWrapper: The model with DWCs inserted where necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    # This is a simplified approach. Depending on your model, you might need a more sophisticated check.\n",
    "    # InsertStreamConverter handles the insertion of DWCs automatically when stream widths mismatch.\n",
    "    model_dwc = model_dwc.transform(InsertDWC())\n",
    "\n",
    "    return model_dwc\n",
    "\n"
   ],
   "id": "90d653d3382d15b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def folding_transform(input_folding_model: ModelWrapper, save_name: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Applies a user-defined folding configuration (from generate_folding_config) to MVAU_hls and\n",
    "    ConvolutionInputGenerator_rtl layers, then saves the result.\n",
    "\n",
    "    Parameters:\n",
    "        input_folding_model (ModelWrapper): The specialized model to transform.\n",
    "        save_name (str): Path to save the transformed model (e.g., checkpoint).\n",
    "        **kwargs: Pass additional arguments to generate_folding_config (e.g. max_pe, max_simd, etc.)\n",
    "\n",
    "    Returns:\n",
    "        ModelWrapper: The transformed and folded dataflow model.\n",
    "    \"\"\"\n",
    "    max_pe=16\n",
    "    max_simd=16\n",
    "    max_bitwidth=8191\n",
    "    min_infifodepth=8\n",
    "    max_infifodepth=128\n",
    "    # Generate folding config dictionary keyed by node name\n",
    "    input_folding_model = input_folding_model.transform(GiveUniqueNodeNames())\n",
    "\n",
    "    # Generate folding configuration\n",
    "    folding_config = generate_valid_folding_factors(input_folding_model, max_pe=max_pe, max_simd=max_simd, max_bitwidth=max_bitwidth, min_infifodepth=min_infifodepth, max_infifodepth=max_infifodepth)\n",
    "\n",
    "    # Apply folding configuration\n",
    "    input_folding_model = apply_folding_config(input_folding_model, folding_config)\n",
    "\n",
    "    # Validate stream widths\n",
    "    try:\n",
    "        validate_stream_widths(input_folding_model, max_bitwidth=max_bitwidth)\n",
    "    except AssertionError as e:\n",
    "        print(f\"Stream width validation failed: {e}\")\n",
    "        # Optionally, you can implement a fallback strategy here\n",
    "        # For simplicity, we'll reduce SIMD and PE by half and retry\n",
    "        print(\"Reducing max_simd and max_pe by half and retrying...\")\n",
    "        new_max_simd = max(1, max_simd // 2)\n",
    "        new_max_pe = max(1, max_pe // 2)\n",
    "        folding_config = generate_valid_folding_factors(input_folding_model, max_pe=new_max_pe, max_simd=new_max_simd, max_bitwidth=max_bitwidth)\n",
    "        input_folding_model = apply_folding_config(input_folding_model, folding_config)\n",
    "        validate_stream_widths(input_folding_model, max_bitwidth=max_bitwidth)\n",
    "\n",
    "    # Insert DWCs where needed\n",
    "    input_folding_model = insert_dwcs_if_needed(input_folding_model)\n",
    "    input_folding_model.save(save_name)\n",
    "\n",
    "    return input_folding_model\n"
   ],
   "id": "cd5527c3889d3ed2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "fc_layers = model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "# each tuple is (PE, SIMD, in_fifo_depth) for a layer\n",
    "folding = [\n",
    "    (16, 3, [128]),\n",
    "    (32, 32, [128]),\n",
    "    (16, 32, [128]),\n",
    "    (16, 32, [128]),\n",
    "    (4, 32, [81]),\n",
    "    (1, 32, [2]),\n",
    "    (1, 4, [2]),\n",
    "    (1, 8, [128]),\n",
    "    (5, 1, [3]),\n",
    "]\n",
    "for fcl, (pe, simd, ififodepth) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepths\", ififodepth)\n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator_rtl\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model.save(set_onnx_checkpoint(Project_Info, \"Folded Model\"))\n",
    "'''\n",
    "model = folding_transform(model, set_onnx_checkpoint(Project_Info, \"Folded Model\"))\n",
    "model = model.transform(GiveUniqueNodeNames())"
   ],
   "id": "1269bd96fdeb9db3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "\n",
    "def zynq_build_transform(input_zynq_model, save_name, brd_name):\n",
    "    \"\"\"\n",
    "    Applies the ZynqBuild transformation to a model for the specified PYNQ board and clock period.\n",
    "\n",
    "    Parameters:\n",
    "        input_zynq_model (ModelWrapper): Folded model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "        brd_name (str): Name of the PYNQ board to target.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model after applying ZynqBuild.\n",
    "    \"\"\"\n",
    "    target_clk_ns = 10\n",
    "    # Apply ZynqBuild transformation\n",
    "    input_zynq_model = input_zynq_model.transform(ZynqBuild(platform=brd_name, period_ns=target_clk_ns))\n",
    "    \n",
    "    # Save the transformed model\n",
    "    input_zynq_model.save(save_name)\n",
    "\n",
    "    return input_zynq_model\n"
   ],
   "id": "394bed78132471e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "\n",
    "def pynq_driver_transform(input_driver_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies the MakePYNQDriver transformation to the model to generate a PYNQ-compatible driver.\n",
    "\n",
    "    Parameters:\n",
    "        input_driver_model (ModelWrapper): ZynqBuild model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model with PYNQ driver compatibility.\n",
    "    \"\"\"\n",
    "    # Apply MakePYNQDriver transformation\n",
    "    input_driver_model = input_driver_model.transform(MakePYNQDriver(\"zynq-iodma\"))\n",
    "    \n",
    "    # Save the transformed model\n",
    "    input_driver_model.save(save_name)\n",
    "\n",
    "    return input_driver_model\n"
   ],
   "id": "d1f6d7caa00a8452",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model = zynq_build_transform(model, set_onnx_checkpoint(Project_Info, \"Zynq Build\"), Project_Info['Board_name'])\n",
    "model = pynq_driver_transform(model, set_onnx_checkpoint(Project_Info, \"Pynq Driver\"))"
   ],
   "id": "669b7375d6509996",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6c73406abcc4a5fd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
