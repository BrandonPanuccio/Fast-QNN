{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:31.744555Z",
     "start_time": "2024-11-22T01:28:31.731139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.models import list_models\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "def log_message(message, level=\"info\"):\n",
    "    \"\"\"\n",
    "    Log a message, with support for info, warning, and error levels.\n",
    "    If the level is \"error\", this function raises a ValueError with the message.\n",
    "\n",
    "    Parameters:\n",
    "        message (str): The message to log.\n",
    "        level (str): The level of the message (\"info\", \"warning\", or \"error\").\n",
    "    \"\"\"\n",
    "    if level == \"info\":\n",
    "        print(f\"[INFO] {message}\")\n",
    "    elif level == \"warning\":\n",
    "        print(f\"[WARNING] {message}\")\n",
    "    elif level == \"error\":\n",
    "        print(f\"[ERROR] {message}\")\n",
    "        raise ValueError(message)  # Raise an error if level is \"error\"\n",
    "\n",
    "def sanitize_string(s):\n",
    "    \"\"\"\n",
    "    Convert a string to lowercase, strip special characters, and replace spaces with underscores.\n",
    "\n",
    "    Parameters:\n",
    "        s (str): The input string to sanitize.\n",
    "\n",
    "    Returns:\n",
    "        str: The sanitized string.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    s = s.replace(' ', '_')\n",
    "    s = re.sub(r'[^a-z0-9_]', '', s)\n",
    "    return s\n",
    "\n",
    "def ensure_directory_exists(dir_path):\n",
    "    \"\"\"\n",
    "    Ensure the specified directory exists; create it if it does not, including required subdirectories.\n",
    "    Set permissions to 777 for the main directory and its contents recursively.\n",
    "\n",
    "    Parameters:\n",
    "        dir_path (str): The path of the directory to ensure exists.\n",
    "\n",
    "    Returns:\n",
    "        str: The absolute path of the directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # List of required subdirectories\n",
    "    subdirs = ['dataset', 'src', 'checkpoints', 'output']\n",
    "    dirs_to_clear = ['checkpoints', 'output']    \n",
    "    \n",
    "    # Create each subdirectory if it doesnâ€™t exist\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(dir_path, subdir)\n",
    "        os.makedirs(subdir_path, exist_ok=True)\n",
    "    \n",
    "    # Set permissions recursively for main directory and all subdirectories/files\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        os.chmod(root, 0o777)\n",
    "        for d in dirs:\n",
    "            os.chmod(os.path.join(root, d), 0o777)\n",
    "        for f in files:\n",
    "            os.chmod(os.path.join(root, f), 0o777)\n",
    "    \n",
    "    # Clear all contents in the specified directories\n",
    "    for clear_dir in dirs_to_clear:\n",
    "        clear_path = os.path.join(dir_path, clear_dir)\n",
    "        for filename in os.listdir(clear_path):\n",
    "            file_path = os.path.join(clear_path, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)  # Remove file or link\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove directory and all contents\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "    return os.path.abspath(dir_path)\n",
    "\n",
    "def check_file_exists(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file exists at the specified path.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path of the file to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return os.path.isfile(file_path)\n",
    "\n",
    "def is_archive_file(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file is a valid archive (zip or tar).\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path of the file to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file is a valid archive, False otherwise.\n",
    "    \"\"\"\n",
    "    return zipfile.is_zipfile(file_path) or tarfile.is_tarfile(file_path)"
   ],
   "id": "d749f7e3abf77d99",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:31.768171Z",
     "start_time": "2024-11-22T01:28:31.750798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "\n",
    "def setup_project(prj_name, brd_name, model_type, project_folder=None, model_py_file=None, model_pth_file=None, torch_vision_model=None, finn_pretrained_model=None, dataset_type=None, custom_dataset=None, torch_vision_dataset=None):\n",
    "    \"\"\"\n",
    "    Set up a project with a specified structure, including creating necessary directories, \n",
    "    validating model type, and checking for necessary files.\n",
    "\n",
    "    Parameters:\n",
    "        finn_pretrained_model: \n",
    "        prj_name (str): The name of the project.\n",
    "        model_type (str): The type of model ('untrained', 'custom_pretrained', or 'torch_vision_pretrained').\n",
    "        project_folder (str, optional): The main folder for the project. A new folder is generated if not provided.\n",
    "        model_py_file (str, optional): The filename of the Python script defining the model architecture, required for 'untrained' and 'custom_pretrained' models.\n",
    "        model_pth_file (str, optional): The filename of the .pth file with pre-trained weights, required for 'custom_pretrained' models.\n",
    "        torch_vision_model (str, optional): The name of the TorchVision model to load if 'torch_vision_pretrained' is selected.\n",
    "        dataset_type (str, optional): Type of dataset for 'untrained' models ('torch_vision_dataset' or 'custom_dataset').\n",
    "        custom_dataset (str, optional): Path to the custom dataset file for 'untrained' models with 'custom_dataset' dataset type.\n",
    "        torch_vision_dataset (str, optional): Name of the TorchVision dataset class for 'untrained' models with 'torch_vision_dataset' dataset type.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with project setup information.\n",
    "    \"\"\"\n",
    "    log_message(\"Setting up project\")\n",
    "    working_folder = \"/home/fastqnn/finn/notebooks/Fast-QNN/outputs/txaviour/\"\n",
    "    prj_info = {}\n",
    "    \n",
    "    # Ensure Project Name is provided\n",
    "    if not prj_name:\n",
    "        log_message(\"Project Name is required\", level=\"error\")\n",
    "\n",
    "    # Sanitize project name and set project info\n",
    "    prj_name_stripped = sanitize_string(prj_name)\n",
    "    display_name = prj_name\n",
    "    prj_info[\"Display_Name\"] = display_name\n",
    "    prj_info[\"Stripped_Name\"] = prj_name_stripped\n",
    "\n",
    "    if not project_folder:\n",
    "        # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") # for production mode\n",
    "        timestamp = \"0\"\n",
    "        project_folder = f\"{working_folder}{prj_name_stripped}_{timestamp}\"\n",
    "    \n",
    "    folder_path = ensure_directory_exists(project_folder)\n",
    "    prj_info[\"Folder\"] = folder_path\n",
    "    \n",
    "    available_boards = pynq_part_map.keys()\n",
    "    if brd_name in available_boards:\n",
    "        prj_info[\"Board_name\"] = brd_name\n",
    "    else:\n",
    "        log_message(f\"'{brd_name}' is not a valid board name. Available board names are: {available_boards}\", level=\"error\")\n",
    "\n",
    "    # Validate Model Type\n",
    "    valid_model_types = [\"untrained\", \"custom_pretrained\", \"torch_vision_pretrained\", \"finn_pretrained\"]\n",
    "    if model_type not in valid_model_types:\n",
    "        log_message(f\"Model Type must be one of {valid_model_types}\", level=\"error\")\n",
    "    prj_info['Model_Type'] = model_type\n",
    "    \n",
    "    # Handle Model Type specific requirements\n",
    "    if model_type in [\"untrained\", \"custom_pretrained\"]:\n",
    "        if not model_py_file or not check_file_exists(os.path.join(project_folder, 'src', model_py_file)):\n",
    "            log_message(f\"Model Py File '{model_py_file}' does not exist in '{project_folder}'\", level=\"error\")\n",
    "        prj_info[\"Model_Py_File\"] = model_py_file\n",
    "\n",
    "    if model_type == \"custom_pretrained\":\n",
    "        if not model_pth_file or not check_file_exists(os.path.join(project_folder, 'src', model_pth_file)):\n",
    "            log_message(f\"Model Pth File '{model_pth_file}' does not exist in '{project_folder}'\", level=\"error\")\n",
    "        prj_info[\"Model_Pth_File\"] = model_pth_file\n",
    "\n",
    "    if model_type == \"torch_vision_pretrained\":\n",
    "        available_models = list_models(module=models)\n",
    "        if not torch_vision_model or torch_vision_model not in available_models:\n",
    "            log_message(f\"Torch Vision Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Pretrained_Model\"] = torch_vision_model\n",
    "\n",
    "    if model_type == \"finn_pretrained\":\n",
    "        available_models = [\"cnv_1w1a\", \"cnv_1w2a\", \"cnv_2w2a\", \"lfc_1w1a\", \"lfc_1w2a\", \"sfc_1w1a\", \"sfc_1w2a\", \"sfc_2w2a\", \"tfc_1w1a\", \"tfc_1w2a\", \"tfc_2w2a\", \"quant_mobilenet_v1_4b\"]\n",
    "        if not finn_pretrained_model or finn_pretrained_model not in available_models:\n",
    "            log_message(f\"Finn Pretrained Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Pretrained_Model\"] = finn_pretrained_model\n",
    "\n",
    "    # Handle Dataset requirements for Untrained models\n",
    "    if model_type == \"untrained\":\n",
    "        if dataset_type not in [\"torch_vision_dataset\", \"custom_dataset\"]:\n",
    "            log_message(\"Dataset Type must be either 'Torch Vision' or 'Custom Dataset' for Untrained models\", level=\"error\")\n",
    "        prj_info[\"Dataset_Type\"] = dataset_type\n",
    "\n",
    "        if dataset_type == \"custom_dataset\":\n",
    "            custom_dataset_path = os.path.join(project_folder, 'dataset', custom_dataset)\n",
    "            if not custom_dataset or not check_file_exists(custom_dataset_path) or not is_archive_file(custom_dataset_path):\n",
    "                log_message(f\"Custom Dataset '{custom_dataset}' must exist in '{project_folder}' and be an archive file (zip or tar)\", level=\"error\")\n",
    "            prj_info[\"Custom_Dataset\"] = custom_dataset\n",
    "\n",
    "        elif dataset_type == \"torch_vision_dataset\":\n",
    "            available_datasets = [cls_name.lower() for cls_name in dir(datasets) if not cls_name.startswith('_')]\n",
    "            if not torch_vision_dataset or torch_vision_dataset.lower() not in available_datasets:\n",
    "                log_message(f\"Torch Vision Dataset must be one of: {available_datasets}\", level=\"error\")\n",
    "            prj_info[\"Torch_Vision_Dataset\"] = torch_vision_dataset\n",
    "            \n",
    "    log_message(f\"Project setup complete. {prj_name} has been initialized.\")\n",
    "    return prj_info"
   ],
   "id": "4df70eb120061ecb",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:31.818647Z",
     "start_time": "2024-11-22T01:28:31.802695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.util.test import get_test_model_trained\n",
    "\n",
    "\n",
    "def load_pretrained_model(model_name, model_type, src_folder, initial_channels = 3, max_size = 4096):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model from TorchVision and ensures all downloads are in the src folder of the Project.\n",
    "\n",
    "    Parameters:\n",
    "        model_type: \n",
    "        initial_channels: \n",
    "        max_size: \n",
    "        model_name (str): The name of the pre-trained model to load (e.g., 'alexnet', 'resnet50').\n",
    "        src_folder (str): The folder where model downloads will be stored. Default is 'src'.\n",
    "                \n",
    "    Returns:\n",
    "        torch.nn.Module: The loaded pre-trained model.\n",
    "    \"\"\"\n",
    "    log_message(f\"Loading {model_type} Model: {model_name}\")\n",
    "    # Ensure the src folder exists\n",
    "    os.makedirs(src_folder, exist_ok=True)\n",
    "    # Set TORCH_HOME to the src folder to store the model downloads there\n",
    "    os.environ['TORCH_HOME'] = src_folder\n",
    "    pretrained_model = None\n",
    "    if model_type == \"torch_vision_pretrained\":\n",
    "        # Load the model from torch.hub with the specified model name\n",
    "        pretrained_model = torch.hub.load('pytorch/vision:v0.10.0', model_name, pretrained=True)\n",
    "    elif model_type == \"finn_pretrained\":\n",
    "        if model_name == \"cnv_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 1, 1)\n",
    "        elif model_name == \"cnv_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 1, 2)\n",
    "        elif model_name == \"cnv_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 2, 2)\n",
    "        elif model_name == \"lfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"LFC\", 1, 1)\n",
    "        elif model_name == \"lfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"LFC\", 1, 2)\n",
    "        elif model_name == \"sfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 1, 1)\n",
    "        elif model_name == \"sfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 1, 2)\n",
    "        elif model_name == \"sfc_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 2, 2)\n",
    "        elif model_name == \"tfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 1, 1)\n",
    "        elif model_name == \"tfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 1, 2)\n",
    "        elif model_name == \"tfc_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 2, 2)\n",
    "        elif model_name == \"quant_mobilenet_v1_4b\":\n",
    "            pretrained_model = get_test_model_trained(\"mobilenet\", 4, 4)\n",
    "    # List of common input shapes to test first (both square and non-square)\n",
    "    common_shapes = [\n",
    "        (1, initial_channels, 32, 32),  # Typical for many models\n",
    "        (1, initial_channels, 28, 28),  # Typical for many models\n",
    "        (1, initial_channels, 224, 224),  # Typical for many models\n",
    "        (1, initial_channels, 299, 299),  # For models like Inception\n",
    "        (1, initial_channels, 128, 128),  # Smaller size\n",
    "        (1, initial_channels, 256, 256),  # Larger square size\n",
    "        (1, initial_channels, 320, 240),  # Common non-square size\n",
    "        (1, initial_channels, 240, 320),  # Non-square (swapped dimensions)\n",
    "        (1, initial_channels, 256, 128),  # Non-square\n",
    "        (1, initial_channels, 128, 256),  # Non-square (swapped)\n",
    "    ]\n",
    "\n",
    "    # Try common shapes first\n",
    "    for shape in common_shapes:\n",
    "        try:\n",
    "            dummy_input = torch.rand(*shape)\n",
    "            pretrained_model(dummy_input)\n",
    "            log_message(f\"Compatible common input shape found: {shape}\")\n",
    "            return pretrained_model, shape\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "\n",
    "    # If no common shape worked, test all possible square and non-square shapes up to max_size\n",
    "    for width in range(1, max_size + 1, 1):  # Step by 16 for efficiency\n",
    "        for height in range(1, max_size + 1, 1):\n",
    "            try:\n",
    "                dummy_input = torch.rand(1, initial_channels, width, height)\n",
    "                pretrained_model(dummy_input)\n",
    "                pretrained_model_input_shape = (1, initial_channels, width, height)\n",
    "                log_message(f\"Compatible input shape found: {pretrained_model_input_shape}\")\n",
    "                return pretrained_model, pretrained_model_input_shape\n",
    "            except RuntimeError:\n",
    "                continue\n",
    "\n",
    "    log_message(\"Could not determine a compatible input shape within the specified range.\", level=\"error\")"
   ],
   "id": "e912d407f448ecb4",
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "id": "3b4a8b34b65b86a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:33.346863Z",
     "start_time": "2024-11-22T01:28:31.847964Z"
    }
   },
   "source": [
    "prj_name_input = \"AlexNet 1W1A Test\"\n",
    "board_name_input = \"Pynq-Z2\"\n",
    "prj_folder_input = sanitize_string(prj_name_input)\n",
    "model_type_input = \"torch_vision_pretrained\"\n",
    "torch_vision_model_input = \"alexnet\"\n",
    "Project_Info = setup_project(prj_name=prj_name_input, brd_name=board_name_input, model_type=model_type_input, torch_vision_model=torch_vision_model_input)\n",
    "\n",
    "input_model= None\n",
    "input_model_shape = None\n",
    "\n",
    "if Project_Info['Model_Type'] == \"untrained\":\n",
    "    log_message(\"Training for untrained models are not supported at the moment!\", level=\"error\")\n",
    "elif Project_Info['Model_Type'] == \"custom_pretrained\":\n",
    "    log_message(\"Custom Pretrained models are not supported at the moment!\", level=\"error\")\n",
    "elif Project_Info['Model_Type'] == \"torch_vision_pretrained\" or Project_Info['Model_Type'] == \"finn_pretrained\":\n",
    "    pretrained_folder = os.path.join(Project_Info['Folder'],\"src\")\n",
    "    input_model, input_model_shape = load_pretrained_model(Project_Info['Pretrained_Model'], Project_Info['Model_Type'], pretrained_folder, initial_channels=3)\n",
    "else:\n",
    "    log_message(\"Unsupported Model Type\", level=\"error\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting up project\n",
      "[INFO] Project setup complete. AlexNet 1W1A Test has been initialized.\n",
      "[INFO] Loading torch_vision_pretrained Model: alexnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fastqnn/finn/notebooks/Fast-QNN/outputs/txaviour/alexnet_1w1a_test_0/src/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Compatible common input shape found: (1, 3, 224, 224)\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:33.397526Z",
     "start_time": "2024-11-22T01:28:33.392924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_onnx_checkpoint(project_info, suffix):\n",
    "    \"\"\"\n",
    "    Generates the export path for ONNX files based on a specified suffix.\n",
    "\n",
    "    Parameters:\n",
    "        project_info (dict): Dictionary containing project information (e.g., 'Folder' and 'Stripped_Name').\n",
    "        suffix (str): The suffix to append to the exported ONNX filename (e.g., \"model1\" for \"model1_export.onnx\").\n",
    "\n",
    "    Returns:\n",
    "        str: The full path to the export file.\n",
    "    \"\"\"\n",
    "    log_message(f\"Saving Checkpoint: {suffix}\")\n",
    "    suffix = sanitize_string(suffix)\n",
    "    filename = f\"{project_info['Stripped_Name']}_{suffix}.onnx\"\n",
    "    return os.path.join(project_info['Folder'], \"checkpoints\", filename)\n"
   ],
   "id": "1f4ae7b982c2a64c",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:40.015891Z",
     "start_time": "2024-11-22T01:28:33.427749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "export_onnx_path = set_onnx_checkpoint(Project_Info,\"Brevitas Export\")\n",
    "export_qonnx(input_model, torch.randn(input_model_shape), export_onnx_path, opset_version=9)\n",
    "qonnx_cleanup(export_onnx_path, out_file=export_onnx_path)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Brevitas Export\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:46.866755Z",
     "start_time": "2024-11-22T01:28:40.039776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "model = ModelWrapper(export_onnx_path)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"QONNX to FINN\"))"
   ],
   "id": "de436732503bc809",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: QONNX to FINN\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:46.903992Z",
     "start_time": "2024-11-22T01:28:46.897566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from qonnx.transformation.remove import RemoveIdentityOps\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import (GiveReadableTensorNames,\n",
    "                                          GiveUniqueNodeNames,\n",
    "                                          RemoveStaticGraphInputs,\n",
    "                                          RemoveUnusedTensors, GiveUniqueParameterTensors, SortGraph)\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "def tidy_up_transforms(input_tidy_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a series of transformations to a model and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_tidy_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply transformations\n",
    "    input_tidy_model = input_tidy_model.transform(GiveUniqueParameterTensors())\n",
    "    input_tidy_model = input_tidy_model.transform(InferShapes())\n",
    "    input_tidy_model = input_tidy_model.transform(FoldConstants())\n",
    "    input_tidy_model = input_tidy_model.transform(GiveUniqueNodeNames())\n",
    "    input_tidy_model = input_tidy_model.transform(GiveReadableTensorNames())\n",
    "    input_tidy_model = input_tidy_model.transform(InferDataTypes())\n",
    "    input_tidy_model = input_tidy_model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "    # Save the transformed model\n",
    "    input_tidy_model.save(save_name)\n",
    "\n",
    "    return input_tidy_model"
   ],
   "id": "a5d959d6407a1ec2",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:50.813702Z",
     "start_time": "2024-11-22T01:28:46.932491Z"
    }
   },
   "cell_type": "code",
   "source": "model = tidy_up_transforms(model, set_onnx_checkpoint(Project_Info,\"Tidy ONNX Post Finn\"))",
   "id": "21fb27ba948a8cbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Tidy ONNX Post Finn\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:50.845063Z",
     "start_time": "2024-11-22T01:28:50.840433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.util.pytorch import ToTensor\n",
    "\n",
    "log_message(\"Skipping Pre-Processing. Will be expecting the user to handle it in application!\", level=\"warning\")"
   ],
   "id": "511d18741790a4a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Skipping Pre-Processing. Will be expecting the user to handle it in application!\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:50.907030Z",
     "start_time": "2024-11-22T01:28:50.881119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = set_onnx_checkpoint(Project_Info,\"Pre Proc ONNX Finn\")\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name, opset_version=9)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\"\"\""
   ],
   "id": "57af62ce8ef3c010",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nglobal_inp_name = model.graph.input[0].name\\nishape = model.get_tensor_shape(global_inp_name)\\n# preprocessing: torchvision\\'s ToTensor divides uint8 inputs by 255\\ntotensor_pyt = ToTensor()\\nchkpt_preproc_name = set_onnx_checkpoint(Project_Info,\"Pre Proc ONNX Finn\")\\nexport_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name, opset_version=9)\\nqonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\\npre_model = ModelWrapper(chkpt_preproc_name)\\npre_model = pre_model.transform(ConvertQONNXtoFINN())\\nmodel = model.transform(MergeONNXModels(pre_model))\\n# add input quantization annotation: UINT8 for all BNN-PYNQ models\\nglobal_inp_name = model.graph.input[0].name\\nmodel.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:52.069729Z",
     "start_time": "2024-11-22T01:28:50.943032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiplyByOne(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = x * 2\n",
    "        out = out + 0  # Adding a no-op to prevent graph simplification\n",
    "        out = out / 2  # Adding a no-op to prevent graph simplification\n",
    "        return out\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "mul_model = MultiplyByOne()\n",
    "\n",
    "chkpt_mul_name = set_onnx_checkpoint(Project_Info, \"Mul_One_ONNX\")\n",
    "export_qonnx(mul_model, torch.randn(ishape), chkpt_mul_name, opset_version=9)\n",
    "qonnx_cleanup(chkpt_mul_name, out_file=chkpt_mul_name)\n",
    "\n",
    "mul_model_wrapper = ModelWrapper(chkpt_mul_name)\n",
    "mul_model_wrapper = mul_model_wrapper.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(MergeONNXModels(mul_model_wrapper))\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])"
   ],
   "id": "13c356373544a8a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Mul_One_ONNX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/home_dir/.local/lib/python3.10/site-packages/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:53.895822Z",
     "start_time": "2024-11-22T01:28:52.099684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"Post Processing\"))\n",
    "model = tidy_up_transforms(model, set_onnx_checkpoint(Project_Info,\"Tidy Post PrePost Proc\"))"
   ],
   "id": "49bbd723443c4e65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Post Processing\n",
      "[INFO] Saving Checkpoint: Tidy Post PrePost Proc\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:53.985431Z",
     "start_time": "2024-11-22T01:28:53.956040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from finn.transformation.qonnx.qonnx_activation_handlers import QuantActBaseHandler, np_default_dtype\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "\n",
    "class ReluToMultiThresholdHandler(QuantActBaseHandler):\n",
    "    \"\"\"Class for converting a regular non-quantized ReLU operation to a MultiThreshold node in Xilinx FINN.\"\"\"\n",
    "    def _check_compatibility(self):\n",
    "        # Check if the ReLU operation is supported for transformation\n",
    "        pass\n",
    "\n",
    "    def _calculate_act_bias(self):\n",
    "        # No bias is applied for standard ReLU\n",
    "        return np.array([0.0], dtype=np_default_dtype)\n",
    "\n",
    "    def _calculate_thresholds(self):\n",
    "        # Thresholds for ReLU are typically defined as a single zero threshold\n",
    "        output_shape = self._model.get_tensor_shape(self._q_node.output[0])\n",
    "        num_output_channels = output_shape[1]  # Assuming NCHW or NHWC format\n",
    "        thresholds = np.zeros((num_output_channels, 1), dtype=np_default_dtype)\n",
    "        return thresholds\n",
    "\n",
    "    def _calculate_act_scale(self):\n",
    "        # Standard ReLU does not apply scaling\n",
    "        return np.array([1.0], dtype=np_default_dtype)\n",
    "\n",
    "    def _remove_activation_node(self, multi_threshold_node):\n",
    "        # Find the activation node\n",
    "        act_node = self._model.find_direct_predecessors(self._q_node)\n",
    "        if act_node is None:\n",
    "            raise RuntimeError(\n",
    "                \"For handling of Relu activations a predecesor to dgdgdg\"\n",
    "            )\n",
    "        act_node = act_node[0]\n",
    "        if act_node.op_type != \"Relu\":\n",
    "            raise RuntimeError(\n",
    "                \"The predecesor of the Quant node must be Relu for handling \"\n",
    "                \"of activations.\"\n",
    "            )\n",
    "\n",
    "        # Remove the activation node\n",
    "        self._model.graph.node.remove(act_node)\n",
    "        return multi_threshold_node\n",
    "\n",
    "    def calculate_node_parameters(self):\n",
    "        # Calculate all parameters for replacing ReLU with MultiThreshold\n",
    "        return {\n",
    "            \"out_dtype\": \"FLOAT32\",  # Regular ReLU typically works with float32\n",
    "            \"thresholds\": self._calculate_thresholds(),\n",
    "            \"adder_bias\": self._calculate_act_bias(),\n",
    "            \"mul_scale\": self._calculate_act_scale(),\n",
    "        }\n",
    "\n",
    "    def replace_relu_node(self):\n",
    "        \"\"\"Replace a regular ReLU operation with a MultiThreshold node.\"\"\"\n",
    "        # Check compatibility\n",
    "        self._check_compatibility()\n",
    "\n",
    "        # Shorten instance variables\n",
    "        model = self._model\n",
    "        graph = model.graph\n",
    "        n = self._q_node\n",
    "        running_node_index = self._q_index\n",
    "\n",
    "        # Calculate parameters for the MultiThreshold node\n",
    "        parameter_dict = self.calculate_node_parameters()\n",
    "        thresholds = parameter_dict[\"thresholds\"]\n",
    "        adder_bias = parameter_dict[\"adder_bias\"]\n",
    "        mul_scale = parameter_dict[\"mul_scale\"]\n",
    "        out_dtype = parameter_dict[\"out_dtype\"]\n",
    "\n",
    "        # Create threshold tensor\n",
    "        thresh_tensor = oh.make_tensor_value_info(\n",
    "            model.make_new_valueinfo_name(),\n",
    "            TensorProto.FLOAT,\n",
    "            thresholds.shape,\n",
    "        )\n",
    "        graph.value_info.append(thresh_tensor)\n",
    "        model.set_initializer(thresh_tensor.name, thresholds)\n",
    "        act_node = self._model.find_direct_predecessors(self._q_node)\n",
    "        act_node = act_node[0]\n",
    "        # Insert MultiThreshold node\n",
    "        mt_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            [act_node.input[0], thresh_tensor.name],\n",
    "            [n.output[0]],\n",
    "            out_dtype=out_dtype,\n",
    "            domain=\"qonnx.custom_op.general\",\n",
    "        )\n",
    "        graph.node.insert(running_node_index, mt_node)\n",
    "        running_node_index += 1\n",
    "\n",
    "        # Get MultiThreshold instance\n",
    "        mt_inst = getCustomOp(graph.node[running_node_index - 1])\n",
    "\n",
    "        # Set bias and scale attributes\n",
    "        mt_inst.set_nodeattr(\"out_scale\", mul_scale[0].item())\n",
    "        mt_inst.set_nodeattr(\"out_bias\", adder_bias[0].item())\n",
    "        mt_inst.set_nodeattr(\"out_dtype\", out_dtype)\n",
    "\n",
    "        # Remove the original ReLU node\n",
    "        self._remove_activation_node(mt_node)\n",
    "        graph.node.remove(n)\n",
    "\n",
    "        # Return the updated model\n",
    "        return self._model\n"
   ],
   "id": "a7dc93abb28939d8",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:54.069219Z",
     "start_time": "2024-11-22T01:28:54.032524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.qonnx.qonnx_activation_handlers import QuantReluHandler\n",
    "from onnx import TensorProto\n",
    "from qonnx.transformation.base import Transformation\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from onnx import helper as oh\n",
    "\n",
    "class CustomReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Implementing an approximation of abs(x) using only arithmetic operations\n",
    "        epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "        abs_x = (x * x + epsilon) / (x + epsilon)\n",
    "        return (x + abs_x) / 2\n",
    "\n",
    "class CustomReluBreakdown(Transformation):\n",
    "    \"\"\"\n",
    "    Custom transformation to iterate through the nodes, identify ReLU nodes,\n",
    "    and convert the next node to a quant node while applying the QuantReluHandler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bit_width=1, quant_type=DataType[\"UINT8\"], narrow=0, signed=0):\n",
    "        super().__init__()\n",
    "        self.bit_width = bit_width\n",
    "        self.quant_type = quant_type\n",
    "        self.narrow = narrow\n",
    "        self.signed = signed\n",
    "\n",
    "    def apply(self, transform_model):\n",
    "        graph = transform_model.graph\n",
    "        graph_modified = False\n",
    "        convert_next_node = False\n",
    "\n",
    "        for idx, node in enumerate(graph.node):\n",
    "            if convert_next_node:\n",
    "                # Call QuantReluHandler on the current Quant node\n",
    "                handler = ReluToMultiThresholdHandler(transform_model, node, idx)\n",
    "                handler.replace_relu_node()\n",
    "\n",
    "                convert_next_node = False\n",
    "                graph_modified = True\n",
    "\n",
    "            if node.op_type == \"Relu\":\n",
    "                # Raise a flag to convert the next node to a quant node\n",
    "                convert_next_node = True\n",
    "\n",
    "       # transform_model = transform_model.transform(InferShapes())\n",
    "        return (transform_model, graph_modified)\n"
   ],
   "id": "a45466c16fde453c",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:54.134033Z",
     "start_time": "2024-11-22T01:28:54.105241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from onnx import helper as oh\n",
    "from onnx import TensorProto\n",
    "from qonnx.transformation.base import Transformation\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReluToMultiThresholdTransform(Transformation):\n",
    "    \"\"\"\n",
    "    Custom transformation to replace ReLU nodes with MultiThreshold nodes\n",
    "    in Xilinx FINN. Works directly on ReLU nodes and reroutes their inputs/outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply(self, transform_model):\n",
    "        graph = transform_model.graph\n",
    "        graph_modified = False\n",
    "\n",
    "        for idx, node in enumerate(graph.node):\n",
    "            if node.op_type == \"Relu\":\n",
    "                # Replace the ReLU node with a MultiThreshold node\n",
    "                self.replace_relu_with_multithreshold(transform_model, node, idx)\n",
    "                graph_modified = True\n",
    "\n",
    "        return (transform_model, graph_modified)\n",
    "\n",
    "    def replace_relu_with_multithreshold(self, transform_model, relu_node, relu_index):\n",
    "        \"\"\"\n",
    "        Replaces a ReLU node with a MultiThreshold node.\n",
    "        \"\"\"\n",
    "        graph = transform_model.graph\n",
    "\n",
    "        # Get the input to the ReLU node (its predecessor)\n",
    "        if len(relu_node.input) != 1:\n",
    "            raise RuntimeError(f\"ReLU node {relu_node.name} has unexpected inputs.\")\n",
    "        relu_input = relu_node.input[0]\n",
    "\n",
    "        # Get the output of the ReLU node (its successor)\n",
    "        if len(relu_node.output) != 1:\n",
    "            raise RuntimeError(f\"ReLU node {relu_node.name} has unexpected outputs.\")\n",
    "        relu_output = relu_node.output[0]\n",
    "\n",
    "        # Calculate parameters for MultiThreshold\n",
    "        thresholds = self._calculate_thresholds(transform_model, relu_node)\n",
    "        adder_bias = self._calculate_act_bias()\n",
    "        mul_scale = self._calculate_act_scale()\n",
    "        out_dtype = \"FLOAT32\"\n",
    "\n",
    "        # Create threshold tensor\n",
    "        thresh_tensor = oh.make_tensor_value_info(\n",
    "            transform_model.make_new_valueinfo_name(),\n",
    "            TensorProto.FLOAT,\n",
    "            thresholds.shape,\n",
    "        )\n",
    "        graph.value_info.append(thresh_tensor)\n",
    "        transform_model.set_initializer(thresh_tensor.name, thresholds)\n",
    "\n",
    "        # Create MultiThreshold node\n",
    "        mt_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs=[relu_input, thresh_tensor.name],\n",
    "            outputs=[relu_output],\n",
    "            domain=\"qonnx.custom_op.general\",\n",
    "        )\n",
    "        mt_node.attribute.extend([\n",
    "            oh.make_attribute(\"out_scale\", float(mul_scale[0])),\n",
    "            oh.make_attribute(\"out_bias\", float(adder_bias[0])),\n",
    "            oh.make_attribute(\"out_dtype\", out_dtype),\n",
    "        ])\n",
    "\n",
    "        # Insert MultiThreshold node in place of ReLU node\n",
    "        graph.node.insert(relu_index, mt_node)\n",
    "\n",
    "        # Remove the original ReLU node\n",
    "        graph.node.remove(relu_node)\n",
    "\n",
    "    def _calculate_thresholds(self, transform_model, relu_node):\n",
    "        \"\"\"\n",
    "        Calculates the thresholds for the MultiThreshold node based on the ReLU operation.\n",
    "        \"\"\"\n",
    "        # ReLU threshold is always zero\n",
    "        output_shape = transform_model.get_tensor_shape(relu_node.output[0])\n",
    "        num_output_channels = output_shape[1]  # Assuming NCHW or NHWC format\n",
    "        thresholds = np.zeros((num_output_channels, 1), dtype=np.float32)\n",
    "        return thresholds\n",
    "\n",
    "    def _calculate_act_bias(self):\n",
    "        \"\"\"\n",
    "        Calculates the bias for the MultiThreshold node.\n",
    "        \"\"\"\n",
    "        # ReLU does not apply a bias\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "\n",
    "    def _calculate_act_scale(self):\n",
    "        \"\"\"\n",
    "        Calculates the scale for the MultiThreshold node.\n",
    "        \"\"\"\n",
    "        # ReLU does not apply scaling\n",
    "        return np.array([1.0], dtype=np.float32)\n"
   ],
   "id": "53fa26aa1d5bd4e3",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:28:54.205221Z",
     "start_time": "2024-11-22T01:28:54.166814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import traceback\n",
    "from qonnx.transformation.batchnorm_to_affine import BatchNormToAffine\n",
    "from qonnx.transformation.general import ConvertSubToAdd, ConvertDivToMul\n",
    "from finn.transformation.streamline import Streamline, RoundAndClipThresholds, CollapseRepeatedMul, ConvertSignToThres, \\\n",
    "    CollapseRepeatedAdd\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from finn.transformation.streamline.absorb import (AbsorbTransposeIntoMultiThreshold,\n",
    "                                                   AbsorbScalarMulAddIntoTopK,\n",
    "                                                   AbsorbSignBiasIntoMultiThreshold,\n",
    "                                                   AbsorbAddIntoMultiThreshold,\n",
    "                                                   AbsorbMulIntoMultiThreshold, FactorOutMulSignMagnitude,\n",
    "                                                   Absorb1BitMulIntoMatMul, Absorb1BitMulIntoConv)\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants, MoveAddPastMul, \\\n",
    "    MoveScalarAddPastMatMul, MoveAddPastConv, MoveScalarMulPastMatMul, MoveScalarMulPastConv, \\\n",
    "    MoveMaxPoolPastMultiThreshold, MoveLinearPastEltwiseAdd, MoveLinearPastFork\n",
    "from finn.transformation.streamline.absorb import AbsorbConsecutiveTransposes\n",
    "\n",
    "def streamline_transforms(input_streamline_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a series of streamlining transformations to a model and saves the resulting model.\n",
    "    If a transformation fails, it logs the error and moves on to the next transformation.\n",
    "\n",
    "    Parameters:\n",
    "        input_streamline_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        AbsorbSignBiasIntoMultiThreshold(),\n",
    "        ConvertSubToAdd(),\n",
    "        ConvertDivToMul(),\n",
    "        CollapseRepeatedMul(),\n",
    "        BatchNormToAffine(),\n",
    "        ConvertSignToThres(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarAddPastMatMul(),\n",
    "        MoveAddPastConv(),\n",
    "        MoveScalarMulPastMatMul(),\n",
    "        MoveScalarMulPastConv(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarLinearPastInvariants(),\n",
    "        CollapseRepeatedAdd(),\n",
    "        AbsorbAddIntoMultiThreshold(),\n",
    "        FactorOutMulSignMagnitude(),\n",
    "        MoveMaxPoolPastMultiThreshold(),\n",
    "        AbsorbMulIntoMultiThreshold(),\n",
    "        Absorb1BitMulIntoMatMul(),\n",
    "        Absorb1BitMulIntoConv(),\n",
    "        AbsorbMulIntoMultiThreshold(),\n",
    "        Streamline(),\n",
    "        LowerConvsToMatMul(),\n",
    "        MakeMaxPoolNHWC(),\n",
    "        AbsorbTransposeIntoMultiThreshold(),\n",
    "        ConvertBipolarMatMulToXnorPopcount(),\n",
    "        Streamline(),\n",
    "        AbsorbScalarMulAddIntoTopK(),\n",
    "        RoundAndClipThresholds(),\n",
    "        MoveLinearPastEltwiseAdd(),\n",
    "        MoveLinearPastFork()\n",
    "    ]\n",
    "    \n",
    "    input_streamline_model = input_streamline_model.transform(ReluToMultiThresholdTransform())\n",
    "    input_streamline_model = tidy_up_transforms(input_streamline_model, save_name)\n",
    "    for iter_id in range(3):\n",
    "        for transform in transformations:\n",
    "            try:\n",
    "                input_streamline_model = input_streamline_model.transform(transform)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "        input_streamline_model = tidy_up_transforms(input_streamline_model, save_name)\n",
    "        \n",
    "        input_streamline_model = input_streamline_model.transform(InferDataLayouts())\n",
    "        input_streamline_model = input_streamline_model.transform(RemoveUnusedTensors())\n",
    "        input_streamline_model = input_streamline_model.transform(DoubleToSingleFloat())\n",
    "        input_streamline_model = input_streamline_model.transform(SortGraph())\n",
    "        input_streamline_model = input_streamline_model.transform(RemoveIdentityOps())\n",
    "    \n",
    "    return input_streamline_model"
   ],
   "id": "6bdb92934093ceb8",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:32:02.752507Z",
     "start_time": "2024-11-22T01:28:54.231853Z"
    }
   },
   "cell_type": "code",
   "source": "model = streamline_transforms(model, set_onnx_checkpoint(Project_Info,\"Streamlined ONNX\"))",
   "id": "d969950c0d96fc53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Streamlined ONNX\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:32:02.860008Z",
     "start_time": "2024-11-22T01:32:02.850298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.fpgadataflow.convert_to_hw_layers import (\n",
    "    InferBinaryMatrixVectorActivation,\n",
    "    InferQuantizedMatrixVectorActivation,\n",
    "    InferLabelSelectLayer,\n",
    "    InferThresholdingLayer,\n",
    "    InferStreamingMaxPool,\n",
    "    InferConvInpGen,\n",
    "    InferAddStreamsLayer,\n",
    "    InferChannelwiseLinearLayer,\n",
    "    InferConcatLayer,\n",
    "    InferDuplicateStreamsLayer,\n",
    "    InferGlobalAccPoolLayer,\n",
    "    InferLookupLayer,\n",
    "    InferPool,\n",
    "    InferStreamingEltwise,\n",
    "    InferUpsample,\n",
    "    InferVectorVectorActivation\n",
    ")\n",
    "\n",
    "def to_hw_transforms(input_hw_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a comprehensive series of hardware-oriented transformations to a model and saves the resulting model.\n",
    "    If a transformation fails, it logs the error and moves on to the next transformation.\n",
    "\n",
    "    Parameters:\n",
    "        input_hw_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        InferBinaryMatrixVectorActivation(),\n",
    "        InferQuantizedMatrixVectorActivation(),\n",
    "        InferLabelSelectLayer(),\n",
    "        InferThresholdingLayer(),\n",
    "        InferConvInpGen(),\n",
    "        InferStreamingMaxPool(),\n",
    "        InferAddStreamsLayer(),\n",
    "        InferChannelwiseLinearLayer(),\n",
    "        InferConcatLayer(),\n",
    "        InferDuplicateStreamsLayer(),\n",
    "        InferGlobalAccPoolLayer(),\n",
    "        InferLookupLayer(),\n",
    "        InferPool(),\n",
    "        InferStreamingEltwise(),\n",
    "        InferUpsample(),\n",
    "        InferVectorVectorActivation(),\n",
    "        RemoveCNVtoFCFlatten(),\n",
    "        AbsorbConsecutiveTransposes()\n",
    "    ]\n",
    "\n",
    "    for iter_id in range(3):\n",
    "        for transform in transformations:\n",
    "            try:\n",
    "                input_hw_model = input_hw_model.transform(transform)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "        # Apply final tidy-up transformations\n",
    "        input_hw_model = tidy_up_transforms(input_hw_model, save_name)\n",
    "        \n",
    "        input_hw_model = input_hw_model.transform(InferDataLayouts())\n",
    "        input_hw_model = input_hw_model.transform(RemoveUnusedTensors())\n",
    "        input_hw_model = input_hw_model.transform(DoubleToSingleFloat())\n",
    "        input_hw_model = input_hw_model.transform(SortGraph())\n",
    "        input_hw_model = input_hw_model.transform(RemoveIdentityOps())\n",
    "\n",
    "    # Save the final transformed model\n",
    "    input_hw_model.save(save_name)\n",
    "\n",
    "    return input_hw_model"
   ],
   "id": "e34005da6e651b95",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:32:13.003157Z",
     "start_time": "2024-11-22T01:32:02.905207Z"
    }
   },
   "cell_type": "code",
   "source": "model = to_hw_transforms(model, set_onnx_checkpoint(Project_Info,\"To HW Layers\"))",
   "id": "3231949dbb03adb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: To HW Layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_1 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_2 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_3 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_4 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n",
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:394: UserWarning: Stride is not equal to kernel. Node cannot be converted to\n",
      "                        StreamingMaxPool layer.\n",
      "  warnings.warn(warn_str)\n",
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:63: UserWarning: Im2Col_0 : Input is not int. Can't infer ConvInpGen.\n",
      "  warnings.warn(\"%s : Input is not int. Can't infer ConvInpGen.\" % n.name)\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:32:13.033298Z",
     "start_time": "2024-11-22T01:32:13.028614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "\n",
    "def dataflow_partitioning(input_data_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies dataflow partitioning transformation to the model and saves the resulting parent and dataflow models.\n",
    "\n",
    "    Parameters:\n",
    "        input_data_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The directory to save the transformed models.\n",
    "\n",
    "    Returns:\n",
    "        ModelWrapper: The transformed parent model.\n",
    "    \"\"\"\n",
    "    # Apply dataflow partitioning\n",
    "    parent_model = input_data_model.transform(CreateDataflowPartition())\n",
    "    parent_model.save(save_name)\n",
    "    \n",
    "    # Retrieve the dataflow partition model filename\n",
    "    sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "    sdp_node = getCustomOp(sdp_node)\n",
    "    dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "    # Load and return the dataflow model\n",
    "    dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "    \n",
    "    return dataflow_model"
   ],
   "id": "7b896786bf7eccae",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T01:32:13.389800Z",
     "start_time": "2024-11-22T01:32:13.063336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = dataflow_partitioning(model, set_onnx_checkpoint(Project_Info,\"Dataflow Partition Parent Model\"))\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"Dataflow Partition Streaming Model\"))"
   ],
   "id": "a18f4bf215064d2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Dataflow Partition Parent Model\n",
      "[INFO] Saving Checkpoint: Dataflow Partition Streaming Model\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "\n",
    "def specialize_layers_transform(input_specialize_model, board_name, save_name):\n",
    "    \"\"\"\n",
    "    Applies layer specialization transformation to a dataflow model for the specified FPGA part and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_specialize_model (ModelWrapper): The dataflow model to transform.\n",
    "        board_name (str): The FPGA board for which to specialize the layers.\n",
    "        save_name (str): The path to save the specialized model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed and specialized dataflow model.\n",
    "    \"\"\"\n",
    "    fpga_part = pynq_part_map[board_name]\n",
    "    # Apply specialization for FPGA layers\n",
    "    input_specialize_model = input_specialize_model.transform(SpecializeLayers(fpga_part))\n",
    "\n",
    "    # Save the specialized model\n",
    "    input_specialize_model.save(save_name)\n",
    "\n",
    "    return input_specialize_model"
   ],
   "id": "29068574207cfe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = specialize_layers_transform(model, Project_Info['Board_name'], set_onnx_checkpoint(Project_Info,f\"Specialize Model Layers to {Project_Info['Board_name']}\"))",
   "id": "66573cba9e8b29aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def folding_transform(input_folding_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies folding configuration to fully connected (MVAU_hls) and sliding window (ConvolutionInputGenerator_rtl)\n",
    "    layers in the model and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_folding_model (ModelWrapper): The specialized model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed and folded dataflow model.\n",
    "    \"\"\"\n",
    "    folding_config = [\n",
    "    (16, 3, [128]),\n",
    "    (32, 32, [128]),\n",
    "    (16, 32, [128]),\n",
    "    (16, 32, [128]),\n",
    "    (4, 32, [81]),\n",
    "    (1, 32, [2]),\n",
    "    (1, 4, [2]),\n",
    "    (1, 8, [128]),\n",
    "    (5, 1, [3]),\n",
    "    ]\n",
    "    \n",
    "    # Apply folding configuration to fully connected layers\n",
    "    fc_layers = input_folding_model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "    for fcl, (pe, simd, ififodepth) in zip(fc_layers, folding_config):\n",
    "        fcl_inst = getCustomOp(fcl)\n",
    "        fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "        fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "        fcl_inst.set_nodeattr(\"inFIFODepths\", ififodepth)\n",
    "    \n",
    "    # Apply SIMD values from the folding configuration to sliding window layers\n",
    "    swg_layers = input_folding_model.get_nodes_by_op_type(\"ConvolutionInputGenerator_rtl\")\n",
    "    for i in range(len(swg_layers)):\n",
    "        swg_inst = getCustomOp(swg_layers[i])\n",
    "        simd = folding_config[i][1]\n",
    "        swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    \n",
    "    # Apply unique node names to all nodes and save the transformed model\n",
    "    input_folding_model = input_folding_model.transform(GiveUniqueNodeNames())\n",
    "    input_folding_model.save(save_name)\n",
    "\n",
    "    return input_folding_model\n"
   ],
   "id": "cd5527c3889d3ed2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = folding_transform(model, set_onnx_checkpoint(Project_Info, \"Folded Model\"))",
   "id": "1269bd96fdeb9db3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "\n",
    "def zynq_build_transform(input_zynq_model, save_name, brd_name):\n",
    "    \"\"\"\n",
    "    Applies the ZynqBuild transformation to a model for the specified PYNQ board and clock period.\n",
    "\n",
    "    Parameters:\n",
    "        input_zynq_model (ModelWrapper): Folded model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "        brd_name (str): Name of the PYNQ board to target.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model after applying ZynqBuild.\n",
    "    \"\"\"\n",
    "    target_clk_ns = 10\n",
    "    # Apply ZynqBuild transformation\n",
    "    input_zynq_model = input_zynq_model.transform(ZynqBuild(platform=brd_name, period_ns=target_clk_ns))\n",
    "    \n",
    "    # Save the transformed model\n",
    "    input_zynq_model.save(save_name)\n",
    "\n",
    "    return input_zynq_model\n"
   ],
   "id": "394bed78132471e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "\n",
    "def pynq_driver_transform(input_driver_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies the MakePYNQDriver transformation to the model to generate a PYNQ-compatible driver.\n",
    "\n",
    "    Parameters:\n",
    "        input_driver_model (ModelWrapper): ZynqBuild model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model with PYNQ driver compatibility.\n",
    "    \"\"\"\n",
    "    # Apply MakePYNQDriver transformation\n",
    "    input_driver_model = input_driver_model.transform(MakePYNQDriver(\"zynq-iodma\"))\n",
    "    \n",
    "    # Save the transformed model\n",
    "    input_driver_model.save(save_name)\n",
    "\n",
    "    return input_driver_model\n"
   ],
   "id": "d1f6d7caa00a8452",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = zynq_build_transform(model, set_onnx_checkpoint(Project_Info, \"Zynq Build\"), Project_Info['Board_name'])\n",
    "model = pynq_driver_transform(model, set_onnx_checkpoint(Project_Info, \"Pynq Driver\"))"
   ],
   "id": "669b7375d6509996",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
