{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:00.065742Z",
     "start_time": "2024-12-14T08:24:56.027240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "\n",
    "def log_message(message, level=\"info\"):\n",
    "    \"\"\"\n",
    "    Log a message, with support for info, warning, and error levels.\n",
    "    If the level is \"error\", this function raises a ValueError with the message.\n",
    "\n",
    "    Parameters:\n",
    "        message (str): The message to log.\n",
    "        level (str): The level of the message (\"info\", \"warning\", or \"error\").\n",
    "    \"\"\"\n",
    "    if level == \"info\":\n",
    "        print(f\"[INFO] {message}\")\n",
    "    elif level == \"warning\":\n",
    "        print(f\"[WARNING] {message}\")\n",
    "    elif level == \"error\":\n",
    "        print(f\"[ERROR] {message}\")\n",
    "        raise ValueError(message)  # Raise an error if level is \"error\"\n",
    "\n",
    "def sanitize_string(s):\n",
    "    \"\"\"\n",
    "    Convert a string to lowercase, strip special characters, and replace spaces with underscores.\n",
    "\n",
    "    Parameters:\n",
    "        s (str): The input string to sanitize.\n",
    "\n",
    "    Returns:\n",
    "        str: The sanitized string.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    s = s.replace(' ', '_')\n",
    "    s = re.sub(r'[^a-z0-9_]', '', s)\n",
    "    return s\n",
    "\n",
    "def ensure_directory_exists(dir_path):\n",
    "    \"\"\"\n",
    "    Ensure the specified directory exists; create it if it does not, including required subdirectories.\n",
    "    Set permissions to 777 for the main directory and its contents recursively.\n",
    "\n",
    "    Parameters:\n",
    "        dir_path (str): The path of the directory to ensure exists.\n",
    "\n",
    "    Returns:\n",
    "        str: The absolute path of the directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # List of required subdirectories\n",
    "    subdirs = ['dataset', 'src', 'checkpoints', 'output']\n",
    "    dirs_to_clear = ['checkpoints', 'output']    \n",
    "    \n",
    "    # Create each subdirectory if it doesnâ€™t exist\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(dir_path, subdir)\n",
    "        os.makedirs(subdir_path, exist_ok=True)\n",
    "    \n",
    "    # Set permissions recursively for main directory and all subdirectories/files\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        os.chmod(root, 0o777)\n",
    "        for d in dirs:\n",
    "            os.chmod(os.path.join(root, d), 0o777)\n",
    "        for f in files:\n",
    "            os.chmod(os.path.join(root, f), 0o777)\n",
    "    \n",
    "    # Clear all contents in the specified directories\n",
    "    for clear_dir in dirs_to_clear:\n",
    "        clear_path = os.path.join(dir_path, clear_dir)\n",
    "        for filename in os.listdir(clear_path):\n",
    "            file_path = os.path.join(clear_path, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)  # Remove file or link\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove directory and all contents\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "    return os.path.abspath(dir_path)\n",
    "\n",
    "def check_file_exists(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file exists at the specified path.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path of the file to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return os.path.isfile(file_path)\n",
    "\n",
    "def is_archive_file(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file is a valid archive (zip or tar).\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path of the file to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file is a valid archive, False otherwise.\n",
    "    \"\"\"\n",
    "    return zipfile.is_zipfile(file_path) or tarfile.is_tarfile(file_path)"
   ],
   "id": "d749f7e3abf77d99",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:00.225691Z",
     "start_time": "2024-12-14T08:25:00.115898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "\n",
    "def setup_project(prj_name, brd_name, model_type, project_folder=None, model_py_file=None, model_pth_file=None, sample_untrained_model=None, sample_pretrained_model=None, finn_pretrained_model=None, dataset_type=None, custom_dataset=None, torch_vision_dataset=None):\n",
    "    \"\"\"\n",
    "    Set up a project with a specified structure, including creating necessary directories, \n",
    "    validating model type, and checking for necessary files.\n",
    "\n",
    "    Parameters:\n",
    "        prj_name (str): The name of the project.\n",
    "        brd_name (str): The name of the target board for the project (e.g., PYNQ board).\n",
    "        model_type (str): The type of model ('untrained', 'sample_untrained', 'custom_pretrained', \n",
    "                          'sample_pretrained', or 'finn_pretrained').\n",
    "        project_folder (str, optional): The main folder for the project. A new folder is generated if not provided.\n",
    "        model_py_file (str, optional): The filename of the Python script defining the model architecture, required \n",
    "                                       for 'untrained' and 'custom_pretrained' models.\n",
    "        model_pth_file (str, optional): The filename of the .pth file with pre-trained weights, required for \n",
    "                                        'custom_pretrained' models.\n",
    "        sample_untrained_model (str, optional): The name of the sample untrained model to load, required for \n",
    "                                                'sample_untrained' models.\n",
    "        sample_pretrained_model (str, optional): The name of the sample pretrained model to load, required for \n",
    "                                                 'sample_pretrained' models.\n",
    "        finn_pretrained_model (str, optional): The name of the FINN pretrained model to load, required for \n",
    "                                               'finn_pretrained' models.\n",
    "        dataset_type (str, optional): Type of dataset for 'untrained' models ('torch_vision_dataset' or \n",
    "                                      'custom_dataset').\n",
    "        custom_dataset (str, optional): Path to the custom dataset file for 'untrained' models with 'custom_dataset' \n",
    "                                        dataset type.\n",
    "        torch_vision_dataset (str, optional): Name of the TorchVision dataset class for 'untrained' models with \n",
    "                                              'torch_vision_dataset' dataset type.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with project setup information including project name, folder path, model details, \n",
    "              and dataset information.\n",
    "    \"\"\"\n",
    "    log_message(\"Setting up project\")\n",
    "    working_folder = \"/home/fastqnn/finn/notebooks/Fast-QNN/outputs/txaviour/\"\n",
    "    prj_info = {}\n",
    "    \n",
    "    # Ensure Project Name is provided\n",
    "    if not prj_name:\n",
    "        log_message(\"Project Name is required\", level=\"error\")\n",
    "\n",
    "    # Sanitize project name and set project info\n",
    "    prj_name_stripped = sanitize_string(prj_name)\n",
    "    display_name = prj_name\n",
    "    prj_info[\"Display_Name\"] = display_name\n",
    "    prj_info[\"Stripped_Name\"] = prj_name_stripped\n",
    "\n",
    "    if not project_folder:\n",
    "        # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") # for production mode\n",
    "        timestamp = \"0\"\n",
    "        project_folder = f\"{working_folder}{prj_name_stripped}_{timestamp}\"\n",
    "    \n",
    "    folder_path = ensure_directory_exists(project_folder)\n",
    "    prj_info[\"Folder\"] = folder_path\n",
    "    \n",
    "    available_boards = pynq_part_map.keys()\n",
    "    if brd_name in available_boards:\n",
    "        prj_info[\"Board_name\"] = brd_name\n",
    "    else:\n",
    "        log_message(f\"'{brd_name}' is not a valid board name. Available board names are: {available_boards}\", level=\"error\")\n",
    "\n",
    "    # Validate Model Type\n",
    "    valid_model_types = [\"untrained\", \"sample_untrained\", \"custom_pretrained\", \"sample_pretrained\", \"finn_pretrained\"]\n",
    "    if model_type not in valid_model_types:\n",
    "        log_message(f\"Model Type must be one of {valid_model_types}\", level=\"error\")\n",
    "    prj_info['Model_Type'] = model_type\n",
    "    \n",
    "    # Handle Model Type specific requirements\n",
    "    if model_type in [\"untrained\", \"custom_pretrained\"]:\n",
    "        if not model_py_file or not check_file_exists(os.path.join(project_folder, 'src', model_py_file)):\n",
    "            log_message(f\"Model Py File '{model_py_file}' does not exist in '{project_folder}'\", level=\"error\")\n",
    "        prj_info[\"Model_Py_File\"] = model_py_file\n",
    "\n",
    "    if model_type == \"custom_pretrained\":\n",
    "        if not model_pth_file or not check_file_exists(os.path.join(project_folder, 'src', model_pth_file)):\n",
    "            log_message(f\"Model Pth File '{model_pth_file}' does not exist in '{project_folder}'\", level=\"error\")\n",
    "        prj_info[\"Model_Pth_File\"] = model_pth_file\n",
    "\n",
    "    if model_type == \"sample_pretrained\":\n",
    "        available_models = [\"alexnet_8w8a_mnist\", \"resnet_mnist\", \"alexnet_cifar10\", \"resnet_cifar10\"]\n",
    "        if not sample_pretrained_model or sample_pretrained_model not in available_models:\n",
    "            log_message(f\"Sample Pretrained Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Pretrained_Model\"] = sample_pretrained_model\n",
    "        \n",
    "    if model_type == \"sample_untrained\":\n",
    "        available_models = [\"alexnet\", \"resnet\"]\n",
    "        if not sample_untrained_model or sample_untrained_model not in available_models:\n",
    "            log_message(f\"Sample Untrained Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Sample_Untrained_Model\"] = sample_untrained_model\n",
    "\n",
    "    if model_type == \"finn_pretrained\":\n",
    "        available_models = [\"cnv_1w1a\", \"cnv_1w2a\", \"cnv_2w2a\", \"lfc_1w1a\", \"lfc_1w2a\", \"sfc_1w1a\", \"sfc_1w2a\", \"sfc_2w2a\", \"tfc_1w1a\", \"tfc_1w2a\", \"tfc_2w2a\", \"quant_mobilenet_v1_4b\"]\n",
    "        if not finn_pretrained_model or finn_pretrained_model not in available_models:\n",
    "            log_message(f\"Finn Pretrained Model must be one of: {available_models}\", level=\"error\")\n",
    "        prj_info[\"Pretrained_Model\"] = finn_pretrained_model\n",
    "\n",
    "    # Handle Dataset requirements for Untrained models\n",
    "    if model_type == \"untrained\" or model_type == \"sample_untrained\":\n",
    "        if dataset_type not in [\"torch_vision_dataset\", \"custom_dataset\"]:\n",
    "            log_message(\"Dataset Type must be either 'Torch Vision' or 'Custom Dataset' for Untrained models\", level=\"error\")\n",
    "        prj_info[\"Dataset_Type\"] = dataset_type\n",
    "\n",
    "        if dataset_type == \"custom_dataset\":\n",
    "            custom_dataset_path = os.path.join(project_folder, 'dataset', custom_dataset)\n",
    "            if not custom_dataset or not check_file_exists(custom_dataset_path) or not is_archive_file(custom_dataset_path):\n",
    "                log_message(f\"Custom Dataset '{custom_dataset}' must exist in '{project_folder}' and be an archive file (zip or tar)\", level=\"error\")\n",
    "            prj_info[\"Custom_Dataset\"] = custom_dataset\n",
    "\n",
    "        elif dataset_type == \"torch_vision_dataset\":\n",
    "            available_datasets = [cls_name.lower() for cls_name in dir(datasets) if not cls_name.startswith('_')]\n",
    "            if not torch_vision_dataset or torch_vision_dataset.lower() not in available_datasets:\n",
    "                log_message(f\"Torch Vision Dataset must be one of: {available_datasets}\", level=\"error\")\n",
    "            prj_info[\"Torch_Vision_Dataset\"] = torch_vision_dataset\n",
    "            \n",
    "    log_message(f\"Project setup complete. {prj_name} has been initialized.\")\n",
    "    return prj_info"
   ],
   "id": "4df70eb120061ecb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:02.654060Z",
     "start_time": "2024-12-14T08:25:01.006834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "\n",
    "class QuantAlexNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A slightly adapted AlexNet-like architecture for e.g. MNIST-like inputs.\n",
    "    Uses Brevitas quantization layers (QuantConv2d, QuantLinear, QuantReLU).\n",
    "    Explicitly sets 'weight_quant_type' and 'quant_type' so that FINN can parse them.\n",
    "    Layer-by-layer structure (no nn.Sequential).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_bits=1, num_classes=10):\n",
    "        super(QuantAlexNet, self).__init__()\n",
    "\n",
    "        # Decide on quantization type based on bit-width\n",
    "        if num_bits == 1:\n",
    "            weight_quant_type = QuantType.BINARY\n",
    "            act_quant_type = QuantType.BINARY\n",
    "        else:\n",
    "            weight_quant_type = QuantType.INT\n",
    "            act_quant_type = QuantType.INT\n",
    "\n",
    "        # ---------------------------\n",
    "        #  CONVOLUTIONAL FEATURES\n",
    "        # ---------------------------\n",
    "\n",
    "        # 1st Conv block\n",
    "        self.conv1 = qnn.QuantConv2d(\n",
    "            in_channels=3, out_channels=64, kernel_size=11, stride=4, padding=2,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act1 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 2nd Conv block\n",
    "        self.conv2 = qnn.QuantConv2d(\n",
    "            in_channels=64, out_channels=192, kernel_size=5, padding=2,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act2 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3rd Conv block\n",
    "        self.conv3 = qnn.QuantConv2d(\n",
    "            in_channels=192, out_channels=384, kernel_size=3, padding=1,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act3 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "\n",
    "        # 4th Conv block\n",
    "        self.conv4 = qnn.QuantConv2d(\n",
    "            in_channels=384, out_channels=256, kernel_size=3, padding=1,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act4 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "\n",
    "        # 5th Conv block\n",
    "        self.conv5 = qnn.QuantConv2d(\n",
    "            in_channels=256, out_channels=256, kernel_size=3, padding=1,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act5 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "        # Skipping final MaxPool because of the small input size (28x28).\n",
    "\n",
    "        # ---------------------------\n",
    "        #    CLASSIFIER LAYERS\n",
    "        # ---------------------------\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc1 = qnn.QuantLinear(\n",
    "            256, 4096,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act_fc1 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = qnn.QuantLinear(\n",
    "            4096, 4096,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "        self.act_fc2 = qnn.QuantReLU(bit_width=num_bits, quant_type=act_quant_type)\n",
    "\n",
    "        self.fc3 = qnn.QuantLinear(\n",
    "            4096, num_classes,\n",
    "            weight_bit_width=num_bits, weight_quant_type=weight_quant_type, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st Conv block\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # 2nd Conv block\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # 3rd Conv block\n",
    "        x = self.conv3(x)\n",
    "        x = self.act3(x)\n",
    "\n",
    "        # 4th Conv block\n",
    "        x = self.conv4(x)\n",
    "        x = self.act4(x)\n",
    "\n",
    "        # 5th Conv block\n",
    "        x = self.conv5(x)\n",
    "        x = self.act5(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Classifier\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fc1(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act_fc2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "8240186d1a3b0798",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:03.137605Z",
     "start_time": "2024-12-14T08:25:02.808097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.util.test import get_test_model_trained\n",
    "\n",
    "\n",
    "def load_pretrained_model(model_name, model_type, src_folder, initial_channels = 3, max_size = 4096):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model from TorchVision and ensures all downloads are in the src folder of the Project.\n",
    "\n",
    "    Parameters:\n",
    "        model_type: \n",
    "        initial_channels: \n",
    "        max_size: \n",
    "        model_name (str): The name of the pre-trained model to load (e.g., 'alexnet', 'resnet50').\n",
    "        src_folder (str): The folder where model downloads will be stored. Default is 'src'.\n",
    "                \n",
    "    Returns:\n",
    "        torch.nn.Module: The loaded pre-trained model.\n",
    "    \"\"\"\n",
    "    log_message(f\"Loading {model_type} Model: {model_name}\")\n",
    "    # Ensure the src folder exists\n",
    "    os.makedirs(src_folder, exist_ok=True)\n",
    "    # Set TORCH_HOME to the src folder to store the model downloads there\n",
    "    os.environ['TORCH_HOME'] = src_folder\n",
    "    pretrained_model = None\n",
    "    if model_type == \"sample_pretrained\":\n",
    "        if model_name == \"alexnet_8w8a_mnist\":\n",
    "            pretrained_model = QuantAlexNet(num_bits=3, num_classes=10).to('cpu')\n",
    "            pretrained_model.load_state_dict(torch.load(\"/home/fastqnn/finn/notebooks/Fast-QNN/ggg.pth\", map_location=torch.device('cpu')))\n",
    "            pretrained_model.eval()\n",
    "    elif model_type == \"finn_pretrained\":\n",
    "        if model_name == \"cnv_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 1, 1)\n",
    "        elif model_name == \"cnv_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 1, 2)\n",
    "        elif model_name == \"cnv_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"CNV\", 2, 2)\n",
    "        elif model_name == \"lfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"LFC\", 1, 1)\n",
    "        elif model_name == \"lfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"LFC\", 1, 2)\n",
    "        elif model_name == \"sfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 1, 1)\n",
    "        elif model_name == \"sfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 1, 2)\n",
    "        elif model_name == \"sfc_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"SFC\", 2, 2)\n",
    "        elif model_name == \"tfc_1w1a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 1, 1)\n",
    "        elif model_name == \"tfc_1w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 1, 2)\n",
    "        elif model_name == \"tfc_2w2a\":\n",
    "            pretrained_model = get_test_model_trained(\"TFC\", 2, 2)\n",
    "        elif model_name == \"quant_mobilenet_v1_4b\":\n",
    "            pretrained_model = get_test_model_trained(\"mobilenet\", 4, 4)\n",
    "    # List of common input shapes to test first (both square and non-square)\n",
    "    common_shapes = [\n",
    "        (1, initial_channels, 32, 32),  # Typical for many models\n",
    "        (1, initial_channels, 28, 28),  # Typical for many models\n",
    "        (1, initial_channels, 224, 224),  # Typical for many models\n",
    "        (1, initial_channels, 299, 299),  # For models like Inception\n",
    "        (1, initial_channels, 128, 128),  # Smaller size\n",
    "        (1, initial_channels, 256, 256),  # Larger square size\n",
    "        (1, initial_channels, 320, 240),  # Common non-square size\n",
    "        (1, initial_channels, 240, 320),  # Non-square (swapped dimensions)\n",
    "        (1, initial_channels, 256, 128),  # Non-square\n",
    "        (1, initial_channels, 128, 256),  # Non-square (swapped)\n",
    "    ]\n",
    "\n",
    "    # Try common shapes first\n",
    "    for shape in common_shapes:\n",
    "        try:\n",
    "            dummy_input = torch.rand(*shape)\n",
    "            pretrained_model(dummy_input)\n",
    "            log_message(f\"Compatible common input shape found: {shape}\")\n",
    "            return pretrained_model, shape\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "\n",
    "    # If no common shape worked, test all possible square and non-square shapes up to max_size\n",
    "    for width in range(1, max_size + 1, 1):  # Step by 16 for efficiency\n",
    "        for height in range(1, max_size + 1, 1):\n",
    "            try:\n",
    "                dummy_input = torch.rand(1, initial_channels, width, height)\n",
    "                pretrained_model(dummy_input)\n",
    "                pretrained_model_input_shape = (1, initial_channels, width, height)\n",
    "                log_message(f\"Compatible input shape found: {pretrained_model_input_shape}\")\n",
    "                return pretrained_model, pretrained_model_input_shape\n",
    "            except RuntimeError:\n",
    "                continue\n",
    "\n",
    "    log_message(\"Could not determine a compatible input shape within the specified range.\", level=\"error\")"
   ],
   "id": "e912d407f448ecb4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3b4a8b34b65b86a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:04.518359Z",
     "start_time": "2024-12-14T08:25:03.174458Z"
    }
   },
   "source": [
    "prj_name_input = \"Alexnet 8w8a MNIST\"\n",
    "board_name_input = \"Pynq-ZU\"\n",
    "prj_folder_input = sanitize_string(prj_name_input)\n",
    "model_type_input = \"sample_pretrained\"\n",
    "pretrained_model_name_input = \"alexnet_8w8a_mnist\"\n",
    "Project_Info = setup_project(prj_name=prj_name_input, brd_name=board_name_input, model_type=model_type_input, sample_pretrained_model=pretrained_model_name_input)\n",
    "\n",
    "input_model= None\n",
    "input_model_shape = None\n",
    "\n",
    "if Project_Info['Model_Type'] == \"untrained\":\n",
    "    log_message(\"Training for untrained models are not supported at the moment!\", level=\"error\")\n",
    "elif Project_Info['Model_Type'] == \"custom_pretrained\":\n",
    "    log_message(\"Custom Pretrained models are not supported at the moment!\", level=\"error\")\n",
    "elif Project_Info['Model_Type'] == \"sample_pretrained\" or Project_Info['Model_Type'] == \"finn_pretrained\":\n",
    "    pretrained_folder = os.path.join(Project_Info['Folder'],\"src\")\n",
    "    input_model, input_model_shape = load_pretrained_model(Project_Info['Pretrained_Model'], Project_Info['Model_Type'], pretrained_folder, initial_channels=3)\n",
    "else:\n",
    "    log_message(\"Unsupported Model Type\", level=\"error\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting up project\n",
      "[INFO] Project setup complete. Alexnet 8w8a MNIST has been initialized.\n",
      "[INFO] Loading sample_pretrained Model: alexnet_8w8a_mnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Compatible common input shape found: (1, 3, 32, 32)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:04.683818Z",
     "start_time": "2024-12-14T08:25:04.673856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_onnx_checkpoint(project_info, suffix):\n",
    "    \"\"\"\n",
    "    Generates the export path for ONNX files based on a specified suffix.\n",
    "\n",
    "    Parameters:\n",
    "        project_info (dict): Dictionary containing project information (e.g., 'Folder' and 'Stripped_Name').\n",
    "        suffix (str): The suffix to append to the exported ONNX filename (e.g., \"model1\" for \"model1_export.onnx\").\n",
    "\n",
    "    Returns:\n",
    "        str: The full path to the export file.\n",
    "    \"\"\"\n",
    "    log_message(f\"Saving Checkpoint: {suffix}\")\n",
    "    suffix = sanitize_string(suffix)\n",
    "    filename = f\"{project_info['Stripped_Name']}_{suffix}.onnx\"\n",
    "    return os.path.join(project_info['Folder'], \"checkpoints\", filename)\n"
   ],
   "id": "1f4ae7b982c2a64c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:13.902587Z",
     "start_time": "2024-12-14T08:25:04.699703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "export_onnx_path = set_onnx_checkpoint(Project_Info,\"Brevitas Export\")\n",
    "export_qonnx(input_model, torch.randn(input_model_shape), export_onnx_path, opset_version=9)\n",
    "qonnx_cleanup(export_onnx_path, out_file=export_onnx_path)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Brevitas Export\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:20.956288Z",
     "start_time": "2024-12-14T08:25:13.960144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "model = ModelWrapper(export_onnx_path)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"QONNX to FINN\"))"
   ],
   "id": "de436732503bc809",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: QONNX to FINN\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:21.074288Z",
     "start_time": "2024-12-14T08:25:21.061955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from qonnx.transformation.remove import RemoveIdentityOps\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import (GiveReadableTensorNames,\n",
    "                                          GiveUniqueNodeNames,\n",
    "                                          RemoveStaticGraphInputs,\n",
    "                                          RemoveUnusedTensors, GiveUniqueParameterTensors, SortGraph)\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "def tidy_up_transforms(input_tidy_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a series of transformations to a model and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_tidy_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply transformations\n",
    "    input_tidy_model = input_tidy_model.transform(GiveUniqueParameterTensors())\n",
    "    input_tidy_model = input_tidy_model.transform(InferShapes())\n",
    "    input_tidy_model = input_tidy_model.transform(FoldConstants())\n",
    "    input_tidy_model = input_tidy_model.transform(GiveUniqueNodeNames())\n",
    "    input_tidy_model = input_tidy_model.transform(GiveReadableTensorNames())\n",
    "    input_tidy_model = input_tidy_model.transform(InferDataTypes())\n",
    "    input_tidy_model = input_tidy_model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "    # Save the transformed model\n",
    "    input_tidy_model.save(save_name)\n",
    "\n",
    "    return input_tidy_model"
   ],
   "id": "a5d959d6407a1ec2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:22.372702Z",
     "start_time": "2024-12-14T08:25:21.123964Z"
    }
   },
   "cell_type": "code",
   "source": "model = tidy_up_transforms(model, set_onnx_checkpoint(Project_Info,\"Tidy ONNX Post Finn\"))",
   "id": "21fb27ba948a8cbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Tidy ONNX Post Finn\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:22.448298Z",
     "start_time": "2024-12-14T08:25:22.436643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.util.pytorch import ToTensor\n",
    "\n",
    "log_message(\"Skipping Pre-Processing. Will be expecting the user to handle it in application!\", level=\"warning\")"
   ],
   "id": "511d18741790a4a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Skipping Pre-Processing. Will be expecting the user to handle it in application!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:22.522268Z",
     "start_time": "2024-12-14T08:25:22.508164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = set_onnx_checkpoint(Project_Info,\"Pre Proc ONNX Finn\")\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name, opset_version=9)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\"\"\""
   ],
   "id": "57af62ce8ef3c010",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nglobal_inp_name = model.graph.input[0].name\\nishape = model.get_tensor_shape(global_inp_name)\\n# preprocessing: torchvision\\'s ToTensor divides uint8 inputs by 255\\ntotensor_pyt = ToTensor()\\nchkpt_preproc_name = set_onnx_checkpoint(Project_Info,\"Pre Proc ONNX Finn\")\\nexport_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name, opset_version=9)\\nqonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\\npre_model = ModelWrapper(chkpt_preproc_name)\\npre_model = pre_model.transform(ConvertQONNXtoFINN())\\nmodel = model.transform(MergeONNXModels(pre_model))\\n# add input quantization annotation: UINT8 for all BNN-PYNQ models\\nglobal_inp_name = model.graph.input[0].name\\nmodel.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:23.518076Z",
     "start_time": "2024-12-14T08:25:22.617020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiplyByOne(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = x * 2\n",
    "        out = out + 0  # Adding a no-op to prevent graph simplification\n",
    "        out = out / 2  # Adding a no-op to prevent graph simplification\n",
    "        return out\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "mul_model = MultiplyByOne()\n",
    "\n",
    "chkpt_mul_name = set_onnx_checkpoint(Project_Info, \"Mul_One_ONNX\")\n",
    "export_qonnx(mul_model, torch.randn(ishape), chkpt_mul_name, opset_version=9)\n",
    "qonnx_cleanup(chkpt_mul_name, out_file=chkpt_mul_name)\n",
    "\n",
    "mul_model_wrapper = ModelWrapper(chkpt_mul_name)\n",
    "mul_model_wrapper = mul_model_wrapper.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(MergeONNXModels(mul_model_wrapper))\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])"
   ],
   "id": "13c356373544a8a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Mul_One_ONNX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/home_dir/.local/lib/python3.10/site-packages/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:24.927239Z",
     "start_time": "2024-12-14T08:25:23.545634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"Post Processing\"))\n",
    "model = tidy_up_transforms(model, set_onnx_checkpoint(Project_Info,\"Tidy Post PrePost Proc\"))"
   ],
   "id": "49bbd723443c4e65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Post Processing\n",
      "[INFO] Saving Checkpoint: Tidy Post PrePost Proc\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:25.050988Z",
     "start_time": "2024-12-14T08:25:25.030030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from finn.transformation.qonnx.qonnx_activation_handlers import QuantActBaseHandler, np_default_dtype\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "\n",
    "class ReluToMultiThresholdHandler(QuantActBaseHandler):\n",
    "    \"\"\"Class for converting a regular non-quantized ReLU operation to a MultiThreshold node in Xilinx FINN.\"\"\"\n",
    "    def _check_compatibility(self):\n",
    "        # Check if the ReLU operation is supported for transformation\n",
    "        pass\n",
    "\n",
    "    def _calculate_act_bias(self):\n",
    "        # No bias is applied for standard ReLU\n",
    "        return np.array([0.0], dtype=np_default_dtype)\n",
    "\n",
    "    def _calculate_thresholds(self):\n",
    "        # Thresholds for ReLU are typically defined as a single zero threshold\n",
    "        output_shape = self._model.get_tensor_shape(self._q_node.output[0])\n",
    "        num_output_channels = output_shape[1]  # Assuming NCHW or NHWC format\n",
    "        thresholds = np.zeros((num_output_channels, 1), dtype=np_default_dtype)\n",
    "        return thresholds\n",
    "\n",
    "    def _calculate_act_scale(self):\n",
    "        # Standard ReLU does not apply scaling\n",
    "        return np.array([1.0], dtype=np_default_dtype)\n",
    "\n",
    "    def _remove_activation_node(self, multi_threshold_node):\n",
    "        # Find the activation node\n",
    "        act_node = self._model.find_direct_predecessors(self._q_node)\n",
    "        if act_node is None:\n",
    "            raise RuntimeError(\n",
    "                \"For handling of Relu activations a predecesor to dgdgdg\"\n",
    "            )\n",
    "        act_node = act_node[0]\n",
    "        if act_node.op_type != \"Relu\":\n",
    "            raise RuntimeError(\n",
    "                \"The predecesor of the Quant node must be Relu for handling \"\n",
    "                \"of activations.\"\n",
    "            )\n",
    "\n",
    "        # Remove the activation node\n",
    "        self._model.graph.node.remove(act_node)\n",
    "        return multi_threshold_node\n",
    "\n",
    "    def calculate_node_parameters(self):\n",
    "        # Calculate all parameters for replacing ReLU with MultiThreshold\n",
    "        return {\n",
    "            \"out_dtype\": \"FLOAT32\",  # Regular ReLU typically works with float32\n",
    "            \"thresholds\": self._calculate_thresholds(),\n",
    "            \"adder_bias\": self._calculate_act_bias(),\n",
    "            \"mul_scale\": self._calculate_act_scale(),\n",
    "        }\n",
    "\n",
    "    def replace_relu_node(self):\n",
    "        \"\"\"Replace a regular ReLU operation with a MultiThreshold node.\"\"\"\n",
    "        # Check compatibility\n",
    "        self._check_compatibility()\n",
    "\n",
    "        # Shorten instance variables\n",
    "        model = self._model\n",
    "        graph = model.graph\n",
    "        n = self._q_node\n",
    "        running_node_index = self._q_index\n",
    "\n",
    "        # Calculate parameters for the MultiThreshold node\n",
    "        parameter_dict = self.calculate_node_parameters()\n",
    "        thresholds = parameter_dict[\"thresholds\"]\n",
    "        adder_bias = parameter_dict[\"adder_bias\"]\n",
    "        mul_scale = parameter_dict[\"mul_scale\"]\n",
    "        out_dtype = parameter_dict[\"out_dtype\"]\n",
    "\n",
    "        # Create threshold tensor\n",
    "        thresh_tensor = oh.make_tensor_value_info(\n",
    "            model.make_new_valueinfo_name(),\n",
    "            TensorProto.FLOAT,\n",
    "            thresholds.shape,\n",
    "        )\n",
    "        graph.value_info.append(thresh_tensor)\n",
    "        model.set_initializer(thresh_tensor.name, thresholds)\n",
    "        act_node = self._model.find_direct_predecessors(self._q_node)\n",
    "        act_node = act_node[0]\n",
    "        # Insert MultiThreshold node\n",
    "        mt_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            [act_node.input[0], thresh_tensor.name],\n",
    "            [n.output[0]],\n",
    "            out_dtype=out_dtype,\n",
    "            domain=\"qonnx.custom_op.general\",\n",
    "        )\n",
    "        graph.node.insert(running_node_index, mt_node)\n",
    "        running_node_index += 1\n",
    "\n",
    "        # Get MultiThreshold instance\n",
    "        mt_inst = getCustomOp(graph.node[running_node_index - 1])\n",
    "\n",
    "        # Set bias and scale attributes\n",
    "        mt_inst.set_nodeattr(\"out_scale\", mul_scale[0].item())\n",
    "        mt_inst.set_nodeattr(\"out_bias\", adder_bias[0].item())\n",
    "        mt_inst.set_nodeattr(\"out_dtype\", out_dtype)\n",
    "\n",
    "        # Remove the original ReLU node\n",
    "        self._remove_activation_node(mt_node)\n",
    "        graph.node.remove(n)\n",
    "\n",
    "        # Return the updated model\n",
    "        return self._model\n"
   ],
   "id": "a7dc93abb28939d8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:25.168099Z",
     "start_time": "2024-12-14T08:25:25.151969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.qonnx.qonnx_activation_handlers import QuantReluHandler\n",
    "from onnx import TensorProto\n",
    "from qonnx.transformation.base import Transformation\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from onnx import helper as oh\n",
    "\n",
    "class CustomReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Implementing an approximation of abs(x) using only arithmetic operations\n",
    "        epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "        abs_x = (x * x + epsilon) / (x + epsilon)\n",
    "        return (x + abs_x) / 2\n",
    "\n",
    "class CustomReluBreakdown(Transformation):\n",
    "    \"\"\"\n",
    "    Custom transformation to iterate through the nodes, identify ReLU nodes,\n",
    "    and convert the next node to a quant node while applying the QuantReluHandler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bit_width=1, quant_type=DataType[\"UINT8\"], narrow=0, signed=0):\n",
    "        super().__init__()\n",
    "        self.bit_width = bit_width\n",
    "        self.quant_type = quant_type\n",
    "        self.narrow = narrow\n",
    "        self.signed = signed\n",
    "\n",
    "    def apply(self, transform_model):\n",
    "        graph = transform_model.graph\n",
    "        graph_modified = False\n",
    "        convert_next_node = False\n",
    "\n",
    "        for idx, node in enumerate(graph.node):\n",
    "            if convert_next_node:\n",
    "                # Call QuantReluHandler on the current Quant node\n",
    "                handler = ReluToMultiThresholdHandler(transform_model, node, idx)\n",
    "                handler.replace_relu_node()\n",
    "\n",
    "                convert_next_node = False\n",
    "                graph_modified = True\n",
    "\n",
    "            if node.op_type == \"Relu\":\n",
    "                # Raise a flag to convert the next node to a quant node\n",
    "                convert_next_node = True\n",
    "\n",
    "       # transform_model = transform_model.transform(InferShapes())\n",
    "        return (transform_model, graph_modified)\n"
   ],
   "id": "a45466c16fde453c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:25.288636Z",
     "start_time": "2024-12-14T08:25:25.262807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from onnx import helper as oh\n",
    "from onnx import TensorProto\n",
    "from qonnx.transformation.base import Transformation\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReluToMultiThresholdTransform(Transformation):\n",
    "    \"\"\"\n",
    "    Custom transformation to replace ReLU nodes with MultiThreshold nodes\n",
    "    in Xilinx FINN. Works directly on ReLU nodes and reroutes their inputs/outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply(self, transform_model):\n",
    "        graph = transform_model.graph\n",
    "        graph_modified = False\n",
    "\n",
    "        for idx, node in enumerate(graph.node):\n",
    "            if node.op_type == \"Relu\":\n",
    "                # Replace the ReLU node with a MultiThreshold node\n",
    "                self.replace_relu_with_multithreshold(transform_model, node, idx)\n",
    "                graph_modified = True\n",
    "\n",
    "        return (transform_model, graph_modified)\n",
    "\n",
    "    def replace_relu_with_multithreshold(self, transform_model, relu_node, relu_index):\n",
    "        \"\"\"\n",
    "        Replaces a ReLU node with a MultiThreshold node.\n",
    "        \"\"\"\n",
    "        graph = transform_model.graph\n",
    "\n",
    "        # Get the input to the ReLU node (its predecessor)\n",
    "        if len(relu_node.input) != 1:\n",
    "            raise RuntimeError(f\"ReLU node {relu_node.name} has unexpected inputs.\")\n",
    "        relu_input = relu_node.input[0]\n",
    "\n",
    "        # Get the output of the ReLU node (its successor)\n",
    "        if len(relu_node.output) != 1:\n",
    "            raise RuntimeError(f\"ReLU node {relu_node.name} has unexpected outputs.\")\n",
    "        relu_output = relu_node.output[0]\n",
    "\n",
    "        # Calculate parameters for MultiThreshold\n",
    "        thresholds = self._calculate_thresholds(transform_model, relu_node)\n",
    "        adder_bias = self._calculate_act_bias()\n",
    "        mul_scale = self._calculate_act_scale()\n",
    "        out_dtype = \"FLOAT32\"\n",
    "\n",
    "        # Create threshold tensor\n",
    "        thresh_tensor = oh.make_tensor_value_info(\n",
    "            transform_model.make_new_valueinfo_name(),\n",
    "            TensorProto.FLOAT,\n",
    "            thresholds.shape,\n",
    "        )\n",
    "        graph.value_info.append(thresh_tensor)\n",
    "        transform_model.set_initializer(thresh_tensor.name, thresholds)\n",
    "\n",
    "        # Create MultiThreshold node\n",
    "        mt_node = oh.make_node(\n",
    "            \"MultiThreshold\",\n",
    "            inputs=[relu_input, thresh_tensor.name],\n",
    "            outputs=[relu_output],\n",
    "            domain=\"qonnx.custom_op.general\",\n",
    "        )\n",
    "        mt_node.attribute.extend([\n",
    "            oh.make_attribute(\"out_scale\", float(mul_scale[0])),\n",
    "            oh.make_attribute(\"out_bias\", float(adder_bias[0])),\n",
    "            oh.make_attribute(\"out_dtype\", out_dtype),\n",
    "        ])\n",
    "\n",
    "        # Insert MultiThreshold node in place of ReLU node\n",
    "        graph.node.insert(relu_index, mt_node)\n",
    "\n",
    "        # Remove the original ReLU node\n",
    "        graph.node.remove(relu_node)\n",
    "\n",
    "    def _calculate_thresholds(self, transform_model, relu_node):\n",
    "        \"\"\"\n",
    "        Calculates the thresholds for the MultiThreshold node based on the ReLU operation.\n",
    "        \"\"\"\n",
    "        # ReLU threshold is always zero\n",
    "        output_shape = transform_model.get_tensor_shape(relu_node.output[0])\n",
    "        num_output_channels = output_shape[1]  # Assuming NCHW or NHWC format\n",
    "        thresholds = np.zeros((num_output_channels, 1), dtype=np.float32)\n",
    "        return thresholds\n",
    "\n",
    "    def _calculate_act_bias(self):\n",
    "        \"\"\"\n",
    "        Calculates the bias for the MultiThreshold node.\n",
    "        \"\"\"\n",
    "        # ReLU does not apply a bias\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "\n",
    "    def _calculate_act_scale(self):\n",
    "        \"\"\"\n",
    "        Calculates the scale for the MultiThreshold node.\n",
    "        \"\"\"\n",
    "        # ReLU does not apply scaling\n",
    "        return np.array([1.0], dtype=np.float32)\n"
   ],
   "id": "53fa26aa1d5bd4e3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:25:25.408597Z",
     "start_time": "2024-12-14T08:25:25.378960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import traceback\n",
    "from qonnx.transformation.batchnorm_to_affine import BatchNormToAffine\n",
    "from qonnx.transformation.general import ConvertSubToAdd, ConvertDivToMul\n",
    "from finn.transformation.streamline import Streamline, RoundAndClipThresholds, CollapseRepeatedMul, ConvertSignToThres, \\\n",
    "    CollapseRepeatedAdd\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from finn.transformation.streamline.absorb import (AbsorbTransposeIntoMultiThreshold,\n",
    "                                                   AbsorbScalarMulAddIntoTopK,\n",
    "                                                   AbsorbSignBiasIntoMultiThreshold,\n",
    "                                                   AbsorbAddIntoMultiThreshold,\n",
    "                                                   AbsorbMulIntoMultiThreshold, FactorOutMulSignMagnitude,\n",
    "                                                   Absorb1BitMulIntoMatMul, Absorb1BitMulIntoConv)\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants, MoveAddPastMul, \\\n",
    "    MoveScalarAddPastMatMul, MoveAddPastConv, MoveScalarMulPastMatMul, MoveScalarMulPastConv, \\\n",
    "    MoveMaxPoolPastMultiThreshold, MoveLinearPastEltwiseAdd, MoveLinearPastFork\n",
    "from finn.transformation.streamline.absorb import AbsorbConsecutiveTransposes\n",
    "\n",
    "def streamline_transforms(input_streamline_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a series of streamlining transformations to a model and saves the resulting model.\n",
    "    If a transformation fails, it logs the error and moves on to the next transformation.\n",
    "\n",
    "    Parameters:\n",
    "        input_streamline_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        AbsorbSignBiasIntoMultiThreshold(),\n",
    "        ConvertSubToAdd(),\n",
    "        ConvertDivToMul(),\n",
    "        CollapseRepeatedMul(),\n",
    "        BatchNormToAffine(),\n",
    "        ConvertSignToThres(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarAddPastMatMul(),\n",
    "        MoveAddPastConv(),\n",
    "        MoveScalarMulPastMatMul(),\n",
    "        MoveScalarMulPastConv(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarLinearPastInvariants(),\n",
    "        CollapseRepeatedAdd(),\n",
    "        AbsorbAddIntoMultiThreshold(),\n",
    "        FactorOutMulSignMagnitude(),\n",
    "        MoveMaxPoolPastMultiThreshold(),\n",
    "        AbsorbMulIntoMultiThreshold(),\n",
    "        Absorb1BitMulIntoMatMul(),\n",
    "        Absorb1BitMulIntoConv(),\n",
    "        AbsorbMulIntoMultiThreshold(),\n",
    "        Streamline(),\n",
    "        LowerConvsToMatMul(),\n",
    "        MakeMaxPoolNHWC(),\n",
    "        AbsorbTransposeIntoMultiThreshold(),\n",
    "        ConvertBipolarMatMulToXnorPopcount(),\n",
    "        Streamline(),\n",
    "        AbsorbScalarMulAddIntoTopK(),\n",
    "        RoundAndClipThresholds(),\n",
    "        MoveLinearPastEltwiseAdd(),\n",
    "        MoveLinearPastFork()\n",
    "    ]\n",
    "    \n",
    "    input_streamline_model = input_streamline_model.transform(ReluToMultiThresholdTransform())\n",
    "    input_streamline_model = tidy_up_transforms(input_streamline_model, save_name)\n",
    "    for iter_id in range(3):\n",
    "        for transform in transformations:\n",
    "            try:\n",
    "                input_streamline_model = input_streamline_model.transform(transform)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "        input_streamline_model = tidy_up_transforms(input_streamline_model, save_name)\n",
    "        \n",
    "        input_streamline_model = input_streamline_model.transform(InferDataLayouts())\n",
    "        input_streamline_model = input_streamline_model.transform(RemoveUnusedTensors())\n",
    "        input_streamline_model = input_streamline_model.transform(DoubleToSingleFloat())\n",
    "        input_streamline_model = input_streamline_model.transform(SortGraph())\n",
    "        input_streamline_model = input_streamline_model.transform(RemoveIdentityOps())\n",
    "    \n",
    "    return input_streamline_model"
   ],
   "id": "6bdb92934093ceb8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:27:58.538369Z",
     "start_time": "2024-12-14T08:25:25.500266Z"
    }
   },
   "cell_type": "code",
   "source": "model = streamline_transforms(model, set_onnx_checkpoint(Project_Info,\"Streamlined ONNX\"))",
   "id": "d969950c0d96fc53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Streamlined ONNX\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:27:58.623775Z",
     "start_time": "2024-12-14T08:27:58.608344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.fpgadataflow.convert_to_hw_layers import (\n",
    "    InferBinaryMatrixVectorActivation,\n",
    "    InferQuantizedMatrixVectorActivation,\n",
    "    InferLabelSelectLayer,\n",
    "    InferThresholdingLayer,\n",
    "    InferStreamingMaxPool,\n",
    "    InferConvInpGen,\n",
    "    InferAddStreamsLayer,\n",
    "    InferChannelwiseLinearLayer,\n",
    "    InferConcatLayer,\n",
    "    InferDuplicateStreamsLayer,\n",
    "    InferGlobalAccPoolLayer,\n",
    "    InferLookupLayer,\n",
    "    InferPool,\n",
    "    InferStreamingEltwise,\n",
    "    InferUpsample,\n",
    "    InferVectorVectorActivation\n",
    ")\n",
    "\n",
    "def to_hw_transforms(input_hw_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies a comprehensive series of hardware-oriented transformations to a model and saves the resulting model.\n",
    "    If a transformation fails, it logs the error and moves on to the next transformation.\n",
    "\n",
    "    Parameters:\n",
    "        input_hw_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The path to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model.\n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        InferBinaryMatrixVectorActivation(),\n",
    "        InferQuantizedMatrixVectorActivation(),\n",
    "        InferLabelSelectLayer(),\n",
    "        InferThresholdingLayer(),\n",
    "        InferConvInpGen(),\n",
    "        InferStreamingMaxPool(),\n",
    "        InferAddStreamsLayer(),\n",
    "        InferChannelwiseLinearLayer(),\n",
    "        InferConcatLayer(),\n",
    "        InferDuplicateStreamsLayer(),\n",
    "        InferGlobalAccPoolLayer(),\n",
    "        InferLookupLayer(),\n",
    "        InferPool(),\n",
    "        InferStreamingEltwise(),\n",
    "        InferUpsample(),\n",
    "        InferVectorVectorActivation(),\n",
    "        RemoveCNVtoFCFlatten(),\n",
    "        AbsorbConsecutiveTransposes()\n",
    "    ]\n",
    "\n",
    "    for iter_id in range(3):\n",
    "        for transform in transformations:\n",
    "            try:\n",
    "                input_hw_model = input_hw_model.transform(transform)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "        # Apply final tidy-up transformations\n",
    "        input_hw_model = tidy_up_transforms(input_hw_model, save_name)\n",
    "        \n",
    "        input_hw_model = input_hw_model.transform(InferDataLayouts())\n",
    "        input_hw_model = input_hw_model.transform(RemoveUnusedTensors())\n",
    "        input_hw_model = input_hw_model.transform(DoubleToSingleFloat())\n",
    "        input_hw_model = input_hw_model.transform(SortGraph())\n",
    "        input_hw_model = input_hw_model.transform(RemoveIdentityOps())\n",
    "\n",
    "    # Save the final transformed model\n",
    "    input_hw_model.save(save_name)\n",
    "\n",
    "    return input_hw_model"
   ],
   "id": "e34005da6e651b95",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:06.878237Z",
     "start_time": "2024-12-14T08:27:58.670511Z"
    }
   },
   "cell_type": "code",
   "source": "model = to_hw_transforms(model, set_onnx_checkpoint(Project_Info,\"To HW Layers\"))",
   "id": "3231949dbb03adb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: To HW Layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:427: UserWarning: MaxPoolNHWC_0: could not convert to HW\n",
      "  warnings.warn(node.name + \": could not convert to HW\")\n",
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:427: UserWarning: MaxPoolNHWC_1: could not convert to HW\n",
      "  warnings.warn(node.name + \": could not convert to HW\")\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:06.929416Z",
     "start_time": "2024-12-14T08:28:06.922346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "\n",
    "def dataflow_partitioning(input_data_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies dataflow partitioning transformation to the model and saves the resulting parent and dataflow models.\n",
    "\n",
    "    Parameters:\n",
    "        input_data_model (ModelWrapper): The model to transform.\n",
    "        save_name (str): The directory to save the transformed models.\n",
    "\n",
    "    Returns:\n",
    "        ModelWrapper: The transformed parent model.\n",
    "    \"\"\"\n",
    "    # Apply dataflow partitioning\n",
    "    parent_model = input_data_model.transform(CreateDataflowPartition())\n",
    "    parent_model.save(save_name)\n",
    "    \n",
    "    # Retrieve the dataflow partition model filename\n",
    "    sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "    sdp_node = getCustomOp(sdp_node)\n",
    "    dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "    # Load and return the dataflow model\n",
    "    dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "    \n",
    "    return dataflow_model"
   ],
   "id": "7b896786bf7eccae",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:07.739353Z",
     "start_time": "2024-12-14T08:28:06.989370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = dataflow_partitioning(model, set_onnx_checkpoint(Project_Info,\"Dataflow Partition Parent Model\"))\n",
    "model.save(set_onnx_checkpoint(Project_Info,\"Dataflow Partition Streaming Model\"))"
   ],
   "id": "a18f4bf215064d2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Dataflow Partition Parent Model\n",
      "[INFO] Saving Checkpoint: Dataflow Partition Streaming Model\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:07.789118Z",
     "start_time": "2024-12-14T08:28:07.782031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "\n",
    "def specialize_layers_transform(input_specialize_model, board_name, save_name):\n",
    "    \"\"\"\n",
    "    Applies layer specialization transformation to a dataflow model for the specified FPGA part and saves the resulting model.\n",
    "\n",
    "    Parameters:\n",
    "        input_specialize_model (ModelWrapper): The dataflow model to transform.\n",
    "        board_name (str): The FPGA board for which to specialize the layers.\n",
    "        save_name (str): The path to save the specialized model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed and specialized dataflow model.\n",
    "    \"\"\"\n",
    "    fpga_part = pynq_part_map[board_name]\n",
    "    # Apply specialization for FPGA layers\n",
    "    input_specialize_model = input_specialize_model.transform(SpecializeLayers(fpga_part))\n",
    "\n",
    "    # Save the specialized model\n",
    "    input_specialize_model.save(save_name)\n",
    "\n",
    "    return input_specialize_model"
   ],
   "id": "29068574207cfe2",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:08.028310Z",
     "start_time": "2024-12-14T08:28:07.831498Z"
    }
   },
   "cell_type": "code",
   "source": "model = specialize_layers_transform(model, Project_Info['Board_name'], set_onnx_checkpoint(Project_Info,f\"Specialize Model Layers to {Project_Info['Board_name']}\"))",
   "id": "66573cba9e8b29aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Specialize Model Layers to Pynq-ZU\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:08.352337Z",
     "start_time": "2024-12-14T08:28:08.167520Z"
    }
   },
   "cell_type": "code",
   "source": "model = ModelWrapper(\"/home/fastqnn/finn/notebooks/Fast-QNN/outputs/txaviour/alexnet_8w8a_mnist_0/checkpoints/alexnet_8w8a_mnist_specialize_model_layers_to_pynqzu.onnx\")",
   "id": "eb11b8cb9d541fbc",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:08.472554Z",
     "start_time": "2024-12-14T08:28:08.453107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_folding_config(\n",
    "    model: ModelWrapper,\n",
    "    max_pe=32,\n",
    "    max_simd=32,\n",
    "    default_fifo_depth_fc=128,\n",
    "    default_fifo_depth_swg=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate folding configuration for each MVAU_hls (FC) and ConvolutionInputGenerator_rtl (SWG)\n",
    "    node in a FINN model. Returns a dictionary keyed by node name, each containing\n",
    "    {\"PE\": ..., \"SIMD\": ..., \"inFIFODepths\": [...]}\n",
    "    \"\"\"\n",
    "    folding_config = {}\n",
    "\n",
    "    # Process all MVAU (fully connected) layers\n",
    "    fc_layers = model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "    for node in fc_layers:\n",
    "        node_name = node.name\n",
    "        input_shape = model.get_tensor_shape(node.input[0])  # [N, IFMChannels]\n",
    "        output_shape = model.get_tensor_shape(node.output[0])  # [N, OFMChannels]\n",
    "        ifm_channels = input_shape[1]\n",
    "        ofm_channels = output_shape[1]\n",
    "\n",
    "        # Determine PE (divisor of OFMChannels) if required\n",
    "        pe_candidates = [x for x in range(1, max_pe + 1) if ofm_channels % x == 0]\n",
    "        pe = max(pe_candidates) if pe_candidates else 1  # fallback to 1 if no valid divisors\n",
    "\n",
    "        # Determine valid SIMDs (must divide IFMChannels)\n",
    "        simd_candidates = [x for x in range(1, max_simd + 1) if ifm_channels % x == 0]\n",
    "        simd = max(simd_candidates) if simd_candidates else 1\n",
    "\n",
    "        fifo_depths = [default_fifo_depth_fc]\n",
    "        folding_config[node_name] = {\n",
    "            \"PE\": pe,\n",
    "            \"SIMD\": simd,\n",
    "            \"inFIFODepths\": fifo_depths,\n",
    "        }\n",
    "\n",
    "    # Process all ConvolutionInputGenerator (sliding window) layers\n",
    "    swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator_rtl\")\n",
    "    for node in swg_layers:\n",
    "        node_name = node.name\n",
    "        input_shape = model.get_tensor_shape(node.input[0])  # [N, IFMChannels, H, W]\n",
    "        ifm_channels = input_shape[1]\n",
    "\n",
    "        # Determine valid SIMDs (must divide IFMChannels)\n",
    "        simd_candidates = [x for x in range(1, max_simd + 1) if ifm_channels % x == 0]\n",
    "        simd = max(simd_candidates) if simd_candidates else 1\n",
    "\n",
    "        fifo_depths = [default_fifo_depth_swg]\n",
    "        folding_config[node_name] = {\n",
    "            \"PE\": 1,  # Typically not used for SWG layers\n",
    "            \"SIMD\": simd,\n",
    "            \"inFIFODepths\": fifo_depths,\n",
    "        }\n",
    "\n",
    "    return folding_config\n"
   ],
   "id": "90d653d3382d15b4",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:08.496092Z",
     "start_time": "2024-12-14T08:28:08.481778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def folding_transform(input_folding_model: ModelWrapper, save_name: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Applies a user-defined folding configuration (from generate_folding_config) to MVAU_hls and\n",
    "    ConvolutionInputGenerator_rtl layers, then saves the result.\n",
    "\n",
    "    Parameters:\n",
    "        input_folding_model (ModelWrapper): The specialized model to transform.\n",
    "        save_name (str): Path to save the transformed model (e.g., checkpoint).\n",
    "        **kwargs: Pass additional arguments to generate_folding_config (e.g. max_pe, max_simd, etc.)\n",
    "\n",
    "    Returns:\n",
    "        ModelWrapper: The transformed and folded dataflow model.\n",
    "    \"\"\"\n",
    "    # Generate folding config dictionary keyed by node name\n",
    "    folding_config = generate_folding_config(input_folding_model, **kwargs)\n",
    "\n",
    "    # Fold MVAU (fully-connected) layers\n",
    "    fc_layers = input_folding_model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "    for node in fc_layers:\n",
    "        node_name = node.name\n",
    "        if node_name in folding_config:\n",
    "            config = folding_config[node_name]\n",
    "            node_inst = getCustomOp(node)\n",
    "            node_inst.set_nodeattr(\"PE\", config[\"PE\"])\n",
    "            node_inst.set_nodeattr(\"SIMD\", config[\"SIMD\"])\n",
    "            node_inst.set_nodeattr(\"inFIFODepths\", config[\"inFIFODepths\"])\n",
    "\n",
    "    # Fold ConvolutionInputGenerator (sliding window) layers\n",
    "    swg_layers = input_folding_model.get_nodes_by_op_type(\"ConvolutionInputGenerator_rtl\")\n",
    "    for node in swg_layers:\n",
    "        node_name = node.name\n",
    "        if node_name in folding_config:\n",
    "            config = folding_config[node_name]\n",
    "            node_inst = getCustomOp(node)\n",
    "            node_inst.set_nodeattr(\"SIMD\", config[\"SIMD\"])\n",
    "            node_inst.set_nodeattr(\"inFIFODepths\", config[\"inFIFODepths\"])\n",
    "\n",
    "    # Apply unique node names (good practice to avoid naming collisions)\n",
    "    input_folding_model = input_folding_model.transform(GiveUniqueNodeNames())\n",
    "    input_folding_model.save(save_name)\n",
    "\n",
    "    return input_folding_model\n"
   ],
   "id": "cd5527c3889d3ed2",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:08.905328Z",
     "start_time": "2024-12-14T08:28:08.543464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.set_folding import SetFolding\n",
    "model = model.transform(SetFolding())\n",
    "# Ensure node names are unique\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model.save(set_onnx_checkpoint(Project_Info, \"Folded Model\"))"
   ],
   "id": "1269bd96fdeb9db3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/set_folding.py:221: UserWarning: SetFolding doesn't know how to handle op_type FMPadding_rtl\n",
      "  warnings.warn(\"SetFolding doesn't know how to handle op_type \" + op_type)\n",
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/set_folding.py:233: UserWarning: Node ConvolutionInputGenerator_rtl_3 is bottleneck with 2879 cycles, running second pass\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Folded Model\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:08.962156Z",
     "start_time": "2024-12-14T08:28:08.952521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "\n",
    "def zynq_build_transform(input_zynq_model, save_name, brd_name):\n",
    "    \"\"\"\n",
    "    Applies the ZynqBuild transformation to a model for the specified PYNQ board and clock period.\n",
    "\n",
    "    Parameters:\n",
    "        input_zynq_model (ModelWrapper): Folded model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "        brd_name (str): Name of the PYNQ board to target.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model after applying ZynqBuild.\n",
    "    \"\"\"\n",
    "    target_clk_ns = 10\n",
    "    # Apply ZynqBuild transformation\n",
    "    input_zynq_model = input_zynq_model.transform(ZynqBuild(platform=brd_name, period_ns=target_clk_ns))\n",
    "    \n",
    "    # Save the transformed model\n",
    "    input_zynq_model.save(save_name)\n",
    "\n",
    "    return input_zynq_model\n"
   ],
   "id": "394bed78132471e6",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:28:09.024405Z",
     "start_time": "2024-12-14T08:28:09.013257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "\n",
    "def pynq_driver_transform(input_driver_model, save_name):\n",
    "    \"\"\"\n",
    "    Applies the MakePYNQDriver transformation to the model to generate a PYNQ-compatible driver.\n",
    "\n",
    "    Parameters:\n",
    "        input_driver_model (ModelWrapper): ZynqBuild model to transform.\n",
    "        save_name (str): Directory to save the transformed model.\n",
    "    \n",
    "    Returns:\n",
    "        ModelWrapper: The transformed model with PYNQ driver compatibility.\n",
    "    \"\"\"\n",
    "    # Apply MakePYNQDriver transformation\n",
    "    input_driver_model = input_driver_model.transform(MakePYNQDriver(\"zynq-iodma\"))\n",
    "    \n",
    "    # Save the transformed model\n",
    "    input_driver_model.save(save_name)\n",
    "\n",
    "    return input_driver_model\n"
   ],
   "id": "d1f6d7caa00a8452",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:55:12.600730Z",
     "start_time": "2024-12-14T08:28:09.074734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = zynq_build_transform(model, set_onnx_checkpoint(Project_Info, \"Zynq Build\"), Project_Info['Board_name'])\n",
    "model = pynq_driver_transform(model, set_onnx_checkpoint(Project_Info, \"Pynq Driver\"))"
   ],
   "id": "669b7375d6509996",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving Checkpoint: Zynq Build\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/floorplan.py:107: UserWarning: 43 nodes have no entry in the provided floorplan, SLR was set to -1\n",
      "  warnings.warn(\n",
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:234: UserWarning: Input FIFO for IODMA_hls_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/fastqnn/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:294: UserWarning: Output FIFO for LabelSelect_hls_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mzynq_build_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_onnx_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mProject_Info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mZynq Build\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mProject_Info\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBoard_name\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m pynq_driver_transform(model, set_onnx_checkpoint(Project_Info, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPynq Driver\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "Cell \u001B[0;32mIn[30], line 17\u001B[0m, in \u001B[0;36mzynq_build_transform\u001B[0;34m(input_zynq_model, save_name, brd_name)\u001B[0m\n\u001B[1;32m     15\u001B[0m target_clk_ns \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Apply ZynqBuild transformation\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m input_zynq_model \u001B[38;5;241m=\u001B[39m \u001B[43minput_zynq_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mZynqBuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplatform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbrd_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mperiod_ns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_clk_ns\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Save the transformed model\u001B[39;00m\n\u001B[1;32m     20\u001B[0m input_zynq_model\u001B[38;5;241m.\u001B[39msave(save_name)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/qonnx/core/modelwrapper.py:140\u001B[0m, in \u001B[0;36mModelWrapper.transform\u001B[0;34m(self, transformation, make_deepcopy, cleanup)\u001B[0m\n\u001B[1;32m    138\u001B[0m model_was_changed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m model_was_changed:\n\u001B[0;32m--> 140\u001B[0m     (transformed_model, model_was_changed) \u001B[38;5;241m=\u001B[39m \u001B[43mtransformation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformed_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cleanup:\n\u001B[1;32m    142\u001B[0m     transformed_model\u001B[38;5;241m.\u001B[39mcleanup()\n",
      "File \u001B[0;32m/home/fastqnn/finn/src/finn/transformation/fpgadataflow/make_zynq_proj.py:345\u001B[0m, in \u001B[0;36mZynqBuild.apply\u001B[0;34m(self, model)\u001B[0m\n\u001B[1;32m    343\u001B[0m kernel_model\u001B[38;5;241m.\u001B[39msave(dataflow_model_filename)\n\u001B[1;32m    344\u001B[0m kernel_model \u001B[38;5;241m=\u001B[39m kernel_model\u001B[38;5;241m.\u001B[39mtransform(PrepareIP(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfpga_part, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mperiod_ns))\n\u001B[0;32m--> 345\u001B[0m kernel_model \u001B[38;5;241m=\u001B[39m \u001B[43mkernel_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mHLSSynthIP\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m kernel_model \u001B[38;5;241m=\u001B[39m kernel_model\u001B[38;5;241m.\u001B[39mtransform(\n\u001B[1;32m    347\u001B[0m     CreateStitchedIP(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfpga_part, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mperiod_ns, sdp_node\u001B[38;5;241m.\u001B[39monnx_node\u001B[38;5;241m.\u001B[39mname, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    348\u001B[0m )\n\u001B[1;32m    349\u001B[0m kernel_model\u001B[38;5;241m.\u001B[39mset_metadata_prop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplatform\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzynq-iodma\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/qonnx/core/modelwrapper.py:140\u001B[0m, in \u001B[0;36mModelWrapper.transform\u001B[0;34m(self, transformation, make_deepcopy, cleanup)\u001B[0m\n\u001B[1;32m    138\u001B[0m model_was_changed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m model_was_changed:\n\u001B[0;32m--> 140\u001B[0m     (transformed_model, model_was_changed) \u001B[38;5;241m=\u001B[39m \u001B[43mtransformation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformed_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cleanup:\n\u001B[1;32m    142\u001B[0m     transformed_model\u001B[38;5;241m.\u001B[39mcleanup()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/qonnx/transformation/base.py:112\u001B[0m, in \u001B[0;36mNodeLocalTransformation.apply\u001B[0;34m(self, model)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_workers \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m mp\u001B[38;5;241m.\u001B[39mPool(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_workers) \u001B[38;5;28;01mas\u001B[39;00m p:\n\u001B[0;32m--> 112\u001B[0m         new_nodes_and_bool \u001B[38;5;241m=\u001B[39m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapplyNodeLocal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mold_nodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# execute without mp.Pool in case of 1 worker to simplify debugging\u001B[39;00m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    115\u001B[0m     new_nodes_and_bool \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapplyNodeLocal(node) \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m old_nodes]\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:367\u001B[0m, in \u001B[0;36mPool.map\u001B[0;34m(self, func, iterable, chunksize)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, iterable, chunksize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    363\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001B[39;00m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;124;03m    in a list that is returned.\u001B[39;00m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[0;32m--> 367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapstar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:768\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 768\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    769\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mready():\n\u001B[1;32m    770\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:765\u001B[0m, in \u001B[0;36mApplyResult.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 765\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:607\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    605\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 607\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
